# Getting_It_Right#
GiR = function(Nsamp = 5000, nDocs = 10, node = 1:4, vocabulary =  c("hi", "hello","bye", "mine", "what"), #
			   nIP = 2, K = 4, nwords = 4, alpha = 2, mvec = rep(1/4, 4), betas = 2, nvec = rep(1/5, 5), #
			   prior.b.mean = c(-3, rep(0, 6)), prior.b.var = diag(7), prior.delta = c(2, 2), sigma_Q = 0.25, #
			   niters = c(1, 1, 1, 50, 0, 1), netstat = "dyadic", generate_PP_plots = TRUE, seed = 1) {#
	set.seed(seed)#
	b = lapply(1L:nIP, function(IP) {#
			   c(rmvnorm(1, prior.b.mean, prior.b.var))#
			   })#
	delta = 	rbeta(1, prior.delta[1], prior.delta[2])#
	currentC = sample(1L:nIP, K, replace = TRUE)	 #
	base.data = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, seed, #
							 base.edge = list(), base.text = list(), base = TRUE)#
	base.edge = base.data$edge	   #
	base.text = base.data$text#
#
	#Forward sampling#
	Forward_stats = matrix(NA, nrow = Nsamp, ncol = 21)#
	colnames(Forward_stats) = c(paste0("B_",1:length(b[[1]])), "Mean_receipients", "Mean_timediff", "Mean_TopicIP", #
						  paste0("Tokens_in_IP_", 1:nIP), paste0("Tokens_in_Topic", 1:K), #
						  paste0("Tokens_in_Word", 1:length(vocabulary)))#
	for (i in 1:Nsamp) { #
		if (i %% 5000 == 0) {cat("Forward sampling", i, "\n")}#
		set.seed(i)#
		b = lapply(1:nIP, function(IP) {#
			c(rmvnorm(1, prior.b.mean, prior.b.var))#
			})#
		delta = rbeta(1, prior.delta[1], prior.delta[2])#
		currentC = sample(1L:nIP, K, replace = TRUE)#
		Forward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
									  base.edge = base.edge, base.text = base.text, seed, forward = TRUE) #
		Forward_stats[i, ] = GiR_stats(Forward_sample, K, currentC, vocabulary, forward = TRUE, backward = FALSE)#
	}#
	#Backward sampling#
	Backward_stats = matrix(NA, nrow = Nsamp, ncol = 21)#
	Backward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
								   base.edge = base.edge, base.text = base.text, seed, backward_init = TRUE) #
	for (i in 1:Nsamp) { #
		if (i %% 500 == 0) {cat("Backward sampling", i, "\n")}#
		seed = seed + 100#
		set.seed(seed)#
		Inference_samp = Inference(Backward_sample$edge, node, Backward_sample$text, vocabulary, nIP, K, sigma_Q, #
						  alpha, mvec, betas, nvec, prior.b.mean, prior.b.var, prior.delta,#
						  out = niters[1], n1 = niters[2], n2 = niters[3], n3 = niters[4], burn = niters[5], thin = niters[6], #
						  netstat, seed)#
		b = lapply(1:nIP, function(IP) {#
			rowMeans(Inference_samp$B[[IP]])#
		})#
		#delta = mean(Inference_samp$D)#
		delta = rbeta(1, prior.delta[1], prior.delta[2])#
		currentC = Inference_samp$C#
		topic_token_assignments = Inference_samp$Z#
		for (d in 1:length(topic_token_assignments)) {#
			names(topic_token_assignments[[d]]) = Backward_sample$text[[d + length(base.text)]]#
		}#
		topic_token_counts = tabulate(unlist(Inference_samp$Z), K)#
		word_type_topic_counts = matrix(NA , length(vocabulary), K)#
		for (w in 1:length(vocabulary)) {#
			word_type_w = which(unlist(Backward_sample$text[-(1:length(base.text))]) == w)#
			word_type_topic_counts[w, ] = tabulate(unlist(Inference_samp$Z)[word_type_w], K)#
		}#
		Backward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
								   	   base.edge = base.edge, base.text = base.text, seed, topic_token_assignments = topic_token_assignments, #
								   	   topic_token_counts = topic_token_counts, word_type_topic_counts = word_type_topic_counts, #
								   	   forward = FALSE, backward = TRUE)#
		Backward_stats[i, ] = GiR_stats(Backward_sample, K, currentC, vocabulary, forward = FALSE, backward = TRUE)#
	}#
	tstats = rep(0, ncol(Forward_stats))#
	wstats = rep(0, ncol(Forward_stats))#
	for (j in 1:ncol(Forward_stats)) {#
		thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 400)#
		Forward_test = Forward_stats[thin, j]#
		Backward_test = Backward_stats[thin, j]#
		tstats[j] = t.test(Backward_test, Forward_test)$p.value#
		wstats[j] = wilcox.test(Backward_test, Forward_test)$p.value#
	}#
	names(tstats) = names(wstats) = colnames(Forward_stats)						#
	if (generate_PP_plots) {#
		par(mfrow=c(3,7), oma = c(3,3,3,3), mar = c(5,5,4,1))#
		GiR_PP_Plots2(Forward_stats, Backward_stats)#
	}			#
	return(list(Forward = Forward_stats, Backward = Backward_stats, tstats = tstats, wstats = wstats))	#
}#
#
unix.time(TryGiR <- GiR(5000, seed = 15))
#PP_Plots#
GiR_PP_Plots2 = function(Forward_stats, Backward_stats) {#
	nms = colnames(Forward_stats)#
	for (i in 1L:ncol(Forward_stats)) {#
		if (nrow(Forward_stats) > 4000) {#
			thin = seq(from = floor(nrow(Forward_stats)/10), to = nrow(Forward_stats), length.out = 1000)#
			Forward_test = Forward_stats[thin, i]#
			Backward_test = Backward_stats[thin, i]#
		} else {#
			Forward_test = Forward_stats[, i]#
			Backward_test = Backward_stats[, i]#
		}#
		all = c(Backward_stats[, i], Forward_stats[, i])#
		ylims = c(min(all) - 0.1 * max(abs(all)), max(all) + 0.1 * max(abs(all)))#
		xlims = ylims#
		quantiles = 50#
		if (grepl("B_", nms[i]) ) {#
			quantiles = 1000#
		}#
		normalmean = mean(c(quantile(Forward_stats[, i], seq(0, 1, length = quantiles)), #
							quantile(Backward_stats[, i], seq(0, 1, length = quantiles))))#
		normalvar = sd(c(quantile(Forward_stats[, i], seq(0, 1, length = quantiles)), #
							quantile(Backward_stats[, i], seq(0, 1, length = quantiles))))#
#
		qqplot(x = pnorm(sort(quantile(Forward_stats[, i], seq(0, 1, length = quantiles))), normalmean, normalvar),#
			   y = pnorm(sort(quantile(Backward_stats[, i], seq(0, 1, length = quantiles))), normalmean, normalvar),#
			   ylim = pnorm(ylims, normalmean, normalvar),#
			   xlim = pnorm(xlims, normalmean, normalvar),#
			   ylab = "Backward",#
			   xlab = "Forward",#
			   col = "blue",#
			   pch = 19,#
			   main = nms[i],#
			   cex.lab = 1,#
			   cex.axis = 1,#
			   cex.main = 1)#
		lines(x = pnorm(xlims, normalmean, normalvar), y = pnorm(ylims, normalmean, normalvar), col = "red", lwd = 3)#
		text(paste("Backward Mean:", round(mean(Backward_stats[,i]), 4),#
				   "\nForward Mean:", round(mean(Forward_stats[,i]), 4),#
				   "\nt-test p-value:", round(t.test(Backward_test, Forward_test)$p.value, 4),#
				   "\nMann-Whitney p-value:", round(wilcox.test(Backward_test, Forward_test)$p.value,4)),#
				   x = pnorm(xlims[2], normalmean, normalvar) - 0.35 * abs(pnorm(xlims[2], normalmean, normalvar) - pnorm(xlims[1], normalmean, normalvar)),#
				   y = pnorm(ylims[1], normalmean, normalvar) + 0.15 * abs(pnorm(ylims[2], normalmean, normalvar) - pnorm(ylims[1], normalmean, normalvar)),#
				   cex = 0.3)#
	}#
}      #
# Getting_It_Right#
GiR = function(Nsamp = 5000, nDocs = 10, node = 1:4, vocabulary =  c("hi", "hello","bye", "mine", "what"), #
			   nIP = 2, K = 4, nwords = 4, alpha = 2, mvec = rep(1/4, 4), betas = 2, nvec = rep(1/5, 5), #
			   prior.b.mean = c(-3, rep(0, 6)), prior.b.var = diag(7), prior.delta = c(2, 2), sigma_Q = 0.25, #
			   niters = c(1, 1, 1, 50, 0, 1), netstat = "dyadic", generate_PP_plots = TRUE, seed = 1) {#
	set.seed(seed)#
	b = lapply(1L:nIP, function(IP) {#
			   c(rmvnorm(1, prior.b.mean, prior.b.var))#
			   })#
	delta = 	rbeta(1, prior.delta[1], prior.delta[2])#
	currentC = sample(1L:nIP, K, replace = TRUE)	 #
	base.data = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, seed, #
							 base.edge = list(), base.text = list(), base = TRUE)#
	base.edge = base.data$edge	   #
	base.text = base.data$text#
#
	#Forward sampling#
	Forward_stats = matrix(NA, nrow = Nsamp, ncol = 21)#
	colnames(Forward_stats) = c(paste0("B_",1:length(b[[1]])), "Mean_receipients", "Mean_timediff", "Mean_TopicIP", #
						  paste0("Tokens_in_IP_", 1:nIP), paste0("Tokens_in_Topic", 1:K), #
						  paste0("Tokens_in_Word", 1:length(vocabulary)))#
	for (i in 1:Nsamp) { #
		if (i %% 5000 == 0) {cat("Forward sampling", i, "\n")}#
		set.seed(i)#
		b = lapply(1:nIP, function(IP) {#
			c(rmvnorm(1, prior.b.mean, prior.b.var))#
			})#
		delta = rbeta(1, prior.delta[1], prior.delta[2])#
		currentC = sample(1L:nIP, K, replace = TRUE)#
		Forward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
									  base.edge = base.edge, base.text = base.text, seed, forward = TRUE) #
		Forward_stats[i, ] = GiR_stats(Forward_sample, K, currentC, vocabulary, forward = TRUE, backward = FALSE)#
	}#
	#Backward sampling#
	Backward_stats = matrix(NA, nrow = Nsamp, ncol = 21)#
	Backward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
								   base.edge = base.edge, base.text = base.text, seed, backward_init = TRUE) #
	for (i in 1:Nsamp) { #
		if (i %% 500 == 0) {cat("Backward sampling", i, "\n")}#
		seed = seed + 100#
		set.seed(seed)#
		Inference_samp = Inference(Backward_sample$edge, node, Backward_sample$text, vocabulary, nIP, K, sigma_Q, #
						  alpha, mvec, betas, nvec, prior.b.mean, prior.b.var, prior.delta,#
						  out = niters[1], n1 = niters[2], n2 = niters[3], n3 = niters[4], burn = niters[5], thin = niters[6], #
						  netstat, seed)#
		b = lapply(1:nIP, function(IP) {#
			rowMeans(Inference_samp$B[[IP]])#
		})#
		#delta = mean(Inference_samp$D)#
		delta = rbeta(1, prior.delta[1], prior.delta[2])#
		currentC = Inference_samp$C#
		topic_token_assignments = Inference_samp$Z#
		for (d in 1:length(topic_token_assignments)) {#
			names(topic_token_assignments[[d]]) = Backward_sample$text[[d + length(base.text)]]#
		}#
		topic_token_counts = tabulate(unlist(Inference_samp$Z), K)#
		word_type_topic_counts = matrix(NA , length(vocabulary), K)#
		for (w in 1:length(vocabulary)) {#
			word_type_w = which(unlist(Backward_sample$text[-(1:length(base.text))]) == w)#
			word_type_topic_counts[w, ] = tabulate(unlist(Inference_samp$Z)[word_type_w], K)#
		}#
		Backward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
								   	   base.edge = base.edge, base.text = base.text, seed, topic_token_assignments = topic_token_assignments, #
								   	   topic_token_counts = topic_token_counts, word_type_topic_counts = word_type_topic_counts, #
								   	   forward = FALSE, backward = TRUE)#
		Backward_stats[i, ] = GiR_stats(Backward_sample, K, currentC, vocabulary, forward = FALSE, backward = TRUE)#
	}#
	tstats = rep(0, ncol(Forward_stats))#
	wstats = rep(0, ncol(Forward_stats))#
	for (j in 1:ncol(Forward_stats)) {#
		thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 400)#
		Forward_test = Forward_stats[thin, j]#
		Backward_test = Backward_stats[thin, j]#
		tstats[j] = t.test(Backward_test, Forward_test)$p.value#
		wstats[j] = wilcox.test(Backward_test, Forward_test)$p.value#
	}#
	names(tstats) = names(wstats) = colnames(Forward_stats)						#
	if (generate_PP_plots) {#
		par(mfrow=c(3,7), oma = c(3,3,3,3), mar = c(5,5,4,1))#
		GiR_PP_Plots2(Forward_stats, Backward_stats)#
	}			#
	return(list(Forward = Forward_stats, Backward = Backward_stats, tstats = tstats, wstats = wstats))	#
}#
#
unix.time(TryGiR <- GiR(5000, seed = 15))
# Getting_It_Right#
GiR = function(Nsamp = 5000, nDocs = 10, node = 1:4, vocabulary =  c("hi", "hello","bye", "mine", "what"), #
			   nIP = 2, K = 4, nwords = 4, alpha = 2, mvec = rep(1/4, 4), betas = 2, nvec = rep(1/5, 5), #
			   prior.b.mean = c(-3, rep(0, 6)), prior.b.var = diag(7), prior.delta = c(2, 2), sigma_Q = 0.25, #
			   niters = c(1, 1, 1, 50, 0, 1), netstat = "dyadic", generate_PP_plots = TRUE, seed = 1) {#
	set.seed(seed)#
	b = lapply(1L:nIP, function(IP) {#
			   c(rmvnorm(1, prior.b.mean, prior.b.var))#
			   })#
	delta = 	rbeta(1, prior.delta[1], prior.delta[2])#
	currentC = sample(1L:nIP, K, replace = TRUE)	 #
	base.data = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, seed, #
							 base.edge = list(), base.text = list(), base = TRUE)#
	base.edge = base.data$edge	   #
	base.text = base.data$text#
#
	#Forward sampling#
	Forward_stats = matrix(NA, nrow = Nsamp, ncol = 21)#
	colnames(Forward_stats) = c(paste0("B_",1:length(b[[1]])), "Mean_receipients", "Mean_timediff", "Mean_TopicIP", #
						  paste0("Tokens_in_IP_", 1:nIP), paste0("Tokens_in_Topic", 1:K), #
						  paste0("Tokens_in_Word", 1:length(vocabulary)))#
	for (i in 1:Nsamp) { #
		if (i %% 5000 == 0) {cat("Forward sampling", i, "\n")}#
		set.seed(i)#
		b = lapply(1:nIP, function(IP) {#
			c(rmvnorm(1, prior.b.mean, prior.b.var))#
			})#
		delta = rbeta(1, prior.delta[1], prior.delta[2])#
		print(delta)#
		currentC = sample(1L:nIP, K, replace = TRUE)#
		Forward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
									  base.edge = base.edge, base.text = base.text, seed, forward = TRUE) #
		Forward_stats[i, ] = GiR_stats(Forward_sample, K, currentC, vocabulary, forward = TRUE, backward = FALSE)#
	}#
	#Backward sampling#
	Backward_stats = matrix(NA, nrow = Nsamp, ncol = 21)#
	Backward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
								   base.edge = base.edge, base.text = base.text, seed, backward_init = TRUE) #
	for (i in 1:Nsamp) { #
		if (i %% 500 == 0) {cat("Backward sampling", i, "\n")}#
		seed = seed + 100#
		set.seed(seed)#
		Inference_samp = Inference(Backward_sample$edge, node, Backward_sample$text, vocabulary, nIP, K, sigma_Q, #
						  alpha, mvec, betas, nvec, prior.b.mean, prior.b.var, prior.delta,#
						  out = niters[1], n1 = niters[2], n2 = niters[3], n3 = niters[4], burn = niters[5], thin = niters[6], #
						  netstat, seed)#
		b = lapply(1:nIP, function(IP) {#
			rowMeans(Inference_samp$B[[IP]])#
		})#
		delta = mean(Inference_samp$D)#
		print(delta)#
		currentC = Inference_samp$C#
		topic_token_assignments = Inference_samp$Z#
		for (d in 1:length(topic_token_assignments)) {#
			names(topic_token_assignments[[d]]) = Backward_sample$text[[d + length(base.text)]]#
		}#
		topic_token_counts = tabulate(unlist(Inference_samp$Z), K)#
		word_type_topic_counts = matrix(NA , length(vocabulary), K)#
		for (w in 1:length(vocabulary)) {#
			word_type_w = which(unlist(Backward_sample$text[-(1:length(base.text))]) == w)#
			word_type_topic_counts[w, ] = tabulate(unlist(Inference_samp$Z)[word_type_w], K)#
		}#
		Backward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
								   	   base.edge = base.edge, base.text = base.text, seed, topic_token_assignments = topic_token_assignments, #
								   	   topic_token_counts = topic_token_counts, word_type_topic_counts = word_type_topic_counts, #
								   	   forward = FALSE, backward = TRUE)#
		Backward_stats[i, ] = GiR_stats(Backward_sample, K, currentC, vocabulary, forward = FALSE, backward = TRUE)#
	}#
	tstats = rep(0, ncol(Forward_stats))#
	wstats = rep(0, ncol(Forward_stats))#
	for (j in 1:ncol(Forward_stats)) {#
		thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 400)#
		Forward_test = Forward_stats[thin, j]#
		Backward_test = Backward_stats[thin, j]#
		tstats[j] = t.test(Backward_test, Forward_test)$p.value#
		wstats[j] = wilcox.test(Backward_test, Forward_test)$p.value#
	}#
	names(tstats) = names(wstats) = colnames(Forward_stats)						#
	if (generate_PP_plots) {#
		par(mfrow=c(3,7), oma = c(3,3,3,3), mar = c(5,5,4,1))#
		GiR_PP_Plots2(Forward_stats, Backward_stats)#
	}			#
	return(list(Forward = Forward_stats, Backward = Backward_stats, tstats = tstats, wstats = wstats))	#
}#
#
unix.time(TryGiR <- GiR(100, seed = 15))
rm(list=ls())
library(mvtnorm)#
library(MCMCpack)#
library(entropy)#
library(Rcpp)#
library(RcppArmadillo)#
sourceCpp('~/Desktop/IPTM-master/pkg/src/sampler.cpp')#
source('~/Desktop/IPTM-master/pkg/R/core.R')#
GenerateDocs = function(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec,#
						b, delta, currentC, netstat, base.edge, base.text, seed,#
						topic_token_assignments = NULL, topic_token_counts = NULL, word_type_topic_counts = NULL, #
						forward = FALSE, backward_init = FALSE, backward = FALSE, base = FALSE) {#
	set.seed(seed)#
 	W = length(vocabulary)#
  	phi = lapply(1L:K, function(k) {#
		rdirichlet_cpp(1, betas * nvec)#
	})#
	L = 3#
    P = 1 + L * (2 * ("dyadic" %in% netstat) + 4 * ("triadic" %in% netstat) + 2 *("degree" %in% netstat))#
    if (base) {#
    	t.d = 0#
    } else {#
    	t.d = 384#
    }#
	edge = base.edge#
	text = base.text#
	p.d = matrix(NA, nrow = nDocs, ncol = nIP)#
	options(warn = -1)#
	for (d in 1:nDocs) {#
		N.d = nwords#
		text[[length(base.text) + d]] = rep(NA, N.d)#
		if (!backward) {#
			theta.d = rdirichlet_cpp(1, alpha * mvec)	#
			topic.d = multinom_vec(max(1, N.d), theta.d)#
			if (N.d > 0) {#
				for (n in 1:N.d){#
					text[[length(base.text) + d]][n] = multinom_vec(1, phi[[topic.d[n]]])#
				}#
				names(text[[length(base.text) + d]]) = topic.d#
			}#
		} else {#
			phi.k = rep(NA, K)#
			topic.d = topic_token_assignments[[d]]#
			word.d = as.numeric(names(topic_token_assignments[[d]]))#
			if (N.d > 0) {#
				for (n in 1:N.d){#
					word_type_topic_counts[word.d [n], topic.d[n]] = word_type_topic_counts[word.d [n], topic.d[n]] - 1#
					for (w in 1:W) {#
						phi.k[w] = (word_type_topic_counts[w, topic.d[n]] + betas * nvec[w])/(topic_token_counts[topic.d[n]] + betas)#
					} #
					text[[length(base.text) + d]][n] = multinom_vec(1, phi.k)#
					word_type_topic_counts[text[[d]][n], topic.d[n]] = word_type_topic_counts[text[[d]][n], topic.d[n]] + 1#
				}#
				names(text[[length(base.text) + d]]) = topic.d#
			}#
		}#
#
		p.d[d, ] = vapply(1L:nIP, function(IP) {#
			sum(topic.d %in% which(currentC == IP))#
			}, c(1)) / max(1, N.d)#
		if (base & t.d < 384) {#
			history.t = lapply(1:nIP, function(IP) {#
				lapply(1:3, function(l){#
				matrix(0, length(node), length(node))#
				})#
			})#
		} else {	#
		history.t = History(edge, p.d, node, t.d + 10^(-10))#
		}#
		X = lapply(node, function(i) {#
  	        Netstats(history.t, node, i, netstat)#
            })#
   	 	XB = MultiplyXBList(X, b)     #
		lambda = lambda_cpp(p.d[d,], XB)#
		iJi = rbinom_mat((delta * lambda) / (delta * lambda + 1))#
		while (sum(iJi) == 0) {#
			iJi = rbinom_mat((delta * lambda) / (delta * lambda + 1))#
		}#
		LambdaiJi = lambdaiJi(p.d[d,], XB, iJi)#
		Time.inc = vapply(LambdaiJi, function(lambda) {#
			rexp(1, lambda)#
			}, c(1))#
		i.d = which(Time.inc == min(Time.inc[!is.na(Time.inc)]))#
		j.d = which(iJi[i.d,] == 1)#
		t.d = t.d + Time.inc[i.d]#
		edge[[length(base.edge) + d]] = list(sender = i.d, receiver = j.d, timestamp = t.d)		#
		}#
	options(warn = 0)#
	if (forward) {#
		edge = edge[-(1:length(base.edge))]#
		text = text[-(1:length(base.text))]#
	}#
	if (base == TRUE & t.d > 384) {#
		cutoff = which_int(384, vapply(1:length(edge), function(d) {edge[[d]][[3]]}, c(1))) - 1#
		edge = edge[1:cutoff]#
		text = text[1:cutoff]#
	}#
	return(list(edge = edge, text = text, b = b, base = length(base.edge)))							#
} #
GiR_stats = function(GiR_sample, K, currentC, vocabulary, forward = FALSE, backward = FALSE) {#
	edge = GiR_sample$edge#
	text = GiR_sample$text#
	if (backward) {#
		edge = edge[-(1:GiR_sample$base)]#
		text = text[-(1:GiR_sample$base)]#
	}#
	GiR_stats = c()#
	nDocs = length(edge)#
	P = length(GiR_sample$b[[1]])#
	nIP = length(GiR_sample$b)#
	nwords = length(GiR_sample$text[[1]])#
	W = length(vocabulary)#
	GiR_stats[1:P] = Reduce('+', GiR_sample$b) / nIP#
	GiR_stats[P + 1] = mean(vapply(1:nDocs, function(d) {#
						length(edge[[d]][[2]])#
						}, c(1)))#
	GiR_stats[P + 2] = mean(vapply(2:nDocs, function(d) {#
					 edge[[d]][[3]] - edge[[d-1]][[3]]#
					 }, c(1))) 			#
	GiR_stats[P + 3] = mean(currentC)#
	Tokens_in_Topic = tabulate(vapply(1:nDocs, function(d){#
					  as.numeric(names(text[[d]]))#
					  }, rep(0, nwords)), K)#
	GiR_stats[(P + 4):(P + 3 + nIP)] = vapply(1:nIP, function(IP) {#
								 Tokens_in_Topic %*% (currentC == IP)#
								 }, c(1))#
	GiR_stats[(P + 4 + nIP):(P + 3 + nIP + K)] = Tokens_in_Topic#
	Tokens_in_Word = tabulate(vapply(1:nDocs, function(d){#
					 text[[d]]#
					 }, rep(0, nwords)), W)#
	GiR_stats[(P + 4 + nIP + K):(P + 3 + nIP + K + W)] = Tokens_in_Word#
	return(GiR_stats)#
}#
Inference = function(edge, node, textlist, vocabulary, nIP, K, sigma_Q, alpha, mvec, betas, nvec, prior.b.mean, prior.b.var, prior.delta, #
					out, n1, n2, n3, burn, thin, netstat, seed, plot = FALSE) {#
  set.seed(seed)#
  # trim the edge so that we only model edges after 16 days#
	timeinc = c(as.numeric(edge[[1]][3]), vapply(seq(along = edge)[-1], function(d) {#
  	as.numeric(edge[[d]][3]) - as.numeric(edge[[d-1]][3])#
 	}, c(1)))#
#
    edge2 = which_int(384, cumsum(timeinc)) : length(edge)#
  # initialize alpha, mvec, delta, nvec, eta, lvec, and gammas#
 	W = length(vocabulary)#
  	phi = lapply(1L:K, function(k) {#
		rdirichlet_cpp(1, betas * nvec)#
	})#
	delta = rbeta(1, prior.delta[1], prior.delta[2])#
	deltamat = delta#
	eta = qnorm(delta)#
	# initialize C, theta and Z#
	currentC = sample(1:nIP, K, replace = TRUE)#
	theta = rdirichlet_cpp(length(edge), alpha * mvec)#
	currentZ = lapply(seq(along = edge), function(d) {#
    if (length(textlist[[d]]) > 0) {#
    		multinom_vec(length(textlist[[d]]), theta[d, ])#
    	} else {#
    		multinom_vec(1, theta[d, ])#
    	}#
    })#
	p.d = t(vapply(seq(along = edge), function(d) {#
	  vapply(1L:nIP, function(IP) {#
	    sum(currentZ[[d]] %in% which(currentC == IP))#
	  }, c(1)) / length(currentZ[[d]])#
	}, rep(1, nIP)))#
#
    # initialize beta#
    sigma = 1#
    L = 3#
    P = 1 + L * (2 * ("dyadic" %in% netstat) + 4 * ("triadic" %in% netstat) + 2 *("degree" %in% netstat))#
    bmat = list()#
	for (IP in 1L:nIP) {#
		bmat[[IP]] = matrix(0, nrow = P, ncol = (n3 - burn) / thin)#
		bmat[[IP]][, 1:(n3 - burn) / thin] = c(rmvnorm(1, prior.b.mean, prior.b.var))#
  	}#
#
    # to check the convergence  #
    if (plot) {#
     	logWZ.mat = c()							  #
     	alpha.mat = c()#
     	entropy.mat = c()#
    }#
    iJi = lapply(seq(along = edge), function(d) {#
  	      matrix(0, nrow = length(node), ncol = length(node))#
  	      })  #
    #start outer iteration#
    for (o in 1L:out) {#
      Beta.old = lapply(bmat, function(b) {#
      			rowMeans(b)#
         		})  #
     # Data augmentation#
      lambda = list()#
      LambdaiJi = list()#
      nonemptyiJi = list()#
	  observediJi = list()#
      for (d in edge2) {#
   	 	history.t = History(edge, p.d, node, as.numeric(edge[[d]][3]))#
   	 	X = lapply(node, function(i) {#
  	        Netstats(history.t, node, i, netstat)#
            })#
   	 	XB = MultiplyXBList(X, Beta.old)     #
		lambda[[d]] = lambda_cpp(p.d[d,], XB)#
		#calculate the resampling probability#
		probij = DataAug_cpp(iJi[[d]], lambda[[d]], delta, timeinc[d])#
		iJi[[d]] = rbinom_mat(probij)#
		iJi[[d]][as.numeric(edge[[d]][1]),] = tabulateC(as.numeric(unlist(edge[[d]][2])), length(node))#
		LambdaiJi[[d]] = lambdaiJi(p.d[d,], XB, iJi[[d]])#
		nonemptyiJi[[d]] = LambdaiJi[[d]][!is.na(LambdaiJi[[d]])]#
		observediJi[[d]] = LambdaiJi[[d]][as.numeric(edge[[d]][1])]#
		}	 #
    textlist.raw = unlist(textlist)#
    table.W = lapply(1L:K, function(k) {#
      tabulateC(textlist.raw[which(unlist(currentZ) == k)], W)#
      })#
    for (i1 in 1L:n1) {#
      for (d in edge2) { #
        textlist.d = textlist[[d]]#
        if (length(textlist.d) > 0) {#
        	topicpart.d = TopicInEqZ(K, currentZ[[d]], alpha, mvec, d)#
       	wordpart.d = WordInEqZ(K, textlist.d, table.W, betas, nvec)#
        } else {#
        topicpart.d = 0#
        wordpart.d = matrix(0, nrow = length(currentZ[[d]]), ncol = K)#
        }#
        edgepart.d = EdgeInEqZ(iJi[[d]], lambda[[d]], delta)#
        timepart.d = TimeInEqZ(nonemptyiJi[[d]], timeinc[d])#
        observed.d = ObservedInEqZ(observediJi[[d]]) #
        fixedpart = topicpart.d + edgepart.d + timepart.d + observed.d #
        for (w in 1L:length(currentZ[[d]])) {#
          const.Z = fixedpart + wordpart.d[w, ]#
          const.Z = const.Z - max(const.Z)#
          zw.old = currentZ[[d]][w]#
          zw.new = multinom_vec(1, exp(const.Z))#
          if (zw.new != zw.old) {#
            currentZ[[d]][w] = zw.new#
            topicpart.d = TopicInEqZ(K, currentZ[[d]], alpha, mvec, d)#
            if (length(textlist.d) > 0) {	#
            wordpart.d = WordInEqZ(K, textlist.d, table.W, betas, nvec)#
            }#
            table.W = lapply(1L:K, function(k) {#
      				  tabulateC(textlist.raw[which(unlist(currentZ) == k)], W)#
      				  })#
      		p.d[d, ] = vapply(1L:nIP, function(IP) {#
	 			sum(currentZ[[d]] %in% which(currentC == IP))#
	 			}, c(1)) / length(currentZ[[d]])#
      		LambdaiJi[[d]] = lambdaiJi(p.d[d,], XB, iJi[[d]])#
			nonemptyiJi[[d]] = LambdaiJi[[d]][!is.na(LambdaiJi[[d]])]#
         	observediJi[[d]] = LambdaiJi[[d]][as.numeric(edge[[d]][1])]#
          }#
        }#
       }#
      }#
#
    for (i2 in 1L:n2) {#
      # C update given Z and B - within each document d#
      for (k in unique(unlist(currentZ[edge2]))) { #
        document.k = which(vapply(currentZ, function(d){k %in% d}, c(1)) == 1)#
        document.k = document.k[document.k %in% edge2]#
        const.C = rep(NA, nIP)#
        for (IP in 1L:nIP) {#
        	if (!currentC[k] == IP){#
          currentC[k] = IP#
          for (d in document.k) {#
            p.d[d, ] =  vapply(1L:nIP, function(IP) {#
              sum(currentZ[[d]] %in% which(currentC == IP))#
            }, c(1)) / length(currentZ[[d]])#
          }#
          for (d in edge2) {#
           history.t = History(edge, p.d, node, as.numeric(edge[[d]][3]))#
    	   	   X = lapply(node, function(i) {#
               Netstats(history.t, node, i, netstat)#
               })#
    	       XB = MultiplyXBList(X, Beta.old)    #
           lambda[[d]] = lambda_cpp(p.d[d,], XB)#
		   LambdaiJi[[d]] = lambdaiJi(p.d[d,], XB, iJi[[d]])#
		   nonemptyiJi[[d]] = LambdaiJi[[d]][!is.na(LambdaiJi[[d]])]#
           observediJi[[d]] = LambdaiJi[[d]][as.numeric(edge[[d]][1])]#
          }#
          }#
          const.C[IP] = sum(vapply(document.k, function(d) {#
            EdgeInEqZ(iJi[[d]], lambda[[d]], delta) + TimeInEqZ(nonemptyiJi[[d]], timeinc[d]) + #
    			ObservedInEqZ(observediJi[[d]]) #
          }, c(1))) / length(document.k)#
          }#
          const.C = const.C - max(const.C)#
          currentC[k] = multinom_vec(1, exp(const.C))#
      }#
    }#
    if (plot) {#
      entropy.mat = c(entropy.mat, entropy.empirical(currentC))#
      alpha.mat = rbind(alpha.mat, alpha)#
      logWZ.mat = c(logWZ.mat, logWZ(K, currentZ, textlist, table.W, alpha, mvec, betas, nvec))#
      }#
     for (d in edge2) {#
     	 p.d[d, ] =  vapply(1L:nIP, function(IP) {#
              		 sum(currentZ[[d]] %in% which(currentC == IP))#
                     }, c(1)) / length(currentZ[[d]])#
     	 history.t = History(edge, p.d, node, as.numeric(edge[[d]][3]))#
    	     X = lapply(node, function(i) {#
              Netstats(history.t, node, i, netstat)#
             })#
    	     XB = MultiplyXBList(X, Beta.old)   #
    	     lambda[[d]] = lambda_cpp(p.d[d,], XB)#
		 LambdaiJi[[d]] = lambdaiJi(p.d[d,], XB, iJi[[d]])#
		 nonemptyiJi[[d]] = LambdaiJi[[d]][!is.na(LambdaiJi[[d]])]#
         observediJi[[d]] = LambdaiJi[[d]][as.numeric(edge[[d]][1])]#
	 }#
	 # Beta and delta update#
	 prior.old1 = sum(vapply(1L:nIP, function(IP) {#
		  		 dmvnorm(Beta.old[[IP]], prior.b.mean, prior.b.var, log = TRUE)#
		  		 }, c(1))) #
	 post.old1 = sum(vapply(edge2, function(d) {#
	     	     EdgeInEqZ(iJi[[d]], lambda[[d]], delta) + TimeInEqZ(nonemptyiJi[[d]], timeinc[d]) +#
    			 	 ObservedInEqZ(observediJi[[d]])#
    			 }, c(1))) / length(edge2)#
    	 prior.old2 = dbeta(delta, prior.delta[1], prior.delta[2], log = TRUE) #
     post.old2 = sum(vapply(edge2, function(d) {#
    			    EdgeInEqZ(iJi[[d]], lambda[[d]], delta)#
    			    }, c(1))) / length(edge2)#
	 options(warn = -1)#
	 proposal.var = list()#
     for (IP in 1L:nIP) {#
     	 proposal.var[[IP]] = cor(t(bmat[[IP]]))#
         proposal.var[[IP]][is.na(proposal.var[[IP]])] = 0#
         if (sum(eigen(proposal.var[[IP]])$values < 0 ) > 0) {#
        	 proposal.var[[IP]] = diag(P)#
         }#
         }#
     proposal.var = lapply(1:nIP, function(IP){diag(P)})#
     options(warn = 0)#
     for (i3 in 1L:n3) {#
     	Beta.new = lapply(1L:nIP, function(IP) {#
           rmvnorm(1, Beta.old[[IP]], sigma_Q * proposal.var[[IP]])#
         }) #
        for (d in edge2) {#
           XB = MultiplyXBList(X, Beta.new)    #
           LambdaiJi[[d]] = lambdaiJi(p.d[d,], XB, iJi[[d]])#
		   nonemptyiJi[[d]] = LambdaiJi[[d]][!is.na(LambdaiJi[[d]])]#
		   observediJi[[d]] = LambdaiJi[[d]][as.numeric(edge[[d]][1])]#
         }#
         prior.new1 = sum(vapply(1L:nIP, function(IP) {#
        		dmvnorm(Beta.new[[IP]], prior.b.mean, prior.b.var, log = TRUE)#
         }, c(1))) #
         post.new1 = sum(vapply(edge2, function(d) {#
    			    EdgeInEqZ(iJi[[d]], lambda[[d]], delta) + TimeInEqZ(nonemptyiJi[[d]], timeinc[d]) + #
    			    ObservedInEqZ(observediJi[[d]])#
    			    }, c(1))) / length(edge2)#
    		 loglike.diff = prior.new1 + post.new1 - prior.old1 - post.old1#
        if (log(runif(1, 0, 1)) < loglike.diff) {#
         for (IP in 1L:nIP) {#
         Beta.old[[IP]]  = Beta.new[[IP]]#
         }#
         prior.old1 = prior.new1#
         post.old1 = post.new1#
         }#
        	 eta.new = rnorm(1, eta, sigma_Q)#
    	 	 delta.new = pnorm(eta.new)#
		 prior.new2 = dbeta(delta.new, prior.delta[1], prior.delta[2], log = TRUE)#
		 post.new2 = sum(vapply(edge2, function(d) {#
    			    EdgeInEqZ(iJi[[d]], lambda[[d]], delta.new)#
    			    }, c(1))) / length(edge2)#
    	 	 loglike.diff2 = prior.new2 + post.new2 - prior.old2 - post.old2#
		  if (log(runif(1, 0, 1)) < loglike.diff2) {#
			delta = delta.new#
			eta = eta.new#
			prior.old2 = prior.new2#
			post.old2 = post.new2#
			}#
#
         if (i3 > burn & i3 %% (thin) == 0) {#
          for (IP in 1L:nIP) {#
           bmat[[IP]][ , (i3 - burn) / thin] = Beta.old[[IP]]#
           }#
           deltamat = c(deltamat, delta)#
          }#
         }#
	 }#
    if (plot) {#
    burnin = round(out / 10)#
    par(mfrow = c(2, 2))#
  	plot(entropy.mat[-1L:-burnin], type = "l", #
  	     xlab = "(Outer) Iterations", ylab = "Entropy of IP")#
  	abline(h = mean(entropy.mat[-1L:-burnin]), lty = 1)#
  	title("Convergence of Entropy")#
  	matplot(alpha.mat[-1L:-burnin,], lty = 1, type = "l", col = 1L:nIP, #
  	        xlab = "(Outer) Iterations", ylab = "alpha")#
  	abline(h = mean(alpha.mat[-1L:-burnin,]), lty = 1, col = 1L:nIP)#
	title("Convergence of Optimized alpha")#
	plot(logWZ.mat[-1L:-burnin], type = "l", #
	       xlab = "(Outer) Iterations", ylab = "logWZ")#
	abline(h = mean(logWZ.mat[-1L:-burnin]), lty = 1)#
	title("Convergence of logWZ")#
	matplot(bmat[[1]][1,], lty = 1, col = 1L:P, type = "l", #
	          main = "Traceplot of beta", xlab = "(Inner) Iterations", ylab = "")#
	abline(h = mean(bmat[[1]][1,]), lty = 1, col = 1L)#
	  }#
  chain.final = list(C = currentC, Z = lapply(edge2, function(d) {currentZ[[d]]}), B = bmat, D = deltamat)#
#
  return(chain.final)#
}#
#
#PP_Plots#
GiR_PP_Plots = function(Forward_stats, Backward_stats) {#
	nms = colnames(Forward_stats)#
	for (i in 1L:ncol(Forward_stats)) {#
		if (nrow(Forward_stats) > 20000) {#
			thin = seq(from = floor(nrow(Forward_stats)/10), to = nrow(Forward_stats), length.out = 10000)#
			Forward_test = Forward_stats[thin, i]#
			Backward_test = Backward_stats[thin, i]#
		} else {#
			Forward_test = Forward_stats[, i]#
			Backward_test = Backward_stats[, i]#
		}#
		all = c(Backward_stats[, i], Forward_stats[, i])#
		ylims = c(min(all) - 0.1 * max(abs(all)), max(all) + 0.1 * max(abs(all)))#
		xlims = ylims#
		quantiles = 50#
		if (grepl("B_", nms[i]) ) {#
			quantiles = 1000#
		}#
		qqplot(x = quantile(Forward_stats[, i], seq(0, 1, length = quantiles)),#
			   y = quantile(Backward_stats[, i], seq(0, 1, length = quantiles)),#
			   ylim = ylims,#
			   xlim = xlims,#
			   ylab = "Backward",#
			   xlab = "Forward",#
			   col = "blue",#
			   pch = 19,#
			   main = nms[i],#
			   cex.lab = 1,#
			   cex.axis = 1,#
			   cex.main = 1)#
		lines(x = xlims, y = ylims, col = "red", lwd = 3)#
		text(paste("Backward Mean:", round(mean(Backward_stats[,i]), 4),#
				   "\nForward Mean:", round(mean(Forward_stats[,i]), 4),#
				   "\nt-test p-value:", round(t.test(Backward_test, Forward_test)$p.value, 4),#
				   "\nMann-Whitney p-value:", round(wilcox.test(Backward_test, Forward_test)$p.value,4)),#
				   x = xlims[2] - 0.35 * abs(xlims[2] - xlims[1]),#
				   y = ylims[1] + 0.15 * abs(ylims[2] - ylims[1]),#
				   cex = 0.3)#
	}#
}      #
#PP_Plots#
GiR_PP_Plots2 = function(Forward_stats, Backward_stats) {#
	nms = colnames(Forward_stats)#
	for (i in 1L:ncol(Forward_stats)) {#
		if (nrow(Forward_stats) > 20000) {#
			thin = seq(from = floor(nrow(Forward_stats)/10), to = nrow(Forward_stats), length.out = 10000)#
			Forward_test = Forward_stats[thin, i]#
			Backward_test = Backward_stats[thin, i]#
		} else {#
			Forward_test = Forward_stats[, i]#
			Backward_test = Backward_stats[, i]#
		}#
		all = c(Backward_stats[, i], Forward_stats[, i])#
		ylims = c(min(all) - 0.1 * max(abs(all)), max(all) + 0.1 * max(abs(all)))#
		xlims = ylims#
		quantiles = 50#
		if (grepl("B_", nms[i]) ) {#
			quantiles = 1000#
		}#
		normalmean = mean(c(quantile(Forward_stats[, i], seq(0, 1, length = quantiles)), #
							quantile(Backward_stats[, i], seq(0, 1, length = quantiles))))#
		normalvar = sd(c(quantile(Forward_stats[, i], seq(0, 1, length = quantiles)), #
							quantile(Backward_stats[, i], seq(0, 1, length = quantiles))))#
#
		qqplot(x = pnorm(sort(quantile(Forward_stats[, i], seq(0, 1, length = quantiles))), normalmean, normalvar),#
			   y = pnorm(sort(quantile(Backward_stats[, i], seq(0, 1, length = quantiles))), normalmean, normalvar),#
			   ylim = pnorm(ylims, normalmean, normalvar),#
			   xlim = pnorm(xlims, normalmean, normalvar),#
			   ylab = "Backward",#
			   xlab = "Forward",#
			   col = "blue",#
			   pch = 19,#
			   main = nms[i],#
			   cex.lab = 1,#
			   cex.axis = 1,#
			   cex.main = 1)#
		lines(x = pnorm(xlims, normalmean, normalvar), y = pnorm(ylims, normalmean, normalvar), col = "red", lwd = 3)#
		text(paste("Backward Mean:", round(mean(Backward_stats[,i]), 4),#
				   "\nForward Mean:", round(mean(Forward_stats[,i]), 4),#
				   "\nt-test p-value:", round(t.test(Backward_test, Forward_test)$p.value, 4),#
				   "\nMann-Whitney p-value:", round(wilcox.test(Backward_test, Forward_test)$p.value,4)),#
				   x = pnorm(xlims[2], normalmean, normalvar) - 0.35 * abs(pnorm(xlims[2], normalmean, normalvar) - pnorm(xlims[1], normalmean, normalvar)),#
				   y = pnorm(ylims[1], normalmean, normalvar) + 0.15 * abs(pnorm(ylims[2], normalmean, normalvar) - pnorm(ylims[1], normalmean, normalvar)),#
				   cex = 0.3)#
	}#
}      #
# Getting_It_Right#
GiR = function(Nsamp = 5000, nDocs = 10, node = 1:4, vocabulary =  c("hi", "hello","bye", "mine", "what"), #
			   nIP = 2, K = 4, nwords = 4, alpha = 2, mvec = rep(1/4, 4), betas = 2, nvec = rep(1/5, 5), #
			   prior.b.mean = c(-3, rep(0, 6)), prior.b.var = diag(7), prior.delta = c(2, 2), sigma_Q = 0.25, #
			   niters = c(1, 1, 1, 50, 0, 1), netstat = "dyadic", generate_PP_plots = TRUE, seed = 1) {#
	set.seed(seed)#
	b = lapply(1L:nIP, function(IP) {#
			   c(rmvnorm(1, prior.b.mean, prior.b.var))#
			   })#
	delta = 	rbeta(1, prior.delta[1], prior.delta[2])#
	currentC = sample(1L:nIP, K, replace = TRUE)	 #
	base.data = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, seed, #
							 base.edge = list(), base.text = list(), base = TRUE)#
	base.edge = base.data$edge	   #
	base.text = base.data$text#
#
	#Forward sampling#
	Forward_stats = matrix(NA, nrow = Nsamp, ncol = 21)#
	colnames(Forward_stats) = c(paste0("B_",1:length(b[[1]])), "Mean_receipients", "Mean_timediff", "Mean_TopicIP", #
						  paste0("Tokens_in_IP_", 1:nIP), paste0("Tokens_in_Topic", 1:K), #
						  paste0("Tokens_in_Word", 1:length(vocabulary)))#
	for (i in 1:Nsamp) { #
		if (i %% 5000 == 0) {cat("Forward sampling", i, "\n")}#
		set.seed(i)#
		b = lapply(1:nIP, function(IP) {#
			c(rmvnorm(1, prior.b.mean, prior.b.var))#
			})#
		delta = rbeta(1, prior.delta[1], prior.delta[2])#
		currentC = sample(1L:nIP, K, replace = TRUE)#
		Forward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
									  base.edge = base.edge, base.text = base.text, seed, forward = TRUE) #
		Forward_stats[i, ] = GiR_stats(Forward_sample, K, currentC, vocabulary, forward = TRUE, backward = FALSE)#
	}#
	#Backward sampling#
	Backward_stats = matrix(NA, nrow = Nsamp, ncol = 21)#
	Backward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
								   base.edge = base.edge, base.text = base.text, seed, backward_init = TRUE) #
	for (i in 1:Nsamp) { #
		if (i %% 500 == 0) {cat("Backward sampling", i, "\n")}#
		seed = seed + 100#
		set.seed(seed)#
		Inference_samp = Inference(Backward_sample$edge, node, Backward_sample$text, vocabulary, nIP, K, sigma_Q, #
						  alpha, mvec, betas, nvec, prior.b.mean, prior.b.var, prior.delta,#
						  out = niters[1], n1 = niters[2], n2 = niters[3], n3 = niters[4], burn = niters[5], thin = niters[6], #
						  netstat, seed)#
		b = lapply(1:nIP, function(IP) {#
			rowMeans(Inference_samp$B[[IP]])#
		})#
		delta = mean(Inference_samp$D)#
		currentC = Inference_samp$C#
		topic_token_assignments = Inference_samp$Z#
		for (d in 1:length(topic_token_assignments)) {#
			names(topic_token_assignments[[d]]) = Backward_sample$text[[d + length(base.text)]]#
		}#
		topic_token_counts = tabulate(unlist(Inference_samp$Z), K)#
		word_type_topic_counts = matrix(NA , length(vocabulary), K)#
		for (w in 1:length(vocabulary)) {#
			word_type_w = which(unlist(Backward_sample$text[-(1:length(base.text))]) == w)#
			word_type_topic_counts[w, ] = tabulate(unlist(Inference_samp$Z)[word_type_w], K)#
		}#
		Backward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
								   	   base.edge = base.edge, base.text = base.text, seed, topic_token_assignments = topic_token_assignments, #
								   	   topic_token_counts = topic_token_counts, word_type_topic_counts = word_type_topic_counts, #
								   	   forward = FALSE, backward = TRUE)#
		Backward_stats[i, ] = GiR_stats(Backward_sample, K, currentC, vocabulary, forward = FALSE, backward = TRUE)#
	}#
	tstats = rep(0, ncol(Forward_stats))#
	wstats = rep(0, ncol(Forward_stats))#
	for (j in 1:ncol(Forward_stats)) {#
		thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 400)#
		Forward_test = Forward_stats[thin, j]#
		Backward_test = Backward_stats[thin, j]#
		tstats[j] = t.test(Backward_test, Forward_test)$p.value#
		wstats[j] = wilcox.test(Backward_test, Forward_test)$p.value#
	}#
	names(tstats) = names(wstats) = colnames(Forward_stats)						#
	if (generate_PP_plots) {#
		par(mfrow=c(3,7), oma = c(3,3,3,3), mar = c(5,5,4,1))#
		GiR_PP_Plots2(Forward_stats, Backward_stats)#
	}			#
	return(list(Forward = Forward_stats, Backward = Backward_stats, tstats = tstats, wstats = wstats))	#
}#
#
unix.time(TryGiR <- GiR(10^5, seed = 15))
head(TryGiR$Forward)
head(TryGiR$Backward)
TryGiR$tstats
TryGiR$wstats
library(mvtnorm)#
library(MCMCpack)#
library(entropy)#
library(Rcpp)#
library(RcppArmadillo)#
sourceCpp('~/Desktop/IPTM-master/pkg/src/sampler.cpp')#
source('~/Desktop/IPTM-master/pkg/R/core.R')#
GenerateDocs = function(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec,#
						b, delta, currentC, netstat, base.edge, base.text, seed,#
						topic_token_assignments = NULL, topic_token_counts = NULL, word_type_topic_counts = NULL, #
						forward = FALSE, backward_init = FALSE, backward = FALSE, base = FALSE) {#
	set.seed(seed)#
 	W = length(vocabulary)#
  	phi = lapply(1L:K, function(k) {#
		rdirichlet_cpp(1, betas * nvec)#
	})#
	L = 3#
    P = 1 + L * (2 * ("dyadic" %in% netstat) + 4 * ("triadic" %in% netstat) + 2 *("degree" %in% netstat))#
    if (base) {#
    	t.d = 0#
    } else {#
    	t.d = 384#
    }#
	edge = base.edge#
	text = base.text#
	p.d = matrix(NA, nrow = nDocs, ncol = nIP)#
	options(warn = -1)#
	for (d in 1:nDocs) {#
		N.d = nwords#
		text[[length(base.text) + d]] = rep(NA, N.d)#
		if (!backward) {#
			theta.d = rdirichlet_cpp(1, alpha * mvec)	#
			topic.d = multinom_vec(max(1, N.d), theta.d)#
			if (N.d > 0) {#
				for (n in 1:N.d){#
					text[[length(base.text) + d]][n] = multinom_vec(1, phi[[topic.d[n]]])#
				}#
				names(text[[length(base.text) + d]]) = topic.d#
			}#
		} else {#
			phi.k = rep(NA, K)#
			topic.d = topic_token_assignments[[d]]#
			word.d = as.numeric(names(topic_token_assignments[[d]]))#
			if (N.d > 0) {#
				for (n in 1:N.d){#
					word_type_topic_counts[word.d [n], topic.d[n]] = word_type_topic_counts[word.d [n], topic.d[n]] - 1#
					for (w in 1:W) {#
						phi.k[w] = (word_type_topic_counts[w, topic.d[n]] + betas * nvec[w])/(topic_token_counts[topic.d[n]] + betas)#
					} #
					text[[length(base.text) + d]][n] = multinom_vec(1, phi.k)#
					word_type_topic_counts[text[[d]][n], topic.d[n]] = word_type_topic_counts[text[[d]][n], topic.d[n]] + 1#
				}#
				names(text[[length(base.text) + d]]) = topic.d#
			}#
		}#
#
		p.d[d, ] = vapply(1L:nIP, function(IP) {#
			sum(topic.d %in% which(currentC == IP))#
			}, c(1)) / max(1, N.d)#
		if (base & t.d < 384) {#
			history.t = lapply(1:nIP, function(IP) {#
				lapply(1:3, function(l){#
				matrix(0, length(node), length(node))#
				})#
			})#
		} else {	#
		history.t = History(edge, p.d, node, t.d + 10^(-10))#
		}#
		X = lapply(node, function(i) {#
  	        Netstats(history.t, node, i, netstat)#
            })#
   	 	XB = MultiplyXBList(X, b)     #
		lambda = lambda_cpp(p.d[d,], XB)#
		iJi = rbinom_mat((delta * lambda) / (delta * lambda + 1))#
		while (sum(iJi) == 0) {#
			iJi = rbinom_mat((delta * lambda) / (delta * lambda + 1))#
		}#
		LambdaiJi = lambdaiJi(p.d[d,], XB, iJi)#
		Time.inc = vapply(LambdaiJi, function(lambda) {#
			rexp(1, lambda)#
			}, c(1))#
		i.d = which(Time.inc == min(Time.inc[!is.na(Time.inc)]))#
		j.d = which(iJi[i.d,] == 1)#
		t.d = t.d + Time.inc[i.d]#
		edge[[length(base.edge) + d]] = list(sender = i.d, receiver = j.d, timestamp = t.d)		#
		}#
	options(warn = 0)#
	if (forward) {#
		edge = edge[-(1:length(base.edge))]#
		text = text[-(1:length(base.text))]#
	}#
	if (base == TRUE & t.d > 384) {#
		cutoff = which_int(384, vapply(1:length(edge), function(d) {edge[[d]][[3]]}, c(1))) - 1#
		edge = edge[1:cutoff]#
		text = text[1:cutoff]#
	}#
	return(list(edge = edge, text = text, b = b, base = length(base.edge)))							#
} #
GiR_stats = function(GiR_sample, K, currentC, vocabulary, forward = FALSE, backward = FALSE) {#
	edge = GiR_sample$edge#
	text = GiR_sample$text#
	if (backward) {#
		edge = edge[-(1:GiR_sample$base)]#
		text = text[-(1:GiR_sample$base)]#
	}#
	GiR_stats = c()#
	nDocs = length(edge)#
	P = length(GiR_sample$b[[1]])#
	nIP = length(GiR_sample$b)#
	nwords = length(GiR_sample$text[[1]])#
	W = length(vocabulary)#
	GiR_stats[1:P] = Reduce('+', GiR_sample$b) / nIP#
	GiR_stats[P + 1] = mean(vapply(1:nDocs, function(d) {#
						length(edge[[d]][[2]])#
						}, c(1)))#
	GiR_stats[P + 2] = mean(vapply(2:nDocs, function(d) {#
					 edge[[d]][[3]] - edge[[d-1]][[3]]#
					 }, c(1))) 			#
	GiR_stats[P + 3] = mean(currentC)#
	Tokens_in_Topic = tabulate(vapply(1:nDocs, function(d){#
					  as.numeric(names(text[[d]]))#
					  }, rep(0, nwords)), K)#
	GiR_stats[(P + 4):(P + 3 + nIP)] = vapply(1:nIP, function(IP) {#
								 Tokens_in_Topic %*% (currentC == IP)#
								 }, c(1))#
	GiR_stats[(P + 4 + nIP):(P + 3 + nIP + K)] = Tokens_in_Topic#
	Tokens_in_Word = tabulate(vapply(1:nDocs, function(d){#
					 text[[d]]#
					 }, rep(0, nwords)), W)#
	GiR_stats[(P + 4 + nIP + K):(P + 3 + nIP + K + W)] = Tokens_in_Word#
	return(GiR_stats)#
}#
Inference = function(edge, node, textlist, vocabulary, nIP, K, sigma_Q, alpha, mvec, betas, nvec, prior.b.mean, prior.b.var, prior.delta, #
					out, n1, n2, n3, burn, thin, netstat, seed, plot = FALSE) {#
  set.seed(seed)#
  # trim the edge so that we only model edges after 16 days#
	timeinc = c(as.numeric(edge[[1]][3]), vapply(seq(along = edge)[-1], function(d) {#
  	as.numeric(edge[[d]][3]) - as.numeric(edge[[d-1]][3])#
 	}, c(1)))#
#
    edge2 = which_int(384, cumsum(timeinc)) : length(edge)#
  # initialize alpha, mvec, delta, nvec, eta, lvec, and gammas#
 	W = length(vocabulary)#
  	phi = lapply(1L:K, function(k) {#
		rdirichlet_cpp(1, betas * nvec)#
	})#
	delta = rbeta(1, prior.delta[1], prior.delta[2])#
	deltamat = delta#
	eta = qnorm(delta)#
	# initialize C, theta and Z#
	currentC = sample(1:nIP, K, replace = TRUE)#
	theta = rdirichlet_cpp(length(edge), alpha * mvec)#
	currentZ = lapply(seq(along = edge), function(d) {#
    if (length(textlist[[d]]) > 0) {#
    		multinom_vec(length(textlist[[d]]), theta[d, ])#
    	} else {#
    		multinom_vec(1, theta[d, ])#
    	}#
    })#
	p.d = t(vapply(seq(along = edge), function(d) {#
	  vapply(1L:nIP, function(IP) {#
	    sum(currentZ[[d]] %in% which(currentC == IP))#
	  }, c(1)) / length(currentZ[[d]])#
	}, rep(1, nIP)))#
#
    # initialize beta#
    sigma = 1#
    L = 3#
    P = 1 + L * (2 * ("dyadic" %in% netstat) + 4 * ("triadic" %in% netstat) + 2 *("degree" %in% netstat))#
    bmat = list()#
	for (IP in 1L:nIP) {#
		bmat[[IP]] = matrix(0, nrow = P, ncol = (n3 - burn) / thin)#
		bmat[[IP]][, 1:(n3 - burn) / thin] = c(rmvnorm(1, prior.b.mean, prior.b.var))#
  	}#
#
    # to check the convergence  #
    if (plot) {#
     	logWZ.mat = c()							  #
     	alpha.mat = c()#
     	entropy.mat = c()#
    }#
    iJi = lapply(seq(along = edge), function(d) {#
  	      matrix(0, nrow = length(node), ncol = length(node))#
  	      })  #
    #start outer iteration#
    for (o in 1L:out) {#
      Beta.old = lapply(bmat, function(b) {#
      			rowMeans(b)#
         		})  #
     # Data augmentation#
      lambda = list()#
      LambdaiJi = list()#
      nonemptyiJi = list()#
	  observediJi = list()#
      for (d in edge2) {#
   	 	history.t = History(edge, p.d, node, as.numeric(edge[[d]][3]))#
   	 	X = lapply(node, function(i) {#
  	        Netstats(history.t, node, i, netstat)#
            })#
   	 	XB = MultiplyXBList(X, Beta.old)     #
		lambda[[d]] = lambda_cpp(p.d[d,], XB)#
		#calculate the resampling probability#
		probij = DataAug_cpp(iJi[[d]], lambda[[d]], delta, timeinc[d])#
		iJi[[d]] = rbinom_mat(probij)#
		iJi[[d]][as.numeric(edge[[d]][1]),] = tabulateC(as.numeric(unlist(edge[[d]][2])), length(node))#
		LambdaiJi[[d]] = lambdaiJi(p.d[d,], XB, iJi[[d]])#
		nonemptyiJi[[d]] = LambdaiJi[[d]][!is.na(LambdaiJi[[d]])]#
		observediJi[[d]] = LambdaiJi[[d]][as.numeric(edge[[d]][1])]#
		}	 #
    textlist.raw = unlist(textlist)#
    table.W = lapply(1L:K, function(k) {#
      tabulateC(textlist.raw[which(unlist(currentZ) == k)], W)#
      })#
    for (i1 in 1L:n1) {#
      for (d in edge2) { #
        textlist.d = textlist[[d]]#
        if (length(textlist.d) > 0) {#
        	topicpart.d = TopicInEqZ(K, currentZ[[d]], alpha, mvec, d)#
       	wordpart.d = WordInEqZ(K, textlist.d, table.W, betas, nvec)#
        } else {#
        topicpart.d = 0#
        wordpart.d = matrix(0, nrow = length(currentZ[[d]]), ncol = K)#
        }#
        edgepart.d = EdgeInEqZ(iJi[[d]], lambda[[d]], delta)#
        timepart.d = TimeInEqZ(nonemptyiJi[[d]], timeinc[d])#
        observed.d = ObservedInEqZ(observediJi[[d]]) #
        fixedpart = topicpart.d + edgepart.d + timepart.d + observed.d #
        for (w in 1L:length(currentZ[[d]])) {#
          const.Z = fixedpart + wordpart.d[w, ]#
          const.Z = const.Z - max(const.Z)#
          zw.old = currentZ[[d]][w]#
          zw.new = multinom_vec(1, exp(const.Z))#
          if (zw.new != zw.old) {#
            currentZ[[d]][w] = zw.new#
            topicpart.d = TopicInEqZ(K, currentZ[[d]], alpha, mvec, d)#
            if (length(textlist.d) > 0) {	#
            wordpart.d = WordInEqZ(K, textlist.d, table.W, betas, nvec)#
            }#
            table.W = lapply(1L:K, function(k) {#
      				  tabulateC(textlist.raw[which(unlist(currentZ) == k)], W)#
      				  })#
      		p.d[d, ] = vapply(1L:nIP, function(IP) {#
	 			sum(currentZ[[d]] %in% which(currentC == IP))#
	 			}, c(1)) / length(currentZ[[d]])#
      		LambdaiJi[[d]] = lambdaiJi(p.d[d,], XB, iJi[[d]])#
			nonemptyiJi[[d]] = LambdaiJi[[d]][!is.na(LambdaiJi[[d]])]#
         	observediJi[[d]] = LambdaiJi[[d]][as.numeric(edge[[d]][1])]#
          }#
        }#
       }#
      }#
#
    for (i2 in 1L:n2) {#
      # C update given Z and B - within each document d#
      for (k in unique(unlist(currentZ[edge2]))) { #
        document.k = which(vapply(currentZ, function(d){k %in% d}, c(1)) == 1)#
        document.k = document.k[document.k %in% edge2]#
        const.C = rep(NA, nIP)#
        for (IP in 1L:nIP) {#
        	if (!currentC[k] == IP){#
          currentC[k] = IP#
          for (d in document.k) {#
            p.d[d, ] =  vapply(1L:nIP, function(IP) {#
              sum(currentZ[[d]] %in% which(currentC == IP))#
            }, c(1)) / length(currentZ[[d]])#
          }#
          for (d in edge2) {#
           history.t = History(edge, p.d, node, as.numeric(edge[[d]][3]))#
    	   	   X = lapply(node, function(i) {#
               Netstats(history.t, node, i, netstat)#
               })#
    	       XB = MultiplyXBList(X, Beta.old)    #
           lambda[[d]] = lambda_cpp(p.d[d,], XB)#
		   LambdaiJi[[d]] = lambdaiJi(p.d[d,], XB, iJi[[d]])#
		   nonemptyiJi[[d]] = LambdaiJi[[d]][!is.na(LambdaiJi[[d]])]#
           observediJi[[d]] = LambdaiJi[[d]][as.numeric(edge[[d]][1])]#
          }#
          }#
          const.C[IP] = sum(vapply(document.k, function(d) {#
            EdgeInEqZ(iJi[[d]], lambda[[d]], delta) + TimeInEqZ(nonemptyiJi[[d]], timeinc[d]) + #
    			ObservedInEqZ(observediJi[[d]]) #
          }, c(1))) / length(document.k)#
          }#
          const.C = const.C - max(const.C)#
          currentC[k] = multinom_vec(1, exp(const.C))#
      }#
    }#
    if (plot) {#
      entropy.mat = c(entropy.mat, entropy.empirical(currentC))#
      alpha.mat = rbind(alpha.mat, alpha)#
      logWZ.mat = c(logWZ.mat, logWZ(K, currentZ, textlist, table.W, alpha, mvec, betas, nvec))#
      }#
     for (d in edge2) {#
     	 p.d[d, ] =  vapply(1L:nIP, function(IP) {#
              		 sum(currentZ[[d]] %in% which(currentC == IP))#
                     }, c(1)) / length(currentZ[[d]])#
     	 history.t = History(edge, p.d, node, as.numeric(edge[[d]][3]))#
    	     X = lapply(node, function(i) {#
              Netstats(history.t, node, i, netstat)#
             })#
    	     XB = MultiplyXBList(X, Beta.old)   #
    	     lambda[[d]] = lambda_cpp(p.d[d,], XB)#
		 LambdaiJi[[d]] = lambdaiJi(p.d[d,], XB, iJi[[d]])#
		 nonemptyiJi[[d]] = LambdaiJi[[d]][!is.na(LambdaiJi[[d]])]#
         observediJi[[d]] = LambdaiJi[[d]][as.numeric(edge[[d]][1])]#
	 }#
	 # Beta and delta update#
	 prior.old1 = sum(vapply(1L:nIP, function(IP) {#
		  		 dmvnorm(Beta.old[[IP]], prior.b.mean, prior.b.var, log = TRUE)#
		  		 }, c(1))) #
	 post.old1 = sum(vapply(edge2, function(d) {#
	     	     EdgeInEqZ(iJi[[d]], lambda[[d]], delta) + TimeInEqZ(nonemptyiJi[[d]], timeinc[d]) +#
    			 	 ObservedInEqZ(observediJi[[d]])#
    			 }, c(1))) / length(edge2)#
    	 prior.old2 = dbeta(delta, prior.delta[1], prior.delta[2], log = TRUE) #
     post.old2 = sum(vapply(edge2, function(d) {#
    			    EdgeInEqZ(iJi[[d]], lambda[[d]], delta)#
    			    }, c(1))) / length(edge2)#
	 options(warn = -1)#
	 proposal.var = list()#
     for (IP in 1L:nIP) {#
     	 proposal.var[[IP]] = cor(t(bmat[[IP]]))#
         proposal.var[[IP]][is.na(proposal.var[[IP]])] = 0#
         if (sum(eigen(proposal.var[[IP]])$values < 0 ) > 0) {#
        	 proposal.var[[IP]] = diag(P)#
         }#
         }#
     proposal.var = lapply(1:nIP, function(IP){diag(P)})#
     options(warn = 0)#
     for (i3 in 1L:n3) {#
     	Beta.new = lapply(1L:nIP, function(IP) {#
           rmvnorm(1, Beta.old[[IP]], sigma_Q * proposal.var[[IP]])#
         }) #
        for (d in edge2) {#
           XB = MultiplyXBList(X, Beta.new)    #
           LambdaiJi[[d]] = lambdaiJi(p.d[d,], XB, iJi[[d]])#
		   nonemptyiJi[[d]] = LambdaiJi[[d]][!is.na(LambdaiJi[[d]])]#
		   observediJi[[d]] = LambdaiJi[[d]][as.numeric(edge[[d]][1])]#
         }#
         prior.new1 = sum(vapply(1L:nIP, function(IP) {#
        		dmvnorm(Beta.new[[IP]], prior.b.mean, prior.b.var, log = TRUE)#
         }, c(1))) #
         post.new1 = sum(vapply(edge2, function(d) {#
    			    EdgeInEqZ(iJi[[d]], lambda[[d]], delta) + TimeInEqZ(nonemptyiJi[[d]], timeinc[d]) + #
    			    ObservedInEqZ(observediJi[[d]])#
    			    }, c(1))) / length(edge2)#
    		 loglike.diff = prior.new1 + post.new1 - prior.old1 - post.old1#
        if (log(runif(1, 0, 1)) < loglike.diff) {#
         for (IP in 1L:nIP) {#
         Beta.old[[IP]]  = Beta.new[[IP]]#
         }#
         prior.old1 = prior.new1#
         post.old1 = post.new1#
         }#
        	 eta.new = rnorm(1, eta, sigma_Q)#
    	 	 delta.new = pnorm(eta.new)#
		 prior.new2 = dbeta(delta.new, prior.delta[1], prior.delta[2], log = TRUE)#
		 post.new2 = sum(vapply(edge2, function(d) {#
    			    EdgeInEqZ(iJi[[d]], lambda[[d]], delta.new)#
    			    }, c(1))) / length(edge2)#
    	 	 loglike.diff2 = prior.new2 + post.new2 - prior.old2 - post.old2#
		  if (log(runif(1, 0, 1)) < loglike.diff2) {#
			delta = delta.new#
			eta = eta.new#
			prior.old2 = prior.new2#
			post.old2 = post.new2#
			}#
#
         if (i3 > burn & i3 %% (thin) == 0) {#
          for (IP in 1L:nIP) {#
           bmat[[IP]][ , (i3 - burn) / thin] = Beta.old[[IP]]#
           }#
           deltamat = c(deltamat, delta)#
          }#
         }#
	 }#
    if (plot) {#
    burnin = round(out / 10)#
    par(mfrow = c(2, 2))#
  	plot(entropy.mat[-1L:-burnin], type = "l", #
  	     xlab = "(Outer) Iterations", ylab = "Entropy of IP")#
  	abline(h = mean(entropy.mat[-1L:-burnin]), lty = 1)#
  	title("Convergence of Entropy")#
  	matplot(alpha.mat[-1L:-burnin,], lty = 1, type = "l", col = 1L:nIP, #
  	        xlab = "(Outer) Iterations", ylab = "alpha")#
  	abline(h = mean(alpha.mat[-1L:-burnin,]), lty = 1, col = 1L:nIP)#
	title("Convergence of Optimized alpha")#
	plot(logWZ.mat[-1L:-burnin], type = "l", #
	       xlab = "(Outer) Iterations", ylab = "logWZ")#
	abline(h = mean(logWZ.mat[-1L:-burnin]), lty = 1)#
	title("Convergence of logWZ")#
	matplot(bmat[[1]][1,], lty = 1, col = 1L:P, type = "l", #
	          main = "Traceplot of beta", xlab = "(Inner) Iterations", ylab = "")#
	abline(h = mean(bmat[[1]][1,]), lty = 1, col = 1L)#
	  }#
  chain.final = list(C = currentC, Z = lapply(edge2, function(d) {currentZ[[d]]}), B = bmat, D = deltamat)#
#
  return(chain.final)#
}#
#
#PP_Plots#
GiR_PP_Plots = function(Forward_stats, Backward_stats) {#
	nms = colnames(Forward_stats)#
	for (i in 1L:ncol(Forward_stats)) {#
		if (nrow(Forward_stats) > 20000) {#
			thin = seq(from = floor(nrow(Forward_stats)/10), to = nrow(Forward_stats), length.out = 10000)#
			Forward_test = Forward_stats[thin, i]#
			Backward_test = Backward_stats[thin, i]#
		} else {#
			Forward_test = Forward_stats[, i]#
			Backward_test = Backward_stats[, i]#
		}#
		all = c(Backward_stats[, i], Forward_stats[, i])#
		ylims = c(min(all) - 0.1 * max(abs(all)), max(all) + 0.1 * max(abs(all)))#
		xlims = ylims#
		quantiles = 50#
		if (grepl("B_", nms[i]) ) {#
			quantiles = 1000#
		}#
		qqplot(x = quantile(Forward_stats[, i], seq(0, 1, length = quantiles)),#
			   y = quantile(Backward_stats[, i], seq(0, 1, length = quantiles)),#
			   ylim = ylims,#
			   xlim = xlims,#
			   ylab = "Backward",#
			   xlab = "Forward",#
			   col = "blue",#
			   pch = 19,#
			   main = nms[i],#
			   cex.lab = 1,#
			   cex.axis = 1,#
			   cex.main = 1)#
		lines(x = xlims, y = ylims, col = "red", lwd = 3)#
		text(paste("Backward Mean:", round(mean(Backward_stats[,i]), 4),#
				   "\nForward Mean:", round(mean(Forward_stats[,i]), 4),#
				   "\nt-test p-value:", round(t.test(Backward_test, Forward_test)$p.value, 4),#
				   "\nMann-Whitney p-value:", round(wilcox.test(Backward_test, Forward_test)$p.value,4)),#
				   x = xlims[2] - 0.35 * abs(xlims[2] - xlims[1]),#
				   y = ylims[1] + 0.15 * abs(ylims[2] - ylims[1]),#
				   cex = 0.3)#
	}#
}      #
#PP_Plots#
GiR_PP_Plots2 = function(Forward_stats, Backward_stats) {#
	nms = colnames(Forward_stats)#
	for (i in 1L:ncol(Forward_stats)) {#
		if (nrow(Forward_stats) > 20000) {#
			thin = seq(from = floor(nrow(Forward_stats)/10), to = nrow(Forward_stats), length.out = 10000)#
			Forward_test = Forward_stats[thin, i]#
			Backward_test = Backward_stats[thin, i]#
		} else {#
			Forward_test = Forward_stats[, i]#
			Backward_test = Backward_stats[, i]#
		}#
		all = c(Backward_stats[, i], Forward_stats[, i])#
		ylims = c(min(all) - 0.1 * max(abs(all)), max(all) + 0.1 * max(abs(all)))#
		xlims = ylims#
		quantiles = 50#
		if (grepl("B_", nms[i]) ) {#
			quantiles = 1000#
		}#
		normalmean = mean(c(quantile(Forward_stats[, i], seq(0, 1, length = quantiles)), #
							quantile(Backward_stats[, i], seq(0, 1, length = quantiles))))#
		normalvar = sd(c(quantile(Forward_stats[, i], seq(0, 1, length = quantiles)), #
							quantile(Backward_stats[, i], seq(0, 1, length = quantiles))))#
#
		qqplot(x = pnorm(sort(quantile(Forward_stats[, i], seq(0, 1, length = quantiles))), normalmean, normalvar),#
			   y = pnorm(sort(quantile(Backward_stats[, i], seq(0, 1, length = quantiles))), normalmean, normalvar),#
			   ylim = pnorm(ylims, normalmean, normalvar),#
			   xlim = pnorm(xlims, normalmean, normalvar),#
			   ylab = "Backward",#
			   xlab = "Forward",#
			   col = "blue",#
			   pch = 19,#
			   main = nms[i],#
			   cex.lab = 1,#
			   cex.axis = 1,#
			   cex.main = 1)#
		lines(x = pnorm(xlims, normalmean, normalvar), y = pnorm(ylims, normalmean, normalvar), col = "red", lwd = 3)#
		text(paste("Backward Mean:", round(mean(Backward_stats[,i]), 4),#
				   "\nForward Mean:", round(mean(Forward_stats[,i]), 4),#
				   "\nt-test p-value:", round(t.test(Backward_test, Forward_test)$p.value, 4),#
				   "\nMann-Whitney p-value:", round(wilcox.test(Backward_test, Forward_test)$p.value,4)),#
				   x = pnorm(xlims[2], normalmean, normalvar) - 0.35 * abs(pnorm(xlims[2], normalmean, normalvar) - pnorm(xlims[1], normalmean, normalvar)),#
				   y = pnorm(ylims[1], normalmean, normalvar) + 0.15 * abs(pnorm(ylims[2], normalmean, normalvar) - pnorm(ylims[1], normalmean, normalvar)),#
				   cex = 0.3)#
	}#
}      #
# Getting_It_Right#
GiR = function(Nsamp = 5000, nDocs = 5, node = 1:4, vocabulary =  c("hi", "hello","bye", "mine", "what"), #
			   nIP = 2, K = 4, nwords = 4, alpha = 2, mvec = rep(1/4, 4), betas = 2, nvec = rep(1/5, 5), #
			   prior.b.mean = c(-3, rep(0, 6)), prior.b.var = diag(7), prior.delta = c(2, 2), sigma_Q = 0.25, #
			   niters = c(1, 1, 1, 50, 0, 1), netstat = "dyadic", generate_PP_plots = TRUE, seed = 1) {#
	set.seed(seed)#
	b = lapply(1L:nIP, function(IP) {#
			   c(rmvnorm(1, prior.b.mean, prior.b.var))#
			   })#
	delta = 	rbeta(1, prior.delta[1], prior.delta[2])#
	currentC = sample(1L:nIP, K, replace = TRUE)	 #
	base.data = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, seed, #
							 base.edge = list(), base.text = list(), base = TRUE)#
	base.edge = base.data$edge	   #
	base.text = base.data$text#
#
	#Forward sampling#
	Forward_stats = matrix(NA, nrow = Nsamp, ncol = 21)#
	colnames(Forward_stats) = c(paste0("B_",1:length(b[[1]])), "Mean_receipients", "Mean_timediff", "Mean_TopicIP", #
						  paste0("Tokens_in_IP_", 1:nIP), paste0("Tokens_in_Topic", 1:K), #
						  paste0("Tokens_in_Word", 1:length(vocabulary)))#
	for (i in 1:Nsamp) { #
		if (i %% 5000 == 0) {cat("Forward sampling", i, "\n")}#
		set.seed(i)#
		b = lapply(1:nIP, function(IP) {#
			c(rmvnorm(1, prior.b.mean, prior.b.var))#
			})#
		delta = rbeta(1, prior.delta[1], prior.delta[2])#
		currentC = sample(1L:nIP, K, replace = TRUE)#
		Forward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
									  base.edge = base.edge, base.text = base.text, seed, forward = TRUE) #
		Forward_stats[i, ] = GiR_stats(Forward_sample, K, currentC, vocabulary, forward = TRUE, backward = FALSE)#
	}#
	#Backward sampling#
	Backward_stats = matrix(NA, nrow = Nsamp, ncol = 21)#
	Backward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
								   base.edge = base.edge, base.text = base.text, seed, backward_init = TRUE) #
	for (i in 1:Nsamp) { #
		if (i %% 500 == 0) {cat("Backward sampling", i, "\n")}#
		seed = seed + 100#
		set.seed(seed)#
		Inference_samp = Inference(Backward_sample$edge, node, Backward_sample$text, vocabulary, nIP, K, sigma_Q, #
						  alpha, mvec, betas, nvec, prior.b.mean, prior.b.var, prior.delta,#
						  out = niters[1], n1 = niters[2], n2 = niters[3], n3 = niters[4], burn = niters[5], thin = niters[6], #
						  netstat, seed)#
		b = lapply(1:nIP, function(IP) {#
			rowMeans(Inference_samp$B[[IP]])#
		})#
		delta = mean(Inference_samp$D)#
		currentC = Inference_samp$C#
		topic_token_assignments = Inference_samp$Z#
		for (d in 1:length(topic_token_assignments)) {#
			names(topic_token_assignments[[d]]) = Backward_sample$text[[d + length(base.text)]]#
		}#
		topic_token_counts = tabulate(unlist(Inference_samp$Z), K)#
		word_type_topic_counts = matrix(NA , length(vocabulary), K)#
		for (w in 1:length(vocabulary)) {#
			word_type_w = which(unlist(Backward_sample$text[-(1:length(base.text))]) == w)#
			word_type_topic_counts[w, ] = tabulate(unlist(Inference_samp$Z)[word_type_w], K)#
		}#
		Backward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
								   	   base.edge = base.edge, base.text = base.text, seed, topic_token_assignments = topic_token_assignments, #
								   	   topic_token_counts = topic_token_counts, word_type_topic_counts = word_type_topic_counts, #
								   	   forward = FALSE, backward = TRUE)#
		Backward_stats[i, ] = GiR_stats(Backward_sample, K, currentC, vocabulary, forward = FALSE, backward = TRUE)#
	}#
	tstats = rep(0, ncol(Forward_stats))#
	wstats = rep(0, ncol(Forward_stats))#
	for (j in 1:ncol(Forward_stats)) {#
		thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 400)#
		Forward_test = Forward_stats[thin, j]#
		Backward_test = Backward_stats[thin, j]#
		tstats[j] = t.test(Backward_test, Forward_test)$p.value#
		wstats[j] = wilcox.test(Backward_test, Forward_test)$p.value#
	}#
	names(tstats) = names(wstats) = colnames(Forward_stats)						#
	if (generate_PP_plots) {#
		par(mfrow=c(3,7), oma = c(3,3,3,3), mar = c(5,5,4,1))#
		GiR_PP_Plots2(Forward_stats, Backward_stats)#
	}			#
	return(list(Forward = Forward_stats, Backward = Backward_stats, tstats = tstats, wstats = wstats))	#
}#
#
unix.time(TryGiR <- GiR(5*10^5, seed = 12))
21863*2
21863*2/3600
#PP_Plots#
GiR_PP_Plots2 = function(Forward_stats, Backward_stats) {#
	nms = colnames(Forward_stats)#
	for (i in 1L:ncol(Forward_stats)) {#
		if (nrow(Forward_stats) > 20000) {#
			thin = seq(from = floor(nrow(Forward_stats)/10), to = nrow(Forward_stats), length.out = 10000)#
			Forward_test = Forward_stats[thin, i]#
			Backward_test = Backward_stats[thin, i]#
		} else {#
			Forward_test = Forward_stats[, i]#
			Backward_test = Backward_stats[, i]#
		}#
		all = c(Backward_stats[, i], Forward_stats[, i])#
		ylims = c(min(all) - 0.1 * max(abs(all)), max(all) + 0.1 * max(abs(all)))#
		xlims = ylims#
		quantiles = 50#
		if (grepl("B_", nms[i]) ) {#
			quantiles = 1000#
		}#
		normalmean = mean(c(quantile(Forward_stats[, i], seq(0, 1, length = quantiles)), #
							quantile(Backward_stats[, i], seq(0, 1, length = quantiles))))#
		normalvar = sd(c(quantile(Forward_stats[, i], seq(0, 1, length = quantiles)), #
							quantile(Backward_stats[, i], seq(0, 1, length = quantiles))))#
#
		qqplot(x = pnorm(sort(quantile(Forward_stats[, i], seq(0, 1, length = quantiles))), normalmean, normalvar),#
			   y = pnorm(sort(quantile(Backward_stats[, i], seq(0, 1, length = quantiles))), normalmean, normalvar),#
			   ylim = pnorm(ylims, normalmean, normalvar),#
			   xlim = pnorm(xlims, normalmean, normalvar),#
			   ylab = "Backward",#
			   xlab = "Forward",#
			   col = "blue",#
			   pch = 19,#
			   main = nms[i],#
			   cex.lab = 1,#
			   cex.axis = 1,#
			   cex.main = 1)#
		lines(x = pnorm(xlims, normalmean, normalvar), y = pnorm(ylims, normalmean, normalvar), col = "red", lwd = 1)#
		text(paste("Backward Mean:", round(mean(Backward_stats[,i]), 4),#
				   "\nForward Mean:", round(mean(Forward_stats[,i]), 4),#
				   "\nt-test p-value:", round(t.test(Backward_test, Forward_test)$p.value, 4),#
				   "\nMann-Whitney p-value:", round(wilcox.test(Backward_test, Forward_test)$p.value,4)),#
				   x = pnorm(xlims[2], normalmean, normalvar) - 0.35 * abs(pnorm(xlims[2], normalmean, normalvar) - pnorm(xlims[1], normalmean, normalvar)),#
				   y = pnorm(ylims[1], normalmean, normalvar) + 0.15 * abs(pnorm(ylims[2], normalmean, normalvar) - pnorm(ylims[1], normalmean, normalvar)),#
				   cex = 0.3)#
	}#
}
par(mfrow=c(5,5), oma = c(3,3,3,3), mar = c(5,5,4,1))#
		GiR_PP_Plots2(TryGiR$Forward, TryGiR$Backward)
par(mfrow=c(5,5), oma = c(3,3,3,3), mar = c(0,0,4,1))#
		GiR_PP_Plots2(TryGiR$Forward, TryGiR$Backward)
par(mfrow=c(5,5), oma = c(3,3,3,3), mar = c(1,1,1,1))#
		GiR_PP_Plots2(TryGiR$Forward, TryGiR$Backward)
#PP_Plots#
GiR_PP_Plots2 = function(Forward_stats, Backward_stats) {#
	nms = colnames(Forward_stats)#
	for (i in 1L:ncol(Forward_stats)) {#
		if (nrow(Forward_stats) > 20000) {#
			thin = seq(from = floor(nrow(Forward_stats)/10), to = nrow(Forward_stats), length.out = 10000)#
			Forward_test = Forward_stats[thin, i]#
			Backward_test = Backward_stats[thin, i]#
		} else {#
			Forward_test = Forward_stats[, i]#
			Backward_test = Backward_stats[, i]#
		}#
		all = c(Backward_stats[, i], Forward_stats[, i])#
		ylims = c(min(all) - 0.1 * max(abs(all)), max(all) + 0.1 * max(abs(all)))#
		xlims = ylims#
		quantiles = 50#
		if (grepl("B_", nms[i]) ) {#
			quantiles = 1000#
		}#
		normalmean = mean(c(quantile(Forward_stats[, i], seq(0, 1, length = quantiles)), #
							quantile(Backward_stats[, i], seq(0, 1, length = quantiles))))#
		normalvar = sd(c(quantile(Forward_stats[, i], seq(0, 1, length = quantiles)), #
							quantile(Backward_stats[, i], seq(0, 1, length = quantiles))))#
#
		qqplot(x = pnorm(sort(quantile(Forward_stats[, i], seq(0, 1, length = quantiles))), normalmean, normalvar),#
			   y = pnorm(sort(quantile(Backward_stats[, i], seq(0, 1, length = quantiles))), normalmean, normalvar),#
			   ylim = pnorm(ylims, normalmean, normalvar),#
			   xlim = pnorm(xlims, normalmean, normalvar),#
			   ylab = "Backward",#
			   xlab = "Forward",#
			   col = "blue",#
			   pch = 19,#
			   main = nms[i],#
			   cex.lab = 1,#
			   cex.axis = 1,#
			   cex.main = 1)#
		lines(x = pnorm(xlims, normalmean, normalvar), y = pnorm(ylims, normalmean, normalvar), col = "red", lwd = 1)#
		text(paste("Backward Mean:", round(mean(Backward_stats[,i]), 4),#
				   "\nForward Mean:", round(mean(Forward_stats[,i]), 4),#
				   "\nt-test p-value:", round(t.test(Backward_test, Forward_test)$p.value, 4),#
				   "\nMann-Whitney p-value:", round(wilcox.test(Backward_test, Forward_test)$p.value,4)),#
				   x = pnorm(xlims[2], normalmean, normalvar) - 0.35 * abs(pnorm(xlims[2], normalmean, normalvar) - pnorm(xlims[1], normalmean, normalvar)),#
				   y = pnorm(ylims[1], normalmean, normalvar) + 0.15 * abs(pnorm(ylims[2], normalmean, normalvar) - pnorm(ylims[1], normalmean, normalvar)),#
				   cex = 0.5)#
	}
#PP_Plots#
GiR_PP_Plots2 = function(Forward_stats, Backward_stats) {#
	nms = colnames(Forward_stats)#
	for (i in 1L:ncol(Forward_stats)) {#
		if (nrow(Forward_stats) > 20000) {#
			thin = seq(from = floor(nrow(Forward_stats)/10), to = nrow(Forward_stats), length.out = 10000)#
			Forward_test = Forward_stats[thin, i]#
			Backward_test = Backward_stats[thin, i]#
		} else {#
			Forward_test = Forward_stats[, i]#
			Backward_test = Backward_stats[, i]#
		}#
		all = c(Backward_stats[, i], Forward_stats[, i])#
		ylims = c(min(all) - 0.1 * max(abs(all)), max(all) + 0.1 * max(abs(all)))#
		xlims = ylims#
		quantiles = 50#
		if (grepl("B_", nms[i]) ) {#
			quantiles = 1000#
		}#
		normalmean = mean(c(quantile(Forward_stats[, i], seq(0, 1, length = quantiles)), #
							quantile(Backward_stats[, i], seq(0, 1, length = quantiles))))#
		normalvar = sd(c(quantile(Forward_stats[, i], seq(0, 1, length = quantiles)), #
							quantile(Backward_stats[, i], seq(0, 1, length = quantiles))))#
#
		qqplot(x = pnorm(sort(quantile(Forward_stats[, i], seq(0, 1, length = quantiles))), normalmean, normalvar),#
			   y = pnorm(sort(quantile(Backward_stats[, i], seq(0, 1, length = quantiles))), normalmean, normalvar),#
			   ylim = pnorm(ylims, normalmean, normalvar),#
			   xlim = pnorm(xlims, normalmean, normalvar),#
			   ylab = "Backward",#
			   xlab = "Forward",#
			   col = "blue",#
			   pch = 19,#
			   main = nms[i],#
			   cex.lab = 1,#
			   cex.axis = 1,#
			   cex.main = 1)#
		lines(x = pnorm(xlims, normalmean, normalvar), y = pnorm(ylims, normalmean, normalvar), col = "red", lwd = 2)#
		text(paste("Backward Mean:", round(mean(Backward_stats[,i]), 4),#
				   "\nForward Mean:", round(mean(Forward_stats[,i]), 4),#
				   "\nt-test p-value:", round(t.test(Backward_test, Forward_test)$p.value, 4),#
				   "\nMann-Whitney p-value:", round(wilcox.test(Backward_test, Forward_test)$p.value,4)),#
				   x = pnorm(xlims[2], normalmean, normalvar) - 0.35 * abs(pnorm(xlims[2], normalmean, normalvar) - pnorm(xlims[1], normalmean, normalvar)),#
				   y = pnorm(ylims[1], normalmean, normalvar) + 0.15 * abs(pnorm(ylims[2], normalmean, normalvar) - pnorm(ylims[1], normalmean, normalvar)),#
				   cex = 0.5)#
	}#
}
par(mfrow=c(5,5), oma = c(3,3,3,3), mar = c(2,2,1,1))#
		GiR_PP_Plots2(TryGiR$Forward, TryGiR$Backward)
#PP_Plots#
GiR_PP_Plots2 = function(Forward_stats, Backward_stats) {#
	nms = colnames(Forward_stats)#
	for (i in 1L:ncol(Forward_stats)) {#
		if (nrow(Forward_stats) > 20000) {#
			thin = seq(from = floor(nrow(Forward_stats)/10), to = nrow(Forward_stats), length.out = 10000)#
			Forward_test = Forward_stats[thin, i]#
			Backward_test = Backward_stats[thin, i]#
		} else {#
			Forward_test = Forward_stats[, i]#
			Backward_test = Backward_stats[, i]#
		}#
		all = c(Backward_stats[, i], Forward_stats[, i])#
		ylims = c(min(all) - 0.1 * max(abs(all)), max(all) + 0.1 * max(abs(all)))#
		xlims = ylims#
		quantiles = 50#
		if (grepl("B_", nms[i]) ) {#
			quantiles = 1000#
		}#
		normalmean = mean(c(quantile(Forward_stats[, i], seq(0, 1, length = quantiles)), #
							quantile(Backward_stats[, i], seq(0, 1, length = quantiles))))#
		normalvar = sd(c(quantile(Forward_stats[, i], seq(0, 1, length = quantiles)), #
							quantile(Backward_stats[, i], seq(0, 1, length = quantiles))))#
#
		qqplot(x = pnorm(sort(quantile(Forward_stats[, i], seq(0, 1, length = quantiles))), normalmean, normalvar),#
			   y = pnorm(sort(quantile(Backward_stats[, i], seq(0, 1, length = quantiles))), normalmean, normalvar),#
			   ylim = pnorm(ylims, normalmean, normalvar),#
			   xlim = pnorm(xlims, normalmean, normalvar),#
			   ylab = "Backward",#
			   xlab = "Forward",#
			   col = "blue",#
			   pch = 19,#
			   main = nms[i],#
			   cex.lab = 1,#
			   cex.axis = 1,#
			   cex.main = 1)#
		lines(x = pnorm(xlims, normalmean, normalvar), y = pnorm(ylims, normalmean, normalvar), col = "red", lwd = 2)#
		text(paste("Backward Mean:", round(mean(Backward_stats[,i]), 4),#
				   "\nForward Mean:", round(mean(Forward_stats[,i]), 4),#
				   "\nt-test p-value:", round(t.test(Backward_test, Forward_test)$p.value, 4),#
				   "\nMann-Whitney p-value:", round(wilcox.test(Backward_test, Forward_test)$p.value,4)),#
				   x = pnorm(xlims[2], normalmean, normalvar) - 0.35 * abs(pnorm(xlims[2], normalmean, normalvar) - pnorm(xlims[1], normalmean, normalvar)),#
				   y = pnorm(ylims[1], normalmean, normalvar) + 0.15 * abs(pnorm(ylims[2], normalmean, normalvar) - pnorm(ylims[1], normalmean, normalvar)),#
				   cex = 0.4)#
	}#
}
par(mfrow=c(5,5), oma = c(3,3,3,3), mar = c(2,2,1,1))#
		GiR_PP_Plots2(TryGiR$Forward, TryGiR$Backward)
names(TryGiR)
head(TryGiR$Forward)
head(TryGiR$Backward)
TryGiR$tstats
TryGiR$wstats
save(TryGiR, file = "/Users/bomin8319/Desktop/IPTM-master/pkg/R/TryGiR.RData")
library(mvtnorm)#
library(MCMCpack)#
library(entropy)#
library(Rcpp)#
library(RcppArmadillo)#
sourceCpp('~/Desktop/IPTM-master/pkg/src/sampler.cpp')#
source('~/Desktop/IPTM-master/pkg/R/core.R')#
GenerateDocs = function(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec,#
						b, delta, currentC, netstat, base.edge, base.text, seed,#
						topic_token_assignments = NULL, topic_token_counts = NULL, word_type_topic_counts = NULL, #
						forward = FALSE, backward_init = FALSE, backward = FALSE, base = FALSE) {#
	set.seed(seed)#
 	W = length(vocabulary)#
  	phi = lapply(1L:K, function(k) {#
		rdirichlet_cpp(1, betas * nvec)#
	})#
	L = 3#
    P = 1 + L * (2 * ("dyadic" %in% netstat) + 4 * ("triadic" %in% netstat) + 2 *("degree" %in% netstat))#
    if (base) {#
    	t.d = 0#
    } else {#
    	t.d = 384#
    }#
	edge = base.edge#
	text = base.text#
	p.d = matrix(NA, nrow = nDocs, ncol = nIP)#
	options(warn = -1)#
	for (d in 1:nDocs) {#
		N.d = nwords#
		text[[length(base.text) + d]] = rep(NA, N.d)#
		if (!backward) {#
			theta.d = rdirichlet_cpp(1, alpha * mvec)	#
			topic.d = multinom_vec(max(1, N.d), theta.d)#
			if (N.d > 0) {#
				for (n in 1:N.d){#
					text[[length(base.text) + d]][n] = multinom_vec(1, phi[[topic.d[n]]])#
				}#
				names(text[[length(base.text) + d]]) = topic.d#
			}#
		} else {#
			phi.k = rep(NA, K)#
			topic.d = topic_token_assignments[[d]]#
			word.d = as.numeric(names(topic_token_assignments[[d]]))#
			if (N.d > 0) {#
				for (n in 1:N.d){#
					word_type_topic_counts[word.d [n], topic.d[n]] = word_type_topic_counts[word.d [n], topic.d[n]] - 1#
					for (w in 1:W) {#
						phi.k[w] = (word_type_topic_counts[w, topic.d[n]] + betas * nvec[w])/(topic_token_counts[topic.d[n]] + betas)#
					} #
					text[[length(base.text) + d]][n] = multinom_vec(1, phi.k)#
					word_type_topic_counts[text[[d]][n], topic.d[n]] = word_type_topic_counts[text[[d]][n], topic.d[n]] + 1#
				}#
				names(text[[length(base.text) + d]]) = topic.d#
			}#
		}#
#
		p.d[d, ] = vapply(1L:nIP, function(IP) {#
			sum(topic.d %in% which(currentC == IP))#
			}, c(1)) / max(1, N.d)#
		if (base & t.d < 384) {#
			history.t = lapply(1:nIP, function(IP) {#
				lapply(1:3, function(l){#
				matrix(0, length(node), length(node))#
				})#
			})#
		} else {	#
		history.t = History(edge, p.d, node, t.d + 10^(-10))#
		}#
		X = lapply(node, function(i) {#
  	        Netstats(history.t, node, i, netstat)#
            })#
   	 	XB = MultiplyXBList(X, b)     #
		lambda = lambda_cpp(p.d[d,], XB)#
		iJi = rbinom_mat((delta * lambda) / (delta * lambda + 1))#
		while (sum(iJi) == 0) {#
			iJi = rbinom_mat((delta * lambda) / (delta * lambda + 1))#
		}#
		LambdaiJi = lambdaiJi(p.d[d,], XB, iJi)#
		Time.inc = vapply(LambdaiJi, function(lambda) {#
			rexp(1, lambda)#
			}, c(1))#
		i.d = which(Time.inc == min(Time.inc[!is.na(Time.inc)]))#
		j.d = which(iJi[i.d,] == 1)#
		t.d = t.d + Time.inc[i.d]#
		edge[[length(base.edge) + d]] = list(sender = i.d, receiver = j.d, timestamp = t.d)		#
		}#
	options(warn = 0)#
	if (forward) {#
		edge = edge[-(1:length(base.edge))]#
		text = text[-(1:length(base.text))]#
	}#
	if (base == TRUE & t.d > 384) {#
		cutoff = which_int(384, vapply(1:length(edge), function(d) {edge[[d]][[3]]}, c(1))) - 1#
		edge = edge[1:cutoff]#
		text = text[1:cutoff]#
	}#
	return(list(edge = edge, text = text, b = b, base = length(base.edge)))							#
} #
GiR_stats = function(GiR_sample, K, currentC, vocabulary, forward = FALSE, backward = FALSE) {#
	edge = GiR_sample$edge#
	text = GiR_sample$text#
	if (backward) {#
		edge = edge[-(1:GiR_sample$base)]#
		text = text[-(1:GiR_sample$base)]#
	}#
	GiR_stats = c()#
	nDocs = length(edge)#
	P = length(GiR_sample$b[[1]])#
	nIP = length(GiR_sample$b)#
	nwords = length(GiR_sample$text[[1]])#
	W = length(vocabulary)#
	GiR_stats[1:P] = Reduce('+', GiR_sample$b) / nIP#
	GiR_stats[P + 1] = mean(vapply(1:nDocs, function(d) {#
						length(edge[[d]][[2]])#
						}, c(1)))#
	GiR_stats[P + 2] = mean(vapply(2:nDocs, function(d) {#
					 edge[[d]][[3]] - edge[[d-1]][[3]]#
					 }, c(1))) 			#
	GiR_stats[P + 3] = mean(currentC)#
	Tokens_in_Topic = tabulate(vapply(1:nDocs, function(d){#
					  as.numeric(names(text[[d]]))#
					  }, rep(0, nwords)), K)#
	GiR_stats[(P + 4):(P + 3 + nIP)] = vapply(1:nIP, function(IP) {#
								 Tokens_in_Topic %*% (currentC == IP)#
								 }, c(1))#
	GiR_stats[(P + 4 + nIP):(P + 3 + nIP + K)] = Tokens_in_Topic#
	Tokens_in_Word = tabulate(vapply(1:nDocs, function(d){#
					 text[[d]]#
					 }, rep(0, nwords)), W)#
	GiR_stats[(P + 4 + nIP + K):(P + 3 + nIP + K + W)] = Tokens_in_Word#
	return(GiR_stats)#
}#
Inference = function(edge, node, textlist, vocabulary, nIP, K, sigma_Q, alpha, mvec, betas, nvec, prior.b.mean, prior.b.var, prior.delta, #
					out, n1, n2, n3, burn, thin, netstat, seed, plot = FALSE) {#
  set.seed(seed)#
  # trim the edge so that we only model edges after 16 days#
	timeinc = c(as.numeric(edge[[1]][3]), vapply(seq(along = edge)[-1], function(d) {#
  	as.numeric(edge[[d]][3]) - as.numeric(edge[[d-1]][3])#
 	}, c(1)))#
#
    edge2 = which_int(384, cumsum(timeinc)) : length(edge)#
  # initialize alpha, mvec, delta, nvec, eta, lvec, and gammas#
 	W = length(vocabulary)#
  	phi = lapply(1L:K, function(k) {#
		rdirichlet_cpp(1, betas * nvec)#
	})#
	delta = rbeta(1, prior.delta[1], prior.delta[2])#
	deltamat = delta#
	eta = qnorm(delta)#
	# initialize C, theta and Z#
	currentC = sample(1:nIP, K, replace = TRUE)#
	theta = rdirichlet_cpp(length(edge), alpha * mvec)#
	currentZ = lapply(seq(along = edge), function(d) {#
    if (length(textlist[[d]]) > 0) {#
    		multinom_vec(length(textlist[[d]]), theta[d, ])#
    	} else {#
    		multinom_vec(1, theta[d, ])#
    	}#
    })#
	p.d = t(vapply(seq(along = edge), function(d) {#
	  vapply(1L:nIP, function(IP) {#
	    sum(currentZ[[d]] %in% which(currentC == IP))#
	  }, c(1)) / length(currentZ[[d]])#
	}, rep(1, nIP)))#
#
    # initialize beta#
    sigma = 1#
    L = 3#
    P = 1 + L * (2 * ("dyadic" %in% netstat) + 4 * ("triadic" %in% netstat) + 2 *("degree" %in% netstat))#
    bmat = list()#
	for (IP in 1L:nIP) {#
		bmat[[IP]] = matrix(0, nrow = P, ncol = (n3 - burn) / thin)#
		bmat[[IP]][, 1:(n3 - burn) / thin] = c(rmvnorm(1, prior.b.mean, prior.b.var))#
  	}#
#
    # to check the convergence  #
    if (plot) {#
     	logWZ.mat = c()							  #
     	alpha.mat = c()#
     	entropy.mat = c()#
    }#
    iJi = lapply(seq(along = edge), function(d) {#
  	      matrix(0, nrow = length(node), ncol = length(node))#
  	      })  #
    #start outer iteration#
    for (o in 1L:out) {#
      Beta.old = lapply(bmat, function(b) {#
      			rowMeans(b)#
         		})  #
     # Data augmentation#
      lambda = list()#
      LambdaiJi = list()#
      nonemptyiJi = list()#
	  observediJi = list()#
      for (d in edge2) {#
   	 	history.t = History(edge, p.d, node, as.numeric(edge[[d]][3]))#
   	 	X = lapply(node, function(i) {#
  	        Netstats(history.t, node, i, netstat)#
            })#
   	 	XB = MultiplyXBList(X, Beta.old)     #
		lambda[[d]] = lambda_cpp(p.d[d,], XB)#
		#calculate the resampling probability#
		probij = DataAug_cpp(iJi[[d]], lambda[[d]], delta, timeinc[d])#
		iJi[[d]] = rbinom_mat(probij)#
		iJi[[d]][as.numeric(edge[[d]][1]),] = tabulateC(as.numeric(unlist(edge[[d]][2])), length(node))#
		LambdaiJi[[d]] = lambdaiJi(p.d[d,], XB, iJi[[d]])#
		nonemptyiJi[[d]] = LambdaiJi[[d]][!is.na(LambdaiJi[[d]])]#
		observediJi[[d]] = LambdaiJi[[d]][as.numeric(edge[[d]][1])]#
		}	 #
    textlist.raw = unlist(textlist)#
    table.W = lapply(1L:K, function(k) {#
      tabulateC(textlist.raw[which(unlist(currentZ) == k)], W)#
      })#
    for (i1 in 1L:n1) {#
      for (d in edge2) { #
        textlist.d = textlist[[d]]#
        if (length(textlist.d) > 0) {#
        	topicpart.d = TopicInEqZ(K, currentZ[[d]], alpha, mvec, d)#
       	wordpart.d = WordInEqZ(K, textlist.d, table.W, betas, nvec)#
        } else {#
        topicpart.d = 0#
        wordpart.d = matrix(0, nrow = length(currentZ[[d]]), ncol = K)#
        }#
        edgepart.d = EdgeInEqZ(iJi[[d]], lambda[[d]], delta)#
        timepart.d = TimeInEqZ(nonemptyiJi[[d]], timeinc[d])#
        observed.d = ObservedInEqZ(observediJi[[d]]) #
        fixedpart = topicpart.d + edgepart.d + timepart.d + observed.d #
        for (w in 1L:length(currentZ[[d]])) {#
          const.Z = fixedpart + wordpart.d[w, ]#
          const.Z = const.Z - max(const.Z)#
          zw.old = currentZ[[d]][w]#
          zw.new = multinom_vec(1, exp(const.Z))#
          if (zw.new != zw.old) {#
            currentZ[[d]][w] = zw.new#
            topicpart.d = TopicInEqZ(K, currentZ[[d]], alpha, mvec, d)#
            if (length(textlist.d) > 0) {	#
            wordpart.d = WordInEqZ(K, textlist.d, table.W, betas, nvec)#
            }#
            table.W = lapply(1L:K, function(k) {#
      				  tabulateC(textlist.raw[which(unlist(currentZ) == k)], W)#
      				  })#
      		p.d[d, ] = vapply(1L:nIP, function(IP) {#
	 			sum(currentZ[[d]] %in% which(currentC == IP))#
	 			}, c(1)) / length(currentZ[[d]])#
      		LambdaiJi[[d]] = lambdaiJi(p.d[d,], XB, iJi[[d]])#
			nonemptyiJi[[d]] = LambdaiJi[[d]][!is.na(LambdaiJi[[d]])]#
         	observediJi[[d]] = LambdaiJi[[d]][as.numeric(edge[[d]][1])]#
          }#
        }#
       }#
      }#
#
    for (i2 in 1L:n2) {#
      # C update given Z and B - within each document d#
      for (k in unique(unlist(currentZ[edge2]))) { #
        document.k = which(vapply(currentZ, function(d){k %in% d}, c(1)) == 1)#
        document.k = document.k[document.k %in% edge2]#
        const.C = rep(NA, nIP)#
        for (IP in 1L:nIP) {#
        	if (!currentC[k] == IP){#
          currentC[k] = IP#
          for (d in document.k) {#
            p.d[d, ] =  vapply(1L:nIP, function(IP) {#
              sum(currentZ[[d]] %in% which(currentC == IP))#
            }, c(1)) / length(currentZ[[d]])#
          }#
          for (d in edge2) {#
           history.t = History(edge, p.d, node, as.numeric(edge[[d]][3]))#
    	   	   X = lapply(node, function(i) {#
               Netstats(history.t, node, i, netstat)#
               })#
    	       XB = MultiplyXBList(X, Beta.old)    #
           lambda[[d]] = lambda_cpp(p.d[d,], XB)#
		   LambdaiJi[[d]] = lambdaiJi(p.d[d,], XB, iJi[[d]])#
		   nonemptyiJi[[d]] = LambdaiJi[[d]][!is.na(LambdaiJi[[d]])]#
           observediJi[[d]] = LambdaiJi[[d]][as.numeric(edge[[d]][1])]#
          }#
          }#
          const.C[IP] = sum(vapply(document.k, function(d) {#
            EdgeInEqZ(iJi[[d]], lambda[[d]], delta) + TimeInEqZ(nonemptyiJi[[d]], timeinc[d]) + #
    			ObservedInEqZ(observediJi[[d]]) #
          }, c(1))) / length(document.k)#
          }#
          const.C = const.C - max(const.C)#
          currentC[k] = multinom_vec(1, exp(const.C))#
      }#
    }#
    if (plot) {#
      entropy.mat = c(entropy.mat, entropy.empirical(currentC))#
      alpha.mat = rbind(alpha.mat, alpha)#
      logWZ.mat = c(logWZ.mat, logWZ(K, currentZ, textlist, table.W, alpha, mvec, betas, nvec))#
      }#
     for (d in edge2) {#
     	 p.d[d, ] =  vapply(1L:nIP, function(IP) {#
              		 sum(currentZ[[d]] %in% which(currentC == IP))#
                     }, c(1)) / length(currentZ[[d]])#
     	 history.t = History(edge, p.d, node, as.numeric(edge[[d]][3]))#
    	     X = lapply(node, function(i) {#
              Netstats(history.t, node, i, netstat)#
             })#
    	     XB = MultiplyXBList(X, Beta.old)   #
    	     lambda[[d]] = lambda_cpp(p.d[d,], XB)#
		 LambdaiJi[[d]] = lambdaiJi(p.d[d,], XB, iJi[[d]])#
		 nonemptyiJi[[d]] = LambdaiJi[[d]][!is.na(LambdaiJi[[d]])]#
         observediJi[[d]] = LambdaiJi[[d]][as.numeric(edge[[d]][1])]#
	 }#
	 # Beta and delta update#
	 prior.old1 = sum(vapply(1L:nIP, function(IP) {#
		  		 dmvnorm(Beta.old[[IP]], prior.b.mean, prior.b.var, log = TRUE)#
		  		 }, c(1))) #
	 post.old1 = sum(vapply(edge2, function(d) {#
	     	     EdgeInEqZ(iJi[[d]], lambda[[d]], delta) + TimeInEqZ(nonemptyiJi[[d]], timeinc[d]) +#
    			 	 ObservedInEqZ(observediJi[[d]])#
    			 }, c(1))) / length(edge2)#
    	 prior.old2 = dbeta(delta, prior.delta[1], prior.delta[2], log = TRUE) #
     post.old2 = sum(vapply(edge2, function(d) {#
    			    EdgeInEqZ(iJi[[d]], lambda[[d]], delta)#
    			    }, c(1))) / length(edge2)#
	 options(warn = -1)#
	 proposal.var = list()#
     for (IP in 1L:nIP) {#
     	 proposal.var[[IP]] = cor(t(bmat[[IP]]))#
         proposal.var[[IP]][is.na(proposal.var[[IP]])] = 0#
         if (sum(eigen(proposal.var[[IP]])$values < 0 ) > 0) {#
        	 proposal.var[[IP]] = diag(P)#
         }#
         }#
     proposal.var = lapply(1:nIP, function(IP){diag(P)})#
     options(warn = 0)#
     for (i3 in 1L:n3) {#
     	Beta.new = lapply(1L:nIP, function(IP) {#
           rmvnorm(1, Beta.old[[IP]], sigma_Q * proposal.var[[IP]])#
         }) #
        for (d in edge2) {#
           XB = MultiplyXBList(X, Beta.new)    #
           LambdaiJi[[d]] = lambdaiJi(p.d[d,], XB, iJi[[d]])#
		   nonemptyiJi[[d]] = LambdaiJi[[d]][!is.na(LambdaiJi[[d]])]#
		   observediJi[[d]] = LambdaiJi[[d]][as.numeric(edge[[d]][1])]#
         }#
         prior.new1 = sum(vapply(1L:nIP, function(IP) {#
        		dmvnorm(Beta.new[[IP]], prior.b.mean, prior.b.var, log = TRUE)#
         }, c(1))) #
         post.new1 = sum(vapply(edge2, function(d) {#
    			    EdgeInEqZ(iJi[[d]], lambda[[d]], delta) + TimeInEqZ(nonemptyiJi[[d]], timeinc[d]) + #
    			    ObservedInEqZ(observediJi[[d]])#
    			    }, c(1))) / length(edge2)#
    		 loglike.diff = prior.new1 + post.new1 - prior.old1 - post.old1#
        if (log(runif(1, 0, 1)) < loglike.diff) {#
         for (IP in 1L:nIP) {#
         Beta.old[[IP]]  = Beta.new[[IP]]#
         }#
         prior.old1 = prior.new1#
         post.old1 = post.new1#
         }#
        	 eta.new = rnorm(1, eta, sigma_Q)#
    	 	 delta.new = pnorm(eta.new)#
		 prior.new2 = dbeta(delta.new, prior.delta[1], prior.delta[2], log = TRUE)#
		 post.new2 = sum(vapply(edge2, function(d) {#
    			    EdgeInEqZ(iJi[[d]], lambda[[d]], delta.new)#
    			    }, c(1))) / length(edge2)#
    	 	 loglike.diff2 = prior.new2 + post.new2 - prior.old2 - post.old2#
		  if (log(runif(1, 0, 1)) < loglike.diff2) {#
			delta = delta.new#
			eta = eta.new#
			prior.old2 = prior.new2#
			post.old2 = post.new2#
			}#
#
         if (i3 > burn & i3 %% (thin) == 0) {#
          for (IP in 1L:nIP) {#
           bmat[[IP]][ , (i3 - burn) / thin] = Beta.old[[IP]]#
           }#
           deltamat = c(deltamat, delta)#
          }#
         }#
	 }#
    if (plot) {#
    burnin = round(out / 10)#
    par(mfrow = c(2, 2))#
  	plot(entropy.mat[-1L:-burnin], type = "l", #
  	     xlab = "(Outer) Iterations", ylab = "Entropy of IP")#
  	abline(h = mean(entropy.mat[-1L:-burnin]), lty = 1)#
  	title("Convergence of Entropy")#
  	matplot(alpha.mat[-1L:-burnin,], lty = 1, type = "l", col = 1L:nIP, #
  	        xlab = "(Outer) Iterations", ylab = "alpha")#
  	abline(h = mean(alpha.mat[-1L:-burnin,]), lty = 1, col = 1L:nIP)#
	title("Convergence of Optimized alpha")#
	plot(logWZ.mat[-1L:-burnin], type = "l", #
	       xlab = "(Outer) Iterations", ylab = "logWZ")#
	abline(h = mean(logWZ.mat[-1L:-burnin]), lty = 1)#
	title("Convergence of logWZ")#
	matplot(bmat[[1]][1,], lty = 1, col = 1L:P, type = "l", #
	          main = "Traceplot of beta", xlab = "(Inner) Iterations", ylab = "")#
	abline(h = mean(bmat[[1]][1,]), lty = 1, col = 1L)#
	  }#
  chain.final = list(C = currentC, Z = lapply(edge2, function(d) {currentZ[[d]]}), B = bmat, D = deltamat)#
#
  return(chain.final)#
}#
#
#PP_Plots#
GiR_PP_Plots = function(Forward_stats, Backward_stats) {#
	nms = colnames(Forward_stats)#
	for (i in 1L:ncol(Forward_stats)) {#
		if (nrow(Forward_stats) > 20000) {#
			thin = seq(from = floor(nrow(Forward_stats)/10), to = nrow(Forward_stats), length.out = 10000)#
			Forward_test = Forward_stats[thin, i]#
			Backward_test = Backward_stats[thin, i]#
		} else {#
			Forward_test = Forward_stats[, i]#
			Backward_test = Backward_stats[, i]#
		}#
		all = c(Backward_stats[, i], Forward_stats[, i])#
		ylims = c(min(all) - 0.1 * max(abs(all)), max(all) + 0.1 * max(abs(all)))#
		xlims = ylims#
		quantiles = 50#
		if (grepl("B_", nms[i]) ) {#
			quantiles = 1000#
		}#
		qqplot(x = quantile(Forward_stats[, i], seq(0, 1, length = quantiles)),#
			   y = quantile(Backward_stats[, i], seq(0, 1, length = quantiles)),#
			   ylim = ylims,#
			   xlim = xlims,#
			   ylab = "Backward",#
			   xlab = "Forward",#
			   col = "blue",#
			   pch = 19,#
			   main = nms[i],#
			   cex.lab = 1,#
			   cex.axis = 1,#
			   cex.main = 1)#
		lines(x = xlims, y = ylims, col = "red", lwd = 3)#
		text(paste("Backward Mean:", round(mean(Backward_stats[,i]), 4),#
				   "\nForward Mean:", round(mean(Forward_stats[,i]), 4),#
				   "\nt-test p-value:", round(t.test(Backward_test, Forward_test)$p.value, 4),#
				   "\nMann-Whitney p-value:", round(wilcox.test(Backward_test, Forward_test)$p.value,4)),#
				   x = xlims[2] - 0.35 * abs(xlims[2] - xlims[1]),#
				   y = ylims[1] + 0.15 * abs(ylims[2] - ylims[1]),#
				   cex = 0.3)#
	}#
}      #
#PP_Plots#
GiR_PP_Plots2 = function(Forward_stats, Backward_stats) {#
	nms = colnames(Forward_stats)#
	for (i in 1L:ncol(Forward_stats)) {#
		if (nrow(Forward_stats) > 20000) {#
			thin = seq(from = floor(nrow(Forward_stats)/10), to = nrow(Forward_stats), length.out = 10000)#
			Forward_test = Forward_stats[thin, i]#
			Backward_test = Backward_stats[thin, i]#
		} else {#
			Forward_test = Forward_stats[, i]#
			Backward_test = Backward_stats[, i]#
		}#
		all = c(Backward_stats[, i], Forward_stats[, i])#
		ylims = c(min(all) - 0.1 * max(abs(all)), max(all) + 0.1 * max(abs(all)))#
		xlims = ylims#
		quantiles = 50#
		if (grepl("B_", nms[i]) ) {#
			quantiles = 1000#
		}#
		normalmean = mean(c(quantile(Forward_stats[, i], seq(0, 1, length = quantiles)), #
							quantile(Backward_stats[, i], seq(0, 1, length = quantiles))))#
		normalvar = sd(c(quantile(Forward_stats[, i], seq(0, 1, length = quantiles)), #
							quantile(Backward_stats[, i], seq(0, 1, length = quantiles))))#
#
		qqplot(x = pnorm(sort(quantile(Forward_stats[, i], seq(0, 1, length = quantiles))), normalmean, normalvar),#
			   y = pnorm(sort(quantile(Backward_stats[, i], seq(0, 1, length = quantiles))), normalmean, normalvar),#
			   ylim = pnorm(ylims, normalmean, normalvar),#
			   xlim = pnorm(xlims, normalmean, normalvar),#
			   ylab = "Backward",#
			   xlab = "Forward",#
			   col = "blue",#
			   pch = 19,#
			   main = nms[i],#
			   cex.lab = 1,#
			   cex.axis = 1,#
			   cex.main = 1)#
		lines(x = pnorm(xlims, normalmean, normalvar), y = pnorm(ylims, normalmean, normalvar), col = "red", lwd = 2)#
		text(paste("Backward Mean:", round(mean(Backward_stats[,i]), 4),#
				   "\nForward Mean:", round(mean(Forward_stats[,i]), 4),#
				   "\nt-test p-value:", round(t.test(Backward_test, Forward_test)$p.value, 4),#
				   "\nMann-Whitney p-value:", round(wilcox.test(Backward_test, Forward_test)$p.value,4)),#
				   x = pnorm(xlims[2], normalmean, normalvar) - 0.35 * abs(pnorm(xlims[2], normalmean, normalvar) - pnorm(xlims[1], normalmean, normalvar)),#
				   y = pnorm(ylims[1], normalmean, normalvar) + 0.15 * abs(pnorm(ylims[2], normalmean, normalvar) - pnorm(ylims[1], normalmean, normalvar)),#
				   cex = 0.4)#
	}#
}      #
# Getting_It_Right#
GiR = function(Nsamp = 5000, nDocs = 5, node = 1:4, vocabulary =  c("hi", "hello","bye", "mine", "what"), #
			   nIP = 2, K = 4, nwords = 4, alpha = 2, mvec = rep(1/4, 4), betas = 2, nvec = rep(1/5, 5), #
			   prior.b.mean = c(-3, rep(0, 6)), prior.b.var = diag(7), prior.delta = c(2, 2), sigma_Q = 0.25, #
			   niters = c(1, 1, 1, 50, 0, 1), netstat = "dyadic", generate_PP_plots = TRUE, seed = 1) {#
	set.seed(seed)#
	b = lapply(1L:nIP, function(IP) {#
			   c(rmvnorm(1, prior.b.mean, prior.b.var))#
			   })#
	delta = 	rbeta(1, prior.delta[1], prior.delta[2])#
	currentC = sample(1L:nIP, K, replace = TRUE)	 #
	base.data = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, seed, #
							 base.edge = list(), base.text = list(), base = TRUE)#
	base.edge = base.data$edge	   #
	base.text = base.data$text#
#
	#Forward sampling#
	Forward_stats = matrix(NA, nrow = Nsamp, ncol = 21)#
	colnames(Forward_stats) = c(paste0("B_",1:length(b[[1]])), "Mean_receipients", "Mean_timediff", "Mean_TopicIP", #
						  paste0("Tokens_in_IP_", 1:nIP), paste0("Tokens_in_Topic", 1:K), #
						  paste0("Tokens_in_Word", 1:length(vocabulary)))#
	deltamat1 = c()#
	for (i in 1:Nsamp) { #
		if (i %% 5000 == 0) {cat("Forward sampling", i, "\n")}#
		set.seed(i)#
		b = lapply(1:nIP, function(IP) {#
			c(rmvnorm(1, prior.b.mean, prior.b.var))#
			})#
		delta = rbeta(1, prior.delta[1], prior.delta[2])#
		deltamat1 = c(deltamat1, delta)#
		currentC = sample(1L:nIP, K, replace = TRUE)#
		Forward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
									  base.edge = base.edge, base.text = base.text, seed, forward = TRUE) #
		Forward_stats[i, ] = GiR_stats(Forward_sample, K, currentC, vocabulary, forward = TRUE, backward = FALSE)#
	}#
	#Backward sampling#
	Backward_stats = matrix(NA, nrow = Nsamp, ncol = 21)#
	Backward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
								   base.edge = base.edge, base.text = base.text, seed, backward_init = TRUE) #
	deltamat2 = c()							   #
	for (i in 1:Nsamp) { #
		if (i %% 500 == 0) {cat("Backward sampling", i, "\n")}#
		seed = seed + 100#
		set.seed(seed)#
		Inference_samp = Inference(Backward_sample$edge, node, Backward_sample$text, vocabulary, nIP, K, sigma_Q, #
						  alpha, mvec, betas, nvec, prior.b.mean, prior.b.var, prior.delta,#
						  out = niters[1], n1 = niters[2], n2 = niters[3], n3 = niters[4], burn = niters[5], thin = niters[6], #
						  netstat, seed)#
		b = lapply(1:nIP, function(IP) {#
			rowMeans(Inference_samp$B[[IP]])#
		})#
		delta = mean(Inference_samp$D)#
		deltamat2 = c(deltamat2, delta)#
		currentC = Inference_samp$C#
		topic_token_assignments = Inference_samp$Z#
		for (d in 1:length(topic_token_assignments)) {#
			names(topic_token_assignments[[d]]) = Backward_sample$text[[d + length(base.text)]]#
		}#
		topic_token_counts = tabulate(unlist(Inference_samp$Z), K)#
		word_type_topic_counts = matrix(NA , length(vocabulary), K)#
		for (w in 1:length(vocabulary)) {#
			word_type_w = which(unlist(Backward_sample$text[-(1:length(base.text))]) == w)#
			word_type_topic_counts[w, ] = tabulate(unlist(Inference_samp$Z)[word_type_w], K)#
		}#
		Backward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
								   	   base.edge = base.edge, base.text = base.text, seed, topic_token_assignments = topic_token_assignments, #
								   	   topic_token_counts = topic_token_counts, word_type_topic_counts = word_type_topic_counts, #
								   	   forward = FALSE, backward = TRUE)#
		Backward_stats[i, ] = GiR_stats(Backward_sample, K, currentC, vocabulary, forward = FALSE, backward = TRUE)#
	}#
	tstats = rep(0, ncol(Forward_stats))#
	wstats = rep(0, ncol(Forward_stats))#
	for (j in 1:ncol(Forward_stats)) {#
		thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 400)#
		Forward_test = Forward_stats[thin, j]#
		Backward_test = Backward_stats[thin, j]#
		tstats[j] = t.test(Backward_test, Forward_test)$p.value#
		wstats[j] = wilcox.test(Backward_test, Forward_test)$p.value#
	}#
	names(tstats) = names(wstats) = colnames(Forward_stats)						#
	if (generate_PP_plots) {#
		par(mfrow=c(5,5), oma = c(3,3,3,3), mar = c(1,1,1,1))#
		GiR_PP_Plots2(Forward_stats, Backward_stats)#
	}			#
	return(list(Forward = Forward_stats, Backward = Backward_stats, tstats = tstats, wstats = wstats, delta1 = deltamat1, delta2 = deltamat2))	#
}
unix.time(TryGiR <- GiR(5*10^3, seed = 123))
matplot(rbind(TryGiR$delta1, TryGiR$delta2), col = 1:2)
matplot(cbind(TryGiR$delta1, TryGiR$delta2), col = 1:2)
matplot(cbind(TryGiR$delta1, TryGiR$delta2),type = 'l', col = 1:2)
delta = pnorm(rnorm(1, prior.eta[1], prior.eta[2]))
prior.eta = c(0, 1)
rnorm(1, prior.eta[1], prior.eta[2])
pnorm(rnorm(1, prior.eta[1], prior.eta[2]))
rm(list=ls())
library(mvtnorm)#
library(MCMCpack)#
library(entropy)#
library(Rcpp)#
library(RcppArmadillo)#
sourceCpp('~/Desktop/IPTM-master/pkg/src/sampler.cpp')#
source('~/Desktop/IPTM-master/pkg/R/core.R')#
GenerateDocs = function(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec,#
						b, delta, currentC, netstat, base.edge, base.text, seed,#
						topic_token_assignments = NULL, topic_token_counts = NULL, word_type_topic_counts = NULL, #
						forward = FALSE, backward_init = FALSE, backward = FALSE, base = FALSE) {#
	set.seed(seed)#
 	W = length(vocabulary)#
  	phi = lapply(1L:K, function(k) {#
		rdirichlet_cpp(1, betas * nvec)#
	})#
	L = 3#
    P = 1 + L * (2 * ("dyadic" %in% netstat) + 4 * ("triadic" %in% netstat) + 2 *("degree" %in% netstat))#
    if (base) {#
    	t.d = 0#
    } else {#
    	t.d = 384#
    }#
	edge = base.edge#
	text = base.text#
	p.d = matrix(NA, nrow = nDocs, ncol = nIP)#
	options(warn = -1)#
	for (d in 1:nDocs) {#
		N.d = nwords#
		text[[length(base.text) + d]] = rep(NA, N.d)#
		if (!backward) {#
			theta.d = rdirichlet_cpp(1, alpha * mvec)	#
			topic.d = multinom_vec(max(1, N.d), theta.d)#
			if (N.d > 0) {#
				for (n in 1:N.d){#
					text[[length(base.text) + d]][n] = multinom_vec(1, phi[[topic.d[n]]])#
				}#
				names(text[[length(base.text) + d]]) = topic.d#
			}#
		} else {#
			phi.k = rep(NA, K)#
			topic.d = topic_token_assignments[[d]]#
			word.d = as.numeric(names(topic_token_assignments[[d]]))#
			if (N.d > 0) {#
				for (n in 1:N.d){#
					word_type_topic_counts[word.d [n], topic.d[n]] = word_type_topic_counts[word.d [n], topic.d[n]] - 1#
					for (w in 1:W) {#
						phi.k[w] = (word_type_topic_counts[w, topic.d[n]] + betas * nvec[w])/(topic_token_counts[topic.d[n]] + betas)#
					} #
					text[[length(base.text) + d]][n] = multinom_vec(1, phi.k)#
					word_type_topic_counts[text[[d]][n], topic.d[n]] = word_type_topic_counts[text[[d]][n], topic.d[n]] + 1#
				}#
				names(text[[length(base.text) + d]]) = topic.d#
			}#
		}#
#
		p.d[d, ] = vapply(1L:nIP, function(IP) {#
			sum(topic.d %in% which(currentC == IP))#
			}, c(1)) / max(1, N.d)#
		if (base & t.d < 384) {#
			history.t = lapply(1:nIP, function(IP) {#
				lapply(1:3, function(l){#
				matrix(0, length(node), length(node))#
				})#
			})#
		} else {	#
		history.t = History(edge, p.d, node, t.d + 10^(-10))#
		}#
		X = lapply(node, function(i) {#
  	        Netstats(history.t, node, i, netstat)#
            })#
   	 	XB = MultiplyXBList(X, b)     #
		lambda = lambda_cpp(p.d[d,], XB)#
		iJi = rbinom_mat((delta * lambda) / (delta * lambda + 1))#
		while (sum(iJi) == 0) {#
			iJi = rbinom_mat((delta * lambda) / (delta * lambda + 1))#
		}#
		LambdaiJi = lambdaiJi(p.d[d,], XB, iJi)#
		Time.inc = vapply(LambdaiJi, function(lambda) {#
			rexp(1, lambda)#
			}, c(1))#
		i.d = which(Time.inc == min(Time.inc[!is.na(Time.inc)]))#
		j.d = which(iJi[i.d,] == 1)#
		t.d = t.d + Time.inc[i.d]#
		edge[[length(base.edge) + d]] = list(sender = i.d, receiver = j.d, timestamp = t.d)		#
		}#
	options(warn = 0)#
	if (forward) {#
		edge = edge[-(1:length(base.edge))]#
		text = text[-(1:length(base.text))]#
	}#
	if (base == TRUE & t.d > 384) {#
		cutoff = which_int(384, vapply(1:length(edge), function(d) {edge[[d]][[3]]}, c(1))) - 1#
		edge = edge[1:cutoff]#
		text = text[1:cutoff]#
	}#
	return(list(edge = edge, text = text, b = b, base = length(base.edge)))							#
} #
GiR_stats = function(GiR_sample, K, currentC, vocabulary, forward = FALSE, backward = FALSE) {#
	edge = GiR_sample$edge#
	text = GiR_sample$text#
	if (backward) {#
		edge = edge[-(1:GiR_sample$base)]#
		text = text[-(1:GiR_sample$base)]#
	}#
	GiR_stats = c()#
	nDocs = length(edge)#
	P = length(GiR_sample$b[[1]])#
	nIP = length(GiR_sample$b)#
	nwords = length(GiR_sample$text[[1]])#
	W = length(vocabulary)#
	GiR_stats[1:P] = Reduce('+', GiR_sample$b) / nIP#
	GiR_stats[P + 1] = mean(vapply(1:nDocs, function(d) {#
						length(edge[[d]][[2]])#
						}, c(1)))#
	GiR_stats[P + 2] = mean(vapply(2:nDocs, function(d) {#
					 edge[[d]][[3]] - edge[[d-1]][[3]]#
					 }, c(1))) 			#
	GiR_stats[P + 3] = mean(currentC)#
	Tokens_in_Topic = tabulate(vapply(1:nDocs, function(d){#
					  as.numeric(names(text[[d]]))#
					  }, rep(0, nwords)), K)#
	GiR_stats[(P + 4):(P + 3 + nIP)] = vapply(1:nIP, function(IP) {#
								 Tokens_in_Topic %*% (currentC == IP)#
								 }, c(1))#
	GiR_stats[(P + 4 + nIP):(P + 3 + nIP + K)] = Tokens_in_Topic#
	Tokens_in_Word = tabulate(vapply(1:nDocs, function(d){#
					 text[[d]]#
					 }, rep(0, nwords)), W)#
	GiR_stats[(P + 4 + nIP + K):(P + 3 + nIP + K + W)] = Tokens_in_Word#
	return(GiR_stats)#
}#
Inference = function(edge, node, textlist, vocabulary, nIP, K, sigma_Q, alpha, mvec, betas, nvec, prior.b.mean, prior.b.var, prior.eta, #
					out, n1, n2, n3, burn, thin, netstat, seed, plot = FALSE) {#
  set.seed(seed)#
  # trim the edge so that we only model edges after 16 days#
	timeinc = c(as.numeric(edge[[1]][3]), vapply(seq(along = edge)[-1], function(d) {#
  	as.numeric(edge[[d]][3]) - as.numeric(edge[[d-1]][3])#
 	}, c(1)))#
#
    edge2 = which_int(384, cumsum(timeinc)) : length(edge)#
  # initialize alpha, mvec, delta, nvec, eta, lvec, and gammas#
 	W = length(vocabulary)#
  	phi = lapply(1L:K, function(k) {#
		rdirichlet_cpp(1, betas * nvec)#
	})#
	eta = rnorm(1, prior.eta[1], prior.eta[2])#
	delta = pnorm(eta)#
	deltamat = delta#
	# initialize C, theta and Z#
	currentC = sample(1:nIP, K, replace = TRUE)#
	theta = rdirichlet_cpp(length(edge), alpha * mvec)#
	currentZ = lapply(seq(along = edge), function(d) {#
    if (length(textlist[[d]]) > 0) {#
    		multinom_vec(length(textlist[[d]]), theta[d, ])#
    	} else {#
    		multinom_vec(1, theta[d, ])#
    	}#
    })#
	p.d = t(vapply(seq(along = edge), function(d) {#
	  vapply(1L:nIP, function(IP) {#
	    sum(currentZ[[d]] %in% which(currentC == IP))#
	  }, c(1)) / length(currentZ[[d]])#
	}, rep(1, nIP)))#
#
    # initialize beta#
    sigma = 1#
    L = 3#
    P = 1 + L * (2 * ("dyadic" %in% netstat) + 4 * ("triadic" %in% netstat) + 2 *("degree" %in% netstat))#
    bmat = list()#
	for (IP in 1L:nIP) {#
		bmat[[IP]] = matrix(0, nrow = P, ncol = (n3 - burn) / thin)#
		bmat[[IP]][, 1:(n3 - burn) / thin] = c(rmvnorm(1, prior.b.mean, prior.b.var))#
  	}#
#
    # to check the convergence  #
    if (plot) {#
     	logWZ.mat = c()							  #
     	alpha.mat = c()#
     	entropy.mat = c()#
    }#
    iJi = lapply(seq(along = edge), function(d) {#
  	      matrix(0, nrow = length(node), ncol = length(node))#
  	      })  #
    #start outer iteration#
    for (o in 1L:out) {#
      Beta.old = lapply(bmat, function(b) {#
      			rowMeans(b)#
         		})  #
     # Data augmentation#
      lambda = list()#
      LambdaiJi = list()#
      nonemptyiJi = list()#
	  observediJi = list()#
      for (d in edge2) {#
   	 	history.t = History(edge, p.d, node, as.numeric(edge[[d]][3]))#
   	 	X = lapply(node, function(i) {#
  	        Netstats(history.t, node, i, netstat)#
            })#
   	 	XB = MultiplyXBList(X, Beta.old)     #
		lambda[[d]] = lambda_cpp(p.d[d,], XB)#
		#calculate the resampling probability#
		probij = DataAug_cpp(iJi[[d]], lambda[[d]], delta, timeinc[d])#
		iJi[[d]] = rbinom_mat(probij)#
		iJi[[d]][as.numeric(edge[[d]][1]),] = tabulateC(as.numeric(unlist(edge[[d]][2])), length(node))#
		LambdaiJi[[d]] = lambdaiJi(p.d[d,], XB, iJi[[d]])#
		nonemptyiJi[[d]] = LambdaiJi[[d]][!is.na(LambdaiJi[[d]])]#
		observediJi[[d]] = LambdaiJi[[d]][as.numeric(edge[[d]][1])]#
		}	 #
    textlist.raw = unlist(textlist)#
    table.W = lapply(1L:K, function(k) {#
      tabulateC(textlist.raw[which(unlist(currentZ) == k)], W)#
      })#
    for (i1 in 1L:n1) {#
      for (d in edge2) { #
        textlist.d = textlist[[d]]#
        if (length(textlist.d) > 0) {#
        	topicpart.d = TopicInEqZ(K, currentZ[[d]], alpha, mvec, d)#
       	wordpart.d = WordInEqZ(K, textlist.d, table.W, betas, nvec)#
        } else {#
        topicpart.d = 0#
        wordpart.d = matrix(0, nrow = length(currentZ[[d]]), ncol = K)#
        }#
        edgepart.d = EdgeInEqZ(iJi[[d]], lambda[[d]], delta)#
        timepart.d = TimeInEqZ(nonemptyiJi[[d]], timeinc[d])#
        observed.d = ObservedInEqZ(observediJi[[d]]) #
        fixedpart = topicpart.d + edgepart.d + timepart.d + observed.d #
        for (w in 1L:length(currentZ[[d]])) {#
          const.Z = fixedpart + wordpart.d[w, ]#
          const.Z = const.Z - max(const.Z)#
          zw.old = currentZ[[d]][w]#
          zw.new = multinom_vec(1, exp(const.Z))#
          if (zw.new != zw.old) {#
            currentZ[[d]][w] = zw.new#
            topicpart.d = TopicInEqZ(K, currentZ[[d]], alpha, mvec, d)#
            if (length(textlist.d) > 0) {	#
            wordpart.d = WordInEqZ(K, textlist.d, table.W, betas, nvec)#
            }#
            table.W = lapply(1L:K, function(k) {#
      				  tabulateC(textlist.raw[which(unlist(currentZ) == k)], W)#
      				  })#
      		p.d[d, ] = vapply(1L:nIP, function(IP) {#
	 			sum(currentZ[[d]] %in% which(currentC == IP))#
	 			}, c(1)) / length(currentZ[[d]])#
      		LambdaiJi[[d]] = lambdaiJi(p.d[d,], XB, iJi[[d]])#
			nonemptyiJi[[d]] = LambdaiJi[[d]][!is.na(LambdaiJi[[d]])]#
         	observediJi[[d]] = LambdaiJi[[d]][as.numeric(edge[[d]][1])]#
          }#
        }#
       }#
      }#
#
    for (i2 in 1L:n2) {#
      # C update given Z and B - within each document d#
      for (k in unique(unlist(currentZ[edge2]))) { #
        document.k = which(vapply(currentZ, function(d){k %in% d}, c(1)) == 1)#
        document.k = document.k[document.k %in% edge2]#
        const.C = rep(NA, nIP)#
        for (IP in 1L:nIP) {#
        	if (!currentC[k] == IP){#
          currentC[k] = IP#
          for (d in document.k) {#
            p.d[d, ] =  vapply(1L:nIP, function(IP) {#
              sum(currentZ[[d]] %in% which(currentC == IP))#
            }, c(1)) / length(currentZ[[d]])#
          }#
          for (d in edge2) {#
           history.t = History(edge, p.d, node, as.numeric(edge[[d]][3]))#
    	   	   X = lapply(node, function(i) {#
               Netstats(history.t, node, i, netstat)#
               })#
    	       XB = MultiplyXBList(X, Beta.old)    #
           lambda[[d]] = lambda_cpp(p.d[d,], XB)#
		   LambdaiJi[[d]] = lambdaiJi(p.d[d,], XB, iJi[[d]])#
		   nonemptyiJi[[d]] = LambdaiJi[[d]][!is.na(LambdaiJi[[d]])]#
           observediJi[[d]] = LambdaiJi[[d]][as.numeric(edge[[d]][1])]#
          }#
          }#
          const.C[IP] = sum(vapply(document.k, function(d) {#
            EdgeInEqZ(iJi[[d]], lambda[[d]], delta) + TimeInEqZ(nonemptyiJi[[d]], timeinc[d]) + #
    			ObservedInEqZ(observediJi[[d]]) #
          }, c(1))) / length(document.k)#
          }#
          const.C = const.C - max(const.C)#
          currentC[k] = multinom_vec(1, exp(const.C))#
      }#
    }#
    if (plot) {#
      entropy.mat = c(entropy.mat, entropy.empirical(currentC))#
      alpha.mat = rbind(alpha.mat, alpha)#
      logWZ.mat = c(logWZ.mat, logWZ(K, currentZ, textlist, table.W, alpha, mvec, betas, nvec))#
      }#
     for (d in edge2) {#
     	 p.d[d, ] =  vapply(1L:nIP, function(IP) {#
              		 sum(currentZ[[d]] %in% which(currentC == IP))#
                     }, c(1)) / length(currentZ[[d]])#
     	 history.t = History(edge, p.d, node, as.numeric(edge[[d]][3]))#
    	     X = lapply(node, function(i) {#
              Netstats(history.t, node, i, netstat)#
             })#
    	     XB = MultiplyXBList(X, Beta.old)   #
    	     lambda[[d]] = lambda_cpp(p.d[d,], XB)#
		 LambdaiJi[[d]] = lambdaiJi(p.d[d,], XB, iJi[[d]])#
		 nonemptyiJi[[d]] = LambdaiJi[[d]][!is.na(LambdaiJi[[d]])]#
         observediJi[[d]] = LambdaiJi[[d]][as.numeric(edge[[d]][1])]#
	 }#
	 # Beta and delta update#
	 prior.old1 = sum(vapply(1L:nIP, function(IP) {#
		  		 dmvnorm(Beta.old[[IP]], prior.b.mean, prior.b.var, log = TRUE)#
		  		 }, c(1))) #
	 post.old1 = sum(vapply(edge2, function(d) {#
	     	     EdgeInEqZ(iJi[[d]], lambda[[d]], delta) + TimeInEqZ(nonemptyiJi[[d]], timeinc[d]) +#
    			 	 ObservedInEqZ(observediJi[[d]])#
    			 }, c(1))) / length(edge2)#
    	 prior.old2 = dnorm(eta, prior.eta[1], prior.eta[2], log = TRUE) #
     post.old2 = sum(vapply(edge2, function(d) {#
    			    EdgeInEqZ(iJi[[d]], lambda[[d]], delta)#
    			    }, c(1))) / length(edge2)#
	 options(warn = -1)#
	 proposal.var = list()#
     for (IP in 1L:nIP) {#
     	 proposal.var[[IP]] = cor(t(bmat[[IP]]))#
         proposal.var[[IP]][is.na(proposal.var[[IP]])] = 0#
         if (sum(eigen(proposal.var[[IP]])$values < 0 ) > 0) {#
        	 proposal.var[[IP]] = diag(P)#
         }#
         }#
     proposal.var = lapply(1:nIP, function(IP){diag(P)})#
     options(warn = 0)#
     for (i3 in 1L:n3) {#
     	Beta.new = lapply(1L:nIP, function(IP) {#
           rmvnorm(1, Beta.old[[IP]], sigma_Q * proposal.var[[IP]])#
         }) #
        for (d in edge2) {#
           XB = MultiplyXBList(X, Beta.new)    #
           LambdaiJi[[d]] = lambdaiJi(p.d[d,], XB, iJi[[d]])#
		   nonemptyiJi[[d]] = LambdaiJi[[d]][!is.na(LambdaiJi[[d]])]#
		   observediJi[[d]] = LambdaiJi[[d]][as.numeric(edge[[d]][1])]#
         }#
         prior.new1 = sum(vapply(1L:nIP, function(IP) {#
        		dmvnorm(Beta.new[[IP]], prior.b.mean, prior.b.var, log = TRUE)#
         }, c(1))) #
         post.new1 = sum(vapply(edge2, function(d) {#
    			    EdgeInEqZ(iJi[[d]], lambda[[d]], delta) + TimeInEqZ(nonemptyiJi[[d]], timeinc[d]) + #
    			    ObservedInEqZ(observediJi[[d]])#
    			    }, c(1))) / length(edge2)#
    		 loglike.diff = prior.new1 + post.new1 - prior.old1 - post.old1#
        if (log(runif(1, 0, 1)) < loglike.diff) {#
         for (IP in 1L:nIP) {#
         Beta.old[[IP]]  = Beta.new[[IP]]#
         }#
         prior.old1 = prior.new1#
         post.old1 = post.new1#
         }#
        	 eta.new = rnorm(1, eta, sigma_Q)#
    	 	 delta.new = pnorm(eta.new)#
		 prior.new2 = dnorm(eta.new, prior.eta[1], prior.eta[2], log = TRUE)#
		 post.new2 = sum(vapply(edge2, function(d) {#
    			    EdgeInEqZ(iJi[[d]], lambda[[d]], delta.new)#
    			    }, c(1))) / length(edge2)#
    	 	 loglike.diff2 = prior.new2 + post.new2 - prior.old2 - post.old2#
		  if (log(runif(1, 0, 1)) < loglike.diff2) {#
			delta = delta.new#
			eta = eta.new#
			prior.old2 = prior.new2#
			post.old2 = post.new2#
			}#
#
         if (i3 > burn & i3 %% (thin) == 0) {#
          for (IP in 1L:nIP) {#
           bmat[[IP]][ , (i3 - burn) / thin] = Beta.old[[IP]]#
           }#
           deltamat = c(deltamat, delta)#
          }#
         }#
	 }#
    if (plot) {#
    burnin = round(out / 10)#
    par(mfrow = c(2, 2))#
  	plot(entropy.mat[-1L:-burnin], type = "l", #
  	     xlab = "(Outer) Iterations", ylab = "Entropy of IP")#
  	abline(h = mean(entropy.mat[-1L:-burnin]), lty = 1)#
  	title("Convergence of Entropy")#
  	matplot(alpha.mat[-1L:-burnin,], lty = 1, type = "l", col = 1L:nIP, #
  	        xlab = "(Outer) Iterations", ylab = "alpha")#
  	abline(h = mean(alpha.mat[-1L:-burnin,]), lty = 1, col = 1L:nIP)#
	title("Convergence of Optimized alpha")#
	plot(logWZ.mat[-1L:-burnin], type = "l", #
	       xlab = "(Outer) Iterations", ylab = "logWZ")#
	abline(h = mean(logWZ.mat[-1L:-burnin]), lty = 1)#
	title("Convergence of logWZ")#
	matplot(bmat[[1]][1,], lty = 1, col = 1L:P, type = "l", #
	          main = "Traceplot of beta", xlab = "(Inner) Iterations", ylab = "")#
	abline(h = mean(bmat[[1]][1,]), lty = 1, col = 1L)#
	  }#
  chain.final = list(C = currentC, Z = lapply(edge2, function(d) {currentZ[[d]]}), B = bmat, D = deltamat)#
#
  return(chain.final)#
}#
#
#PP_Plots#
GiR_PP_Plots = function(Forward_stats, Backward_stats) {#
	nms = colnames(Forward_stats)#
	for (i in 1L:ncol(Forward_stats)) {#
		if (nrow(Forward_stats) > 20000) {#
			thin = seq(from = floor(nrow(Forward_stats)/10), to = nrow(Forward_stats), length.out = 10000)#
			Forward_test = Forward_stats[thin, i]#
			Backward_test = Backward_stats[thin, i]#
		} else {#
			Forward_test = Forward_stats[, i]#
			Backward_test = Backward_stats[, i]#
		}#
		all = c(Backward_stats[, i], Forward_stats[, i])#
		ylims = c(min(all) - 0.1 * max(abs(all)), max(all) + 0.1 * max(abs(all)))#
		xlims = ylims#
		quantiles = 50#
		if (grepl("B_", nms[i]) ) {#
			quantiles = 1000#
		}#
		qqplot(x = quantile(Forward_stats[, i], seq(0, 1, length = quantiles)),#
			   y = quantile(Backward_stats[, i], seq(0, 1, length = quantiles)),#
			   ylim = ylims,#
			   xlim = xlims,#
			   ylab = "Backward",#
			   xlab = "Forward",#
			   col = "blue",#
			   pch = 19,#
			   main = nms[i],#
			   cex.lab = 1,#
			   cex.axis = 1,#
			   cex.main = 1)#
		lines(x = xlims, y = ylims, col = "red", lwd = 3)#
		text(paste("Backward Mean:", round(mean(Backward_stats[,i]), 4),#
				   "\nForward Mean:", round(mean(Forward_stats[,i]), 4),#
				   "\nt-test p-value:", round(t.test(Backward_test, Forward_test)$p.value, 4),#
				   "\nMann-Whitney p-value:", round(wilcox.test(Backward_test, Forward_test)$p.value,4)),#
				   x = xlims[2] - 0.35 * abs(xlims[2] - xlims[1]),#
				   y = ylims[1] + 0.15 * abs(ylims[2] - ylims[1]),#
				   cex = 0.3)#
	}#
}      #
#PP_Plots#
GiR_PP_Plots2 = function(Forward_stats, Backward_stats) {#
	nms = colnames(Forward_stats)#
	for (i in 1L:ncol(Forward_stats)) {#
		if (nrow(Forward_stats) > 20000) {#
			thin = seq(from = floor(nrow(Forward_stats)/10), to = nrow(Forward_stats), length.out = 10000)#
			Forward_test = Forward_stats[thin, i]#
			Backward_test = Backward_stats[thin, i]#
		} else {#
			Forward_test = Forward_stats[, i]#
			Backward_test = Backward_stats[, i]#
		}#
		all = c(Backward_stats[, i], Forward_stats[, i])#
		ylims = c(min(all) - 0.1 * max(abs(all)), max(all) + 0.1 * max(abs(all)))#
		xlims = ylims#
		quantiles = 50#
		if (grepl("B_", nms[i]) ) {#
			quantiles = 1000#
		}#
		normalmean = mean(c(quantile(Forward_stats[, i], seq(0, 1, length = quantiles)), #
							quantile(Backward_stats[, i], seq(0, 1, length = quantiles))))#
		normalvar = sd(c(quantile(Forward_stats[, i], seq(0, 1, length = quantiles)), #
							quantile(Backward_stats[, i], seq(0, 1, length = quantiles))))#
#
		qqplot(x = pnorm(sort(quantile(Forward_stats[, i], seq(0, 1, length = quantiles))), normalmean, normalvar),#
			   y = pnorm(sort(quantile(Backward_stats[, i], seq(0, 1, length = quantiles))), normalmean, normalvar),#
			   ylim = pnorm(ylims, normalmean, normalvar),#
			   xlim = pnorm(xlims, normalmean, normalvar),#
			   ylab = "Backward",#
			   xlab = "Forward",#
			   col = "blue",#
			   pch = 19,#
			   main = nms[i],#
			   cex.lab = 1,#
			   cex.axis = 1,#
			   cex.main = 1)#
		lines(x = pnorm(xlims, normalmean, normalvar), y = pnorm(ylims, normalmean, normalvar), col = "red", lwd = 2)#
		text(paste("Backward Mean:", round(mean(Backward_stats[,i]), 4),#
				   "\nForward Mean:", round(mean(Forward_stats[,i]), 4),#
				   "\nt-test p-value:", round(t.test(Backward_test, Forward_test)$p.value, 4),#
				   "\nMann-Whitney p-value:", round(wilcox.test(Backward_test, Forward_test)$p.value,4)),#
				   x = pnorm(xlims[2], normalmean, normalvar) - 0.35 * abs(pnorm(xlims[2], normalmean, normalvar) - pnorm(xlims[1], normalmean, normalvar)),#
				   y = pnorm(ylims[1], normalmean, normalvar) + 0.15 * abs(pnorm(ylims[2], normalmean, normalvar) - pnorm(ylims[1], normalmean, normalvar)),#
				   cex = 0.4)#
	}#
}      #
# Getting_It_Right#
GiR = function(Nsamp = 5000, nDocs = 5, node = 1:4, vocabulary =  c("hi", "hello","bye", "mine", "what"), #
			   nIP = 2, K = 4, nwords = 4, alpha = 2, mvec = rep(1/4, 4), betas = 2, nvec = rep(1/5, 5), #
			   prior.b.mean = c(-3, rep(0, 6)), prior.b.var = diag(7), prior.eta = c(0, 1), sigma_Q = 0.25, #
			   niters = c(1, 1, 1, 50, 0, 1), netstat = "dyadic", generate_PP_plots = TRUE, seed = 1) {#
	set.seed(seed)#
	b = lapply(1L:nIP, function(IP) {#
			   c(rmvnorm(1, prior.b.mean, prior.b.var))#
			   })#
	delta = 	rbeta(1, prior.delta[1], prior.delta[2])#
	currentC = sample(1L:nIP, K, replace = TRUE)	 #
	base.data = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, seed, #
							 base.edge = list(), base.text = list(), base = TRUE)#
	base.edge = base.data$edge	   #
	base.text = base.data$text#
#
	#Forward sampling#
	Forward_stats = matrix(NA, nrow = Nsamp, ncol = 21)#
	colnames(Forward_stats) = c(paste0("B_",1:length(b[[1]])), "Mean_receipients", "Mean_timediff", "Mean_TopicIP", #
						  paste0("Tokens_in_IP_", 1:nIP), paste0("Tokens_in_Topic", 1:K), #
						  paste0("Tokens_in_Word", 1:length(vocabulary)))#
	deltamat1 = c()#
	for (i in 1:Nsamp) { #
		if (i %% 5000 == 0) {cat("Forward sampling", i, "\n")}#
		set.seed(i)#
		b = lapply(1:nIP, function(IP) {#
			c(rmvnorm(1, prior.b.mean, prior.b.var))#
			})#
		delta = pnorm(rnorm(1, prior.eta[1], prior.eta[2]))#
		deltamat1 = c(deltamat1, delta)#
		currentC = sample(1L:nIP, K, replace = TRUE)#
		Forward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
									  base.edge = base.edge, base.text = base.text, seed, forward = TRUE) #
		Forward_stats[i, ] = GiR_stats(Forward_sample, K, currentC, vocabulary, forward = TRUE, backward = FALSE)#
	}#
	#Backward sampling#
	Backward_stats = matrix(NA, nrow = Nsamp, ncol = 21)#
	Backward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
								   base.edge = base.edge, base.text = base.text, seed, backward_init = TRUE) #
	deltamat2 = c()							   #
	for (i in 1:Nsamp) { #
		if (i %% 500 == 0) {cat("Backward sampling", i, "\n")}#
		seed = seed + 100#
		set.seed(seed)#
		Inference_samp = Inference(Backward_sample$edge, node, Backward_sample$text, vocabulary, nIP, K, sigma_Q, #
						  alpha, mvec, betas, nvec, prior.b.mean, prior.b.var, prior.eta,#
						  out = niters[1], n1 = niters[2], n2 = niters[3], n3 = niters[4], burn = niters[5], thin = niters[6], #
						  netstat, seed)#
		b = lapply(1:nIP, function(IP) {#
			rowMeans(Inference_samp$B[[IP]])#
		})#
		delta = mean(Inference_samp$D)#
		deltamat2 = c(deltamat2, delta)#
		currentC = Inference_samp$C#
		topic_token_assignments = Inference_samp$Z#
		for (d in 1:length(topic_token_assignments)) {#
			names(topic_token_assignments[[d]]) = Backward_sample$text[[d + length(base.text)]]#
		}#
		topic_token_counts = tabulate(unlist(Inference_samp$Z), K)#
		word_type_topic_counts = matrix(NA , length(vocabulary), K)#
		for (w in 1:length(vocabulary)) {#
			word_type_w = which(unlist(Backward_sample$text[-(1:length(base.text))]) == w)#
			word_type_topic_counts[w, ] = tabulate(unlist(Inference_samp$Z)[word_type_w], K)#
		}#
		Backward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
								   	   base.edge = base.edge, base.text = base.text, seed, topic_token_assignments = topic_token_assignments, #
								   	   topic_token_counts = topic_token_counts, word_type_topic_counts = word_type_topic_counts, #
								   	   forward = FALSE, backward = TRUE)#
		Backward_stats[i, ] = GiR_stats(Backward_sample, K, currentC, vocabulary, forward = FALSE, backward = TRUE)#
	}#
	tstats = rep(0, ncol(Forward_stats))#
	wstats = rep(0, ncol(Forward_stats))#
	for (j in 1:ncol(Forward_stats)) {#
		thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 400)#
		Forward_test = Forward_stats[thin, j]#
		Backward_test = Backward_stats[thin, j]#
		tstats[j] = t.test(Backward_test, Forward_test)$p.value#
		wstats[j] = wilcox.test(Backward_test, Forward_test)$p.value#
	}#
	names(tstats) = names(wstats) = colnames(Forward_stats)						#
	if (generate_PP_plots) {#
		par(mfrow=c(5,5), oma = c(3,3,3,3), mar = c(2,1,1,1))#
		GiR_PP_Plots2(Forward_stats, Backward_stats)#
	}			#
	return(list(Forward = Forward_stats, Backward = Backward_stats, tstats = tstats, wstats = wstats, delta1 = deltamat1, delta2 = deltamat2))	#
}#
#
unix.time(TryGiR <- GiR(5*10^3, seed = 100))
rm(list=ls())
library(mvtnorm)#
library(MCMCpack)#
library(entropy)#
library(Rcpp)#
library(RcppArmadillo)#
sourceCpp('~/Desktop/IPTM-master/pkg/src/sampler.cpp')#
source('~/Desktop/IPTM-master/pkg/R/core.R')#
GenerateDocs = function(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec,#
						b, delta, currentC, netstat, base.edge, base.text, seed,#
						topic_token_assignments = NULL, topic_token_counts = NULL, word_type_topic_counts = NULL, #
						forward = FALSE, backward_init = FALSE, backward = FALSE, base = FALSE) {#
	set.seed(seed)#
 	W = length(vocabulary)#
  	phi = lapply(1L:K, function(k) {#
		rdirichlet_cpp(1, betas * nvec)#
	})#
	L = 3#
    P = 1 + L * (2 * ("dyadic" %in% netstat) + 4 * ("triadic" %in% netstat) + 2 *("degree" %in% netstat))#
    if (base) {#
    	t.d = 0#
    } else {#
    	t.d = 384#
    }#
	edge = base.edge#
	text = base.text#
	p.d = matrix(NA, nrow = nDocs, ncol = nIP)#
	options(warn = -1)#
	for (d in 1:nDocs) {#
		N.d = nwords#
		text[[length(base.text) + d]] = rep(NA, N.d)#
		if (!backward) {#
			theta.d = rdirichlet_cpp(1, alpha * mvec)	#
			topic.d = multinom_vec(max(1, N.d), theta.d)#
			if (N.d > 0) {#
				for (n in 1:N.d){#
					text[[length(base.text) + d]][n] = multinom_vec(1, phi[[topic.d[n]]])#
				}#
				names(text[[length(base.text) + d]]) = topic.d#
			}#
		} else {#
			phi.k = rep(NA, K)#
			topic.d = topic_token_assignments[[d]]#
			word.d = as.numeric(names(topic_token_assignments[[d]]))#
			if (N.d > 0) {#
				for (n in 1:N.d){#
					word_type_topic_counts[word.d [n], topic.d[n]] = word_type_topic_counts[word.d [n], topic.d[n]] - 1#
					for (w in 1:W) {#
						phi.k[w] = (word_type_topic_counts[w, topic.d[n]] + betas * nvec[w])/(topic_token_counts[topic.d[n]] + betas)#
					} #
					text[[length(base.text) + d]][n] = multinom_vec(1, phi.k)#
					word_type_topic_counts[text[[d]][n], topic.d[n]] = word_type_topic_counts[text[[d]][n], topic.d[n]] + 1#
				}#
				names(text[[length(base.text) + d]]) = topic.d#
			}#
		}#
#
		p.d[d, ] = vapply(1L:nIP, function(IP) {#
			sum(topic.d %in% which(currentC == IP))#
			}, c(1)) / max(1, N.d)#
		if (base & t.d < 384) {#
			history.t = lapply(1:nIP, function(IP) {#
				lapply(1:3, function(l){#
				matrix(0, length(node), length(node))#
				})#
			})#
		} else {	#
		history.t = History(edge, p.d, node, t.d + 10^(-10))#
		}#
		X = lapply(node, function(i) {#
  	        Netstats(history.t, node, i, netstat)#
            })#
   	 	XB = MultiplyXBList(X, b)     #
		lambda = lambda_cpp(p.d[d,], XB)#
		iJi = rbinom_mat((delta * lambda) / (delta * lambda + 1))#
		while (sum(iJi) == 0) {#
			iJi = rbinom_mat((delta * lambda) / (delta * lambda + 1))#
		}#
		LambdaiJi = lambdaiJi(p.d[d,], XB, iJi)#
		Time.inc = vapply(LambdaiJi, function(lambda) {#
			rexp(1, lambda)#
			}, c(1))#
		i.d = which(Time.inc == min(Time.inc[!is.na(Time.inc)]))#
		j.d = which(iJi[i.d,] == 1)#
		t.d = t.d + Time.inc[i.d]#
		edge[[length(base.edge) + d]] = list(sender = i.d, receiver = j.d, timestamp = t.d)		#
		}#
	options(warn = 0)#
	if (forward) {#
		edge = edge[-(1:length(base.edge))]#
		text = text[-(1:length(base.text))]#
	}#
	if (base == TRUE & t.d > 384) {#
		cutoff = which_int(384, vapply(1:length(edge), function(d) {edge[[d]][[3]]}, c(1))) - 1#
		edge = edge[1:cutoff]#
		text = text[1:cutoff]#
	}#
	return(list(edge = edge, text = text, b = b, base = length(base.edge)))							#
} #
GiR_stats = function(GiR_sample, K, currentC, vocabulary, forward = FALSE, backward = FALSE) {#
	edge = GiR_sample$edge#
	text = GiR_sample$text#
	if (backward) {#
		edge = edge[-(1:GiR_sample$base)]#
		text = text[-(1:GiR_sample$base)]#
	}#
	GiR_stats = c()#
	nDocs = length(edge)#
	P = length(GiR_sample$b[[1]])#
	nIP = length(GiR_sample$b)#
	nwords = length(GiR_sample$text[[1]])#
	W = length(vocabulary)#
	GiR_stats[1:P] = Reduce('+', GiR_sample$b) / nIP#
	GiR_stats[P + 1] = mean(vapply(1:nDocs, function(d) {#
						length(edge[[d]][[2]])#
						}, c(1)))#
	GiR_stats[P + 2] = mean(vapply(2:nDocs, function(d) {#
					 edge[[d]][[3]] - edge[[d-1]][[3]]#
					 }, c(1))) 			#
	GiR_stats[P + 3] = mean(currentC)#
	Tokens_in_Topic = tabulate(vapply(1:nDocs, function(d){#
					  as.numeric(names(text[[d]]))#
					  }, rep(0, nwords)), K)#
	GiR_stats[(P + 4):(P + 3 + nIP)] = vapply(1:nIP, function(IP) {#
								 Tokens_in_Topic %*% (currentC == IP)#
								 }, c(1))#
	GiR_stats[(P + 4 + nIP):(P + 3 + nIP + K)] = Tokens_in_Topic#
	Tokens_in_Word = tabulate(vapply(1:nDocs, function(d){#
					 text[[d]]#
					 }, rep(0, nwords)), W)#
	GiR_stats[(P + 4 + nIP + K):(P + 3 + nIP + K + W)] = Tokens_in_Word#
	return(GiR_stats)#
}#
Inference = function(edge, node, textlist, vocabulary, nIP, K, sigma_Q, alpha, mvec, betas, nvec, prior.b.mean, prior.b.var, prior.eta, #
					out, n1, n2, n3, burn, thin, netstat, seed, plot = FALSE) {#
  set.seed(seed)#
  # trim the edge so that we only model edges after 16 days#
	timeinc = c(as.numeric(edge[[1]][3]), vapply(seq(along = edge)[-1], function(d) {#
  	as.numeric(edge[[d]][3]) - as.numeric(edge[[d-1]][3])#
 	}, c(1)))#
#
    edge2 = which_int(384, cumsum(timeinc)) : length(edge)#
  # initialize alpha, mvec, delta, nvec, eta, lvec, and gammas#
 	W = length(vocabulary)#
  	phi = lapply(1L:K, function(k) {#
		rdirichlet_cpp(1, betas * nvec)#
	})#
	eta = rnorm(1, prior.eta[1], prior.eta[2])#
	delta = pnorm(eta)#
	deltamat = delta#
	# initialize C, theta and Z#
	currentC = sample(1:nIP, K, replace = TRUE)#
	theta = rdirichlet_cpp(length(edge), alpha * mvec)#
	currentZ = lapply(seq(along = edge), function(d) {#
    if (length(textlist[[d]]) > 0) {#
    		multinom_vec(length(textlist[[d]]), theta[d, ])#
    	} else {#
    		multinom_vec(1, theta[d, ])#
    	}#
    })#
	p.d = t(vapply(seq(along = edge), function(d) {#
	  vapply(1L:nIP, function(IP) {#
	    sum(currentZ[[d]] %in% which(currentC == IP))#
	  }, c(1)) / length(currentZ[[d]])#
	}, rep(1, nIP)))#
#
    # initialize beta#
    sigma = 1#
    L = 3#
    P = 1 + L * (2 * ("dyadic" %in% netstat) + 4 * ("triadic" %in% netstat) + 2 *("degree" %in% netstat))#
    bmat = list()#
	for (IP in 1L:nIP) {#
		bmat[[IP]] = matrix(0, nrow = P, ncol = (n3 - burn) / thin)#
		bmat[[IP]][, 1:(n3 - burn) / thin] = c(rmvnorm(1, prior.b.mean, prior.b.var))#
  	}#
#
    # to check the convergence  #
    if (plot) {#
     	logWZ.mat = c()							  #
     	alpha.mat = c()#
     	entropy.mat = c()#
    }#
    iJi = lapply(seq(along = edge), function(d) {#
  	      matrix(0, nrow = length(node), ncol = length(node))#
  	      })  #
    #start outer iteration#
    for (o in 1L:out) {#
      Beta.old = lapply(bmat, function(b) {#
      			rowMeans(b)#
         		})  #
     # Data augmentation#
      lambda = list()#
      LambdaiJi = list()#
      nonemptyiJi = list()#
	  observediJi = list()#
      for (d in edge2) {#
   	 	history.t = History(edge, p.d, node, as.numeric(edge[[d]][3]))#
   	 	X = lapply(node, function(i) {#
  	        Netstats(history.t, node, i, netstat)#
            })#
   	 	XB = MultiplyXBList(X, Beta.old)     #
		lambda[[d]] = lambda_cpp(p.d[d,], XB)#
		#calculate the resampling probability#
		probij = DataAug_cpp(iJi[[d]], lambda[[d]], delta, timeinc[d])#
		iJi[[d]] = rbinom_mat(probij)#
		iJi[[d]][as.numeric(edge[[d]][1]),] = tabulateC(as.numeric(unlist(edge[[d]][2])), length(node))#
		LambdaiJi[[d]] = lambdaiJi(p.d[d,], XB, iJi[[d]])#
		nonemptyiJi[[d]] = LambdaiJi[[d]][!is.na(LambdaiJi[[d]])]#
		observediJi[[d]] = LambdaiJi[[d]][as.numeric(edge[[d]][1])]#
		}	 #
    textlist.raw = unlist(textlist)#
    table.W = lapply(1L:K, function(k) {#
      tabulateC(textlist.raw[which(unlist(currentZ) == k)], W)#
      })#
    for (i1 in 1L:n1) {#
      for (d in edge2) { #
        textlist.d = textlist[[d]]#
        if (length(textlist.d) > 0) {#
        	topicpart.d = TopicInEqZ(K, currentZ[[d]], alpha, mvec, d)#
       	wordpart.d = WordInEqZ(K, textlist.d, table.W, betas, nvec)#
        } else {#
        topicpart.d = 0#
        wordpart.d = matrix(0, nrow = length(currentZ[[d]]), ncol = K)#
        }#
        edgepart.d = EdgeInEqZ(iJi[[d]], lambda[[d]], delta)#
        timepart.d = TimeInEqZ(nonemptyiJi[[d]], timeinc[d])#
        observed.d = ObservedInEqZ(observediJi[[d]]) #
        fixedpart = topicpart.d + edgepart.d + timepart.d + observed.d #
        for (w in 1L:length(currentZ[[d]])) {#
          const.Z = fixedpart + wordpart.d[w, ]#
          const.Z = const.Z - max(const.Z)#
          zw.old = currentZ[[d]][w]#
          zw.new = multinom_vec(1, exp(const.Z))#
          if (zw.new != zw.old) {#
            currentZ[[d]][w] = zw.new#
            topicpart.d = TopicInEqZ(K, currentZ[[d]], alpha, mvec, d)#
            if (length(textlist.d) > 0) {	#
            wordpart.d = WordInEqZ(K, textlist.d, table.W, betas, nvec)#
            }#
            table.W = lapply(1L:K, function(k) {#
      				  tabulateC(textlist.raw[which(unlist(currentZ) == k)], W)#
      				  })#
      		p.d[d, ] = vapply(1L:nIP, function(IP) {#
	 			sum(currentZ[[d]] %in% which(currentC == IP))#
	 			}, c(1)) / length(currentZ[[d]])#
      		LambdaiJi[[d]] = lambdaiJi(p.d[d,], XB, iJi[[d]])#
			nonemptyiJi[[d]] = LambdaiJi[[d]][!is.na(LambdaiJi[[d]])]#
         	observediJi[[d]] = LambdaiJi[[d]][as.numeric(edge[[d]][1])]#
          }#
        }#
       }#
      }#
#
    for (i2 in 1L:n2) {#
      # C update given Z and B - within each document d#
      for (k in unique(unlist(currentZ[edge2]))) { #
        document.k = which(vapply(currentZ, function(d){k %in% d}, c(1)) == 1)#
        document.k = document.k[document.k %in% edge2]#
        const.C = rep(NA, nIP)#
        for (IP in 1L:nIP) {#
        	if (!currentC[k] == IP){#
          currentC[k] = IP#
          for (d in document.k) {#
            p.d[d, ] =  vapply(1L:nIP, function(IP) {#
              sum(currentZ[[d]] %in% which(currentC == IP))#
            }, c(1)) / length(currentZ[[d]])#
          }#
          for (d in edge2) {#
           history.t = History(edge, p.d, node, as.numeric(edge[[d]][3]))#
    	   	   X = lapply(node, function(i) {#
               Netstats(history.t, node, i, netstat)#
               })#
    	       XB = MultiplyXBList(X, Beta.old)    #
           lambda[[d]] = lambda_cpp(p.d[d,], XB)#
		   LambdaiJi[[d]] = lambdaiJi(p.d[d,], XB, iJi[[d]])#
		   nonemptyiJi[[d]] = LambdaiJi[[d]][!is.na(LambdaiJi[[d]])]#
           observediJi[[d]] = LambdaiJi[[d]][as.numeric(edge[[d]][1])]#
          }#
          }#
          const.C[IP] = sum(vapply(document.k, function(d) {#
            EdgeInEqZ(iJi[[d]], lambda[[d]], delta) + TimeInEqZ(nonemptyiJi[[d]], timeinc[d]) + #
    			ObservedInEqZ(observediJi[[d]]) #
          }, c(1))) / length(document.k)#
          }#
          const.C = const.C - max(const.C)#
          currentC[k] = multinom_vec(1, exp(const.C))#
      }#
    }#
    if (plot) {#
      entropy.mat = c(entropy.mat, entropy.empirical(currentC))#
      alpha.mat = rbind(alpha.mat, alpha)#
      logWZ.mat = c(logWZ.mat, logWZ(K, currentZ, textlist, table.W, alpha, mvec, betas, nvec))#
      }#
     for (d in edge2) {#
     	 p.d[d, ] =  vapply(1L:nIP, function(IP) {#
              		 sum(currentZ[[d]] %in% which(currentC == IP))#
                     }, c(1)) / length(currentZ[[d]])#
     	 history.t = History(edge, p.d, node, as.numeric(edge[[d]][3]))#
    	     X = lapply(node, function(i) {#
              Netstats(history.t, node, i, netstat)#
             })#
    	     XB = MultiplyXBList(X, Beta.old)   #
    	     lambda[[d]] = lambda_cpp(p.d[d,], XB)#
		 LambdaiJi[[d]] = lambdaiJi(p.d[d,], XB, iJi[[d]])#
		 nonemptyiJi[[d]] = LambdaiJi[[d]][!is.na(LambdaiJi[[d]])]#
         observediJi[[d]] = LambdaiJi[[d]][as.numeric(edge[[d]][1])]#
	 }#
	 # Beta and delta update#
	 prior.old1 = sum(vapply(1L:nIP, function(IP) {#
		  		 dmvnorm(Beta.old[[IP]], prior.b.mean, prior.b.var, log = TRUE)#
		  		 }, c(1))) #
	 post.old1 = sum(vapply(edge2, function(d) {#
	     	     EdgeInEqZ(iJi[[d]], lambda[[d]], delta) + TimeInEqZ(nonemptyiJi[[d]], timeinc[d]) +#
    			 	 ObservedInEqZ(observediJi[[d]])#
    			 }, c(1))) / length(edge2)#
    	 prior.old2 = dnorm(eta, prior.eta[1], prior.eta[2], log = TRUE) #
     post.old2 = sum(vapply(edge2, function(d) {#
    			    EdgeInEqZ(iJi[[d]], lambda[[d]], delta)#
    			    }, c(1))) / length(edge2)#
	 options(warn = -1)#
	 proposal.var = list()#
     for (IP in 1L:nIP) {#
     	 proposal.var[[IP]] = cor(t(bmat[[IP]]))#
         proposal.var[[IP]][is.na(proposal.var[[IP]])] = 0#
         if (sum(eigen(proposal.var[[IP]])$values < 0 ) > 0) {#
        	 proposal.var[[IP]] = diag(P)#
         }#
         }#
     proposal.var = lapply(1:nIP, function(IP){diag(P)})#
     options(warn = 0)#
     for (i3 in 1L:n3) {#
     	Beta.new = lapply(1L:nIP, function(IP) {#
           rmvnorm(1, Beta.old[[IP]], sigma_Q * proposal.var[[IP]])#
         }) #
        for (d in edge2) {#
           XB = MultiplyXBList(X, Beta.new)    #
           LambdaiJi[[d]] = lambdaiJi(p.d[d,], XB, iJi[[d]])#
		   nonemptyiJi[[d]] = LambdaiJi[[d]][!is.na(LambdaiJi[[d]])]#
		   observediJi[[d]] = LambdaiJi[[d]][as.numeric(edge[[d]][1])]#
         }#
         prior.new1 = sum(vapply(1L:nIP, function(IP) {#
        		dmvnorm(Beta.new[[IP]], prior.b.mean, prior.b.var, log = TRUE)#
         }, c(1))) #
         post.new1 = sum(vapply(edge2, function(d) {#
    			    EdgeInEqZ(iJi[[d]], lambda[[d]], delta) + TimeInEqZ(nonemptyiJi[[d]], timeinc[d]) + #
    			    ObservedInEqZ(observediJi[[d]])#
    			    }, c(1))) / length(edge2)#
    		 loglike.diff = prior.new1 + post.new1 - prior.old1 - post.old1#
        if (log(runif(1, 0, 1)) < loglike.diff) {#
         for (IP in 1L:nIP) {#
         Beta.old[[IP]]  = Beta.new[[IP]]#
         }#
         prior.old1 = prior.new1#
         post.old1 = post.new1#
         }#
        	 eta.new = rnorm(1, eta, sigma_Q)#
    	 	 delta.new = pnorm(eta.new)#
		 prior.new2 = dnorm(eta.new, prior.eta[1], prior.eta[2], log = TRUE)#
		 post.new2 = sum(vapply(edge2, function(d) {#
    			    EdgeInEqZ(iJi[[d]], lambda[[d]], delta.new)#
    			    }, c(1))) / length(edge2)#
    	 	 loglike.diff2 = prior.new2 + post.new2 - prior.old2 - post.old2#
		  if (log(runif(1, 0, 1)) < loglike.diff2) {#
			delta = delta.new#
			eta = eta.new#
			prior.old2 = prior.new2#
			post.old2 = post.new2#
			}#
#
         if (i3 > burn & i3 %% (thin) == 0) {#
          for (IP in 1L:nIP) {#
           bmat[[IP]][ , (i3 - burn) / thin] = Beta.old[[IP]]#
           }#
           deltamat = c(deltamat, delta)#
          }#
         }#
	 }#
    if (plot) {#
    burnin = round(out / 10)#
    par(mfrow = c(2, 2))#
  	plot(entropy.mat[-1L:-burnin], type = "l", #
  	     xlab = "(Outer) Iterations", ylab = "Entropy of IP")#
  	abline(h = mean(entropy.mat[-1L:-burnin]), lty = 1)#
  	title("Convergence of Entropy")#
  	matplot(alpha.mat[-1L:-burnin,], lty = 1, type = "l", col = 1L:nIP, #
  	        xlab = "(Outer) Iterations", ylab = "alpha")#
  	abline(h = mean(alpha.mat[-1L:-burnin,]), lty = 1, col = 1L:nIP)#
	title("Convergence of Optimized alpha")#
	plot(logWZ.mat[-1L:-burnin], type = "l", #
	       xlab = "(Outer) Iterations", ylab = "logWZ")#
	abline(h = mean(logWZ.mat[-1L:-burnin]), lty = 1)#
	title("Convergence of logWZ")#
	matplot(bmat[[1]][1,], lty = 1, col = 1L:P, type = "l", #
	          main = "Traceplot of beta", xlab = "(Inner) Iterations", ylab = "")#
	abline(h = mean(bmat[[1]][1,]), lty = 1, col = 1L)#
	  }#
  chain.final = list(C = currentC, Z = lapply(edge2, function(d) {currentZ[[d]]}), B = bmat, D = deltamat)#
#
  return(chain.final)#
}#
#
#QQ_Plots#
GiR_QQ_Plots = function(Forward_stats, Backward_stats) {#
	nms = colnames(Forward_stats)#
	for (i in 1L:ncol(Forward_stats)) {#
		if (nrow(Forward_stats) > 20000) {#
			thin = seq(from = floor(nrow(Forward_stats)/10), to = nrow(Forward_stats), length.out = 10000)#
			Forward_test = Forward_stats[thin, i]#
			Backward_test = Backward_stats[thin, i]#
		} else {#
			Forward_test = Forward_stats[, i]#
			Backward_test = Backward_stats[, i]#
		}#
		all = c(Backward_stats[, i], Forward_stats[, i])#
		ylims = c(min(all) - 0.1 * max(abs(all)), max(all) + 0.1 * max(abs(all)))#
		xlims = ylims#
		quantiles = 50#
		if (grepl("B_", nms[i]) ) {#
			quantiles = 1000#
		}#
		qqplot(x = quantile(Forward_stats[, i], seq(0, 1, length = quantiles)),#
			   y = quantile(Backward_stats[, i], seq(0, 1, length = quantiles)),#
			   ylim = ylims,#
			   xlim = xlims,#
			   ylab = "Backward",#
			   xlab = "Forward",#
			   col = "blue",#
			   pch = 19,#
			   main = nms[i],#
			   cex.lab = 1,#
			   cex.axis = 1,#
			   cex.main = 1)#
		lines(x = xlims, y = ylims, col = "red", lwd = 3)#
		text(paste("Backward Mean:", round(mean(Backward_stats[,i]), 4),#
				   "\nForward Mean:", round(mean(Forward_stats[,i]), 4),#
				   "\nt-test p-value:", round(t.test(Backward_test, Forward_test)$p.value, 4),#
				   "\nMann-Whitney p-value:", round(wilcox.test(Backward_test, Forward_test)$p.value,4)),#
				   x = xlims[2] - 0.35 * abs(xlims[2] - xlims[1]),#
				   y = ylims[1] + 0.15 * abs(ylims[2] - ylims[1]),#
				   cex = 0.3)#
	}#
}      #
#PP_Plots#
GiR_PP_Plots = function(Forward_stats, Backward_stats) {#
	nms = colnames(Forward_stats)#
	for (i in 1L:ncol(Forward_stats)) {#
		if (nrow(Forward_stats) > 20000) {#
			thin = seq(from = floor(nrow(Forward_stats)/10), to = nrow(Forward_stats), length.out = 10000)#
			Forward_test = Forward_stats[thin, i]#
			Backward_test = Backward_stats[thin, i]#
		} else {#
			Forward_test = Forward_stats[, i]#
			Backward_test = Backward_stats[, i]#
		}#
		all = c(Backward_stats[, i], Forward_stats[, i])#
		ylims = c(min(all) - 0.1 * max(abs(all)), max(all) + 0.1 * max(abs(all)))#
		xlims = ylims#
		quantiles = 50#
		if (grepl("B_", nms[i]) ) {#
			quantiles = 1000#
		}#
		normalmean = mean(c(quantile(Forward_stats[, i], seq(0, 1, length = quantiles)), #
							quantile(Backward_stats[, i], seq(0, 1, length = quantiles))))#
		normalvar = sd(c(quantile(Forward_stats[, i], seq(0, 1, length = quantiles)), #
							quantile(Backward_stats[, i], seq(0, 1, length = quantiles))))#
#
		qqplot(x = pnorm(sort(quantile(Forward_stats[, i], seq(0, 1, length = quantiles))), normalmean, normalvar),#
			   y = pnorm(sort(quantile(Backward_stats[, i], seq(0, 1, length = quantiles))), normalmean, normalvar),#
			   ylim = pnorm(ylims, normalmean, normalvar),#
			   xlim = pnorm(xlims, normalmean, normalvar),#
			   ylab = "Backward",#
			   xlab = "Forward",#
			   col = "blue",#
			   pch = 19,#
			   main = nms[i],#
			   cex.lab = 1,#
			   cex.axis = 1,#
			   cex.main = 1)#
		lines(x = pnorm(xlims, normalmean, normalvar), y = pnorm(ylims, normalmean, normalvar), col = "red", lwd = 2)#
		text(paste("Backward Mean:", round(mean(Backward_stats[,i]), 4),#
				   "\nForward Mean:", round(mean(Forward_stats[,i]), 4),#
				   "\nt-test p-value:", round(t.test(Backward_test, Forward_test)$p.value, 4),#
				   "\nMann-Whitney p-value:", round(wilcox.test(Backward_test, Forward_test)$p.value,4)),#
				   x = pnorm(xlims[2], normalmean, normalvar) - 0.35 * abs(pnorm(xlims[2], normalmean, normalvar) - pnorm(xlims[1], normalmean, normalvar)),#
				   y = pnorm(ylims[1], normalmean, normalvar) + 0.15 * abs(pnorm(ylims[2], normalmean, normalvar) - pnorm(ylims[1], normalmean, normalvar)),#
				   cex = 0.4)#
	}#
}      #
# Getting_It_Right#
GiR = function(Nsamp = 5000, nDocs = 5, node = 1:4, vocabulary =  c("hi", "hello","bye", "mine", "what"), #
			   nIP = 2, K = 4, nwords = 4, alpha = 2, mvec = rep(1/4, 4), betas = 2, nvec = rep(1/5, 5), #
			   prior.b.mean = c(-3, rep(0, 6)), prior.b.var = diag(7), prior.eta = c(0, 1), sigma_Q = 0.25, #
			   niters = c(1, 1, 1, 50, 0, 1), netstat = "dyadic", generate_PP_plots = TRUE, seed = 1) {#
	set.seed(seed)#
	b = lapply(1L:nIP, function(IP) {#
			   c(rmvnorm(1, prior.b.mean, prior.b.var))#
			   })#
	delta = pnorm(rnorm(1, prior.eta[1], prior.eta[2]))#
	currentC = sample(1L:nIP, K, replace = TRUE)	 #
	base.data = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, seed, #
							 base.edge = list(), base.text = list(), base = TRUE)#
	base.edge = base.data$edge	   #
	base.text = base.data$text#
#
	#Forward sampling#
	Forward_stats = matrix(NA, nrow = Nsamp, ncol = 21)#
	colnames(Forward_stats) = c(paste0("B_",1:length(b[[1]])), "Mean_receipients", "Mean_timediff", "Mean_TopicIP", #
						  paste0("Tokens_in_IP_", 1:nIP), paste0("Tokens_in_Topic", 1:K), #
						  paste0("Tokens_in_Word", 1:length(vocabulary)))#
	deltamat1 = c()#
	for (i in 1:Nsamp) { #
		if (i %% 5000 == 0) {cat("Forward sampling", i, "\n")}#
		set.seed(i)#
		b = lapply(1:nIP, function(IP) {#
			c(rmvnorm(1, prior.b.mean, prior.b.var))#
			})#
		delta = pnorm(rnorm(1, prior.eta[1], prior.eta[2]))#
		deltamat1 = c(deltamat1, delta)#
		currentC = sample(1L:nIP, K, replace = TRUE)#
		Forward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
									  base.edge = base.edge, base.text = base.text, seed, forward = TRUE) #
		Forward_stats[i, ] = GiR_stats(Forward_sample, K, currentC, vocabulary, forward = TRUE, backward = FALSE)#
	}#
	#Backward sampling#
	Backward_stats = matrix(NA, nrow = Nsamp, ncol = 21)#
	Backward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
								   base.edge = base.edge, base.text = base.text, seed, backward_init = TRUE) #
	deltamat2 = c()							   #
	for (i in 1:Nsamp) { #
		if (i %% 500 == 0) {cat("Backward sampling", i, "\n")}#
		seed = seed + 100#
		set.seed(seed)#
		Inference_samp = Inference(Backward_sample$edge, node, Backward_sample$text, vocabulary, nIP, K, sigma_Q, #
						  alpha, mvec, betas, nvec, prior.b.mean, prior.b.var, prior.eta,#
						  out = niters[1], n1 = niters[2], n2 = niters[3], n3 = niters[4], burn = niters[5], thin = niters[6], #
						  netstat, seed)#
		b = lapply(1:nIP, function(IP) {#
			rowMeans(Inference_samp$B[[IP]])#
		})#
		delta = mean(Inference_samp$D)#
		deltamat2 = c(deltamat2, delta)#
		currentC = Inference_samp$C#
		topic_token_assignments = Inference_samp$Z#
		for (d in 1:length(topic_token_assignments)) {#
			names(topic_token_assignments[[d]]) = Backward_sample$text[[d + length(base.text)]]#
		}#
		topic_token_counts = tabulate(unlist(Inference_samp$Z), K)#
		word_type_topic_counts = matrix(NA , length(vocabulary), K)#
		for (w in 1:length(vocabulary)) {#
			word_type_w = which(unlist(Backward_sample$text[-(1:length(base.text))]) == w)#
			word_type_topic_counts[w, ] = tabulate(unlist(Inference_samp$Z)[word_type_w], K)#
		}#
		Backward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
								   	   base.edge = base.edge, base.text = base.text, seed, topic_token_assignments = topic_token_assignments, #
								   	   topic_token_counts = topic_token_counts, word_type_topic_counts = word_type_topic_counts, #
								   	   forward = FALSE, backward = TRUE)#
		Backward_stats[i, ] = GiR_stats(Backward_sample, K, currentC, vocabulary, forward = FALSE, backward = TRUE)#
	}#
	tstats = rep(0, ncol(Forward_stats))#
	wstats = rep(0, ncol(Forward_stats))#
	for (j in 1:ncol(Forward_stats)) {#
		thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 400)#
		Forward_test = Forward_stats[thin, j]#
		Backward_test = Backward_stats[thin, j]#
		tstats[j] = t.test(Backward_test, Forward_test)$p.value#
		wstats[j] = wilcox.test(Backward_test, Forward_test)$p.value#
	}#
	names(tstats) = names(wstats) = colnames(Forward_stats)						#
	if (generate_PP_plots) {#
		par(mfrow=c(5,5), oma = c(3,3,3,3), mar = c(2,1,1,1))#
		GiR_PP_Plots(Forward_stats, Backward_stats)#
	}			#
	return(list(Forward = Forward_stats, Backward = Backward_stats, tstats = tstats, wstats = wstats, delta1 = deltamat1, delta2 = deltamat2))	#
}#
#
unix.time(TryGiR <- GiR(5*10^3, seed = 100))#
#
matplot(cbind(TryGiR$delta1, TryGiR$delta2),type = 'l', col = 1:2)
matplot(cbind(TryGiR$delta1, TryGiR$delta2),type = 'l', col = 1:2)
# Getting_It_Right#
GiR = function(Nsamp = 5000, nDocs = 5, node = 1:4, vocabulary =  c("hi", "hello","bye", "mine", "what"), #
			   nIP = 2, K = 4, nwords = 4, alpha = 2, mvec = rep(1/4, 4), betas = 2, nvec = rep(1/5, 5), #
			   prior.b.mean = c(-3, rep(0, 6)), prior.b.var = diag(7), prior.eta = c(0, 1), sigma_Q = 0.25, #
			   niters = c(1, 1, 1, 50, 0, 1), netstat = "dyadic", generate_PP_plots = TRUE, seed = 1) {#
	set.seed(seed)#
	P = 1 + 3 * (2 * ("dyadic" %in% netstat) + 4 * ("triadic" %in% netstat) + 2 *("degree" %in% netstat))#
	b = lapply(1L:nIP, function(IP) {#
			   c(rmvnorm(1, prior.b.mean, prior.b.var))#
			   })#
	delta = pnorm(rnorm(1, prior.eta[1], prior.eta[2]))#
	currentC = sample(1L:nIP, K, replace = TRUE)	 #
	base.data = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, seed, #
							 base.edge = list(), base.text = list(), base = TRUE)#
	base.edge = base.data$edge	   #
	base.text = base.data$text#
#
	#Forward sampling#
	Forward_stats = matrix(NA, nrow = Nsamp, ncol = 21)#
	colnames(Forward_stats) = c(paste0("B_",1:length(b[[1]])), "Mean_receipients", "Mean_timediff", "Mean_TopicIP", #
						  paste0("Tokens_in_IP_", 1:nIP), paste0("Tokens_in_Topic", 1:K), #
						  paste0("Tokens_in_Word", 1:length(vocabulary)))#
	deltamat1 = rep(NA, Nsamp)#
	bmat1 = matrix(NA, nrow = Nsamp, ncol = P)#
	entropy1 = rep(NA, Nsamp)#
	for (i in 1:Nsamp) { #
		if (i %% 5000 == 0) {cat("Forward sampling", i, "\n")}#
		set.seed(i)#
		b = lapply(1:nIP, function(IP) {#
			c(rmvnorm(1, prior.b.mean, prior.b.var))#
			})#
		delta = pnorm(rnorm(1, prior.eta[1], prior.eta[2]))#
		currentC = sample(1L:nIP, K, replace = TRUE)#
		Forward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
									  base.edge = base.edge, base.text = base.text, seed, forward = TRUE) #
		Forward_stats[i, ] = GiR_stats(Forward_sample, K, currentC, vocabulary, forward = TRUE, backward = FALSE)#
		bmat1[i, ] = b#
		deltamat1[i] = delta#
		entropy1[i] = entropy.empirical(currentC)#
	}#
	#Backward sampling#
	Backward_stats = matrix(NA, nrow = Nsamp, ncol = 21)#
	Backward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
								   base.edge = base.edge, base.text = base.text, seed, backward_init = TRUE) #
	deltamat2 = rep(NA, Nsamp)#
	bmat2 = matrix(NA, nrow = Nsamp, ncol = P)		#
	entropy2 = rep(NA, Nsamp)		   #
	for (i in 1:Nsamp) { #
		if (i %% 500 == 0) {cat("Backward sampling", i, "\n")}#
		seed = seed + 100#
		set.seed(seed)#
		Inference_samp = Inference(Backward_sample$edge, node, Backward_sample$text, vocabulary, nIP, K, sigma_Q, #
						  alpha, mvec, betas, nvec, prior.b.mean, prior.b.var, prior.eta,#
						  out = niters[1], n1 = niters[2], n2 = niters[3], n3 = niters[4], burn = niters[5], thin = niters[6], #
						  netstat, seed)#
		b = lapply(1:nIP, function(IP) {#
			rowMeans(Inference_samp$B[[IP]])#
		})#
		delta = mean(Inference_samp$D)#
		currentC = Inference_samp$C#
		topic_token_assignments = Inference_samp$Z#
		for (d in 1:length(topic_token_assignments)) {#
			names(topic_token_assignments[[d]]) = Backward_sample$text[[d + length(base.text)]]#
		}#
		topic_token_counts = tabulate(unlist(Inference_samp$Z), K)#
		word_type_topic_counts = matrix(NA , length(vocabulary), K)#
		for (w in 1:length(vocabulary)) {#
			word_type_w = which(unlist(Backward_sample$text[-(1:length(base.text))]) == w)#
			word_type_topic_counts[w, ] = tabulate(unlist(Inference_samp$Z)[word_type_w], K)#
		}#
		Backward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
								   	   base.edge = base.edge, base.text = base.text, seed, topic_token_assignments = topic_token_assignments, #
								   	   topic_token_counts = topic_token_counts, word_type_topic_counts = word_type_topic_counts, #
								   	   forward = FALSE, backward = TRUE)#
		Backward_stats[i, ] = GiR_stats(Backward_sample, K, currentC, vocabulary, forward = FALSE, backward = TRUE)#
		bmat2[i, ] = b#
		deltamat2[i] = delta#
		entropy2[i] = entropy.empirical(currentC)#
	}#
	tstats = rep(0, ncol(Forward_stats))#
	wstats = rep(0, ncol(Forward_stats))#
	for (j in 1:ncol(Forward_stats)) {#
		thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 400)#
		Forward_test = Forward_stats[thin, j]#
		Backward_test = Backward_stats[thin, j]#
		tstats[j] = t.test(Backward_test, Forward_test)$p.value#
		wstats[j] = wilcox.test(Backward_test, Forward_test)$p.value#
	}#
	names(tstats) = names(wstats) = colnames(Forward_stats)						#
	if (generate_PP_plots) {#
		par(mfrow=c(5,5), oma = c(3,3,3,3), mar = c(2,1,1,1))#
		GiR_PP_Plots(Forward_stats, Backward_stats)#
	}			#
	return(list(Forward = Forward_stats, Backward = Backward_stats, tstats = tstats, wstats = wstats, delta1 = deltamat1, delta2 = deltamat2))	#
}
unix.time(TryGiR <- GiR(5*10^3, seed = 100))
# Getting_It_Right#
GiR = function(Nsamp = 5000, nDocs = 5, node = 1:4, vocabulary =  c("hi", "hello","bye", "mine", "what"), #
			   nIP = 2, K = 4, nwords = 4, alpha = 2, mvec = rep(1/4, 4), betas = 2, nvec = rep(1/5, 5), #
			   prior.b.mean = c(-3, rep(0, 6)), prior.b.var = diag(7), prior.eta = c(0, 1), sigma_Q = 0.25, #
			   niters = c(1, 1, 1, 50, 0, 1), netstat = "dyadic", generate_PP_plots = TRUE, seed = 1) {#
	set.seed(seed)#
	P = 1 + 3 * (2 * ("dyadic" %in% netstat) + 4 * ("triadic" %in% netstat) + 2 *("degree" %in% netstat))#
	b = lapply(1L:nIP, function(IP) {#
			   c(rmvnorm(1, prior.b.mean, prior.b.var))#
			   })#
	delta = pnorm(rnorm(1, prior.eta[1], prior.eta[2]))#
	currentC = sample(1L:nIP, K, replace = TRUE)	 #
	base.data = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, seed, #
							 base.edge = list(), base.text = list(), base = TRUE)#
	base.edge = base.data$edge	   #
	base.text = base.data$text#
#
	#Forward sampling#
	Forward_stats = matrix(NA, nrow = Nsamp, ncol = 21)#
	colnames(Forward_stats) = c(paste0("B_",1:length(b[[1]])), "Mean_receipients", "Mean_timediff", "Mean_TopicIP", #
						  paste0("Tokens_in_IP_", 1:nIP), paste0("Tokens_in_Topic", 1:K), #
						  paste0("Tokens_in_Word", 1:length(vocabulary)))#
	deltamat1 = rep(NA, Nsamp)#
	bmat1 = matrix(NA, nrow = Nsamp, ncol = nIP * P)#
	entropy1 = rep(NA, Nsamp)#
	for (i in 1:Nsamp) { #
		if (i %% 5000 == 0) {cat("Forward sampling", i, "\n")}#
		set.seed(i)#
		b = lapply(1:nIP, function(IP) {#
			c(rmvnorm(1, prior.b.mean, prior.b.var))#
			})#
		delta = pnorm(rnorm(1, prior.eta[1], prior.eta[2]))#
		currentC = sample(1L:nIP, K, replace = TRUE)#
		Forward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
									  base.edge = base.edge, base.text = base.text, seed, forward = TRUE) #
		Forward_stats[i, ] = GiR_stats(Forward_sample, K, currentC, vocabulary, forward = TRUE, backward = FALSE)#
		bmat1[i, ] = unlist(b)#
		deltamat1[i] = delta#
		entropy1[i] = entropy.empirical(currentC)#
	}#
	#Backward sampling#
	Backward_stats = matrix(NA, nrow = Nsamp, ncol = 21)#
	Backward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
								   base.edge = base.edge, base.text = base.text, seed, backward_init = TRUE) #
	deltamat2 = rep(NA, Nsamp)#
	bmat2 = matrix(NA, nrow = Nsamp, ncol = nIP * P)		#
	entropy2 = rep(NA, Nsamp)		   #
	for (i in 1:Nsamp) { #
		if (i %% 500 == 0) {cat("Backward sampling", i, "\n")}#
		seed = seed + 100#
		set.seed(seed)#
		Inference_samp = Inference(Backward_sample$edge, node, Backward_sample$text, vocabulary, nIP, K, sigma_Q, #
						  alpha, mvec, betas, nvec, prior.b.mean, prior.b.var, prior.eta,#
						  out = niters[1], n1 = niters[2], n2 = niters[3], n3 = niters[4], burn = niters[5], thin = niters[6], #
						  netstat, seed)#
		b = lapply(1:nIP, function(IP) {#
			rowMeans(Inference_samp$B[[IP]])#
		})#
		delta = mean(Inference_samp$D)#
		currentC = Inference_samp$C#
		topic_token_assignments = Inference_samp$Z#
		for (d in 1:length(topic_token_assignments)) {#
			names(topic_token_assignments[[d]]) = Backward_sample$text[[d + length(base.text)]]#
		}#
		topic_token_counts = tabulate(unlist(Inference_samp$Z), K)#
		word_type_topic_counts = matrix(NA , length(vocabulary), K)#
		for (w in 1:length(vocabulary)) {#
			word_type_w = which(unlist(Backward_sample$text[-(1:length(base.text))]) == w)#
			word_type_topic_counts[w, ] = tabulate(unlist(Inference_samp$Z)[word_type_w], K)#
		}#
		Backward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
								   	   base.edge = base.edge, base.text = base.text, seed, topic_token_assignments = topic_token_assignments, #
								   	   topic_token_counts = topic_token_counts, word_type_topic_counts = word_type_topic_counts, #
								   	   forward = FALSE, backward = TRUE)#
		Backward_stats[i, ] = GiR_stats(Backward_sample, K, currentC, vocabulary, forward = FALSE, backward = TRUE)#
		bmat2[i, ] = unlist(b)#
		deltamat2[i] = delta#
		entropy2[i] = entropy.empirical(currentC)#
	}#
	tstats = rep(0, ncol(Forward_stats))#
	wstats = rep(0, ncol(Forward_stats))#
	for (j in 1:ncol(Forward_stats)) {#
		thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 400)#
		Forward_test = Forward_stats[thin, j]#
		Backward_test = Backward_stats[thin, j]#
		tstats[j] = t.test(Backward_test, Forward_test)$p.value#
		wstats[j] = wilcox.test(Backward_test, Forward_test)$p.value#
	}#
	names(tstats) = names(wstats) = colnames(Forward_stats)						#
	if (generate_PP_plots) {#
		par(mfrow=c(5,5), oma = c(3,3,3,3), mar = c(2,1,1,1))#
		GiR_PP_Plots(Forward_stats, Backward_stats)#
	}			#
	return(list(Forward = Forward_stats, Backward = Backward_stats, tstats = tstats, wstats = wstats, delta1 = deltamat1, delta2 = deltamat2))	#
}#
#
unix.time(TryGiR <- GiR(5*10^3, seed = 100))
# Getting_It_Right#
GiR = function(Nsamp = 5000, nDocs = 5, node = 1:4, vocabulary =  c("hi", "hello","bye", "mine", "what"), #
			   nIP = 2, K = 4, nwords = 4, alpha = 2, mvec = rep(1/4, 4), betas = 2, nvec = rep(1/5, 5), #
			   prior.b.mean = c(-3, rep(0, 6)), prior.b.var = diag(7), prior.eta = c(0, 1), sigma_Q = 0.25, #
			   niters = c(1, 1, 1, 50, 0, 1), netstat = "dyadic", generate_PP_plots = TRUE, seed = 1) {#
	set.seed(seed)#
	P = 1 + 3 * (2 * ("dyadic" %in% netstat) + 4 * ("triadic" %in% netstat) + 2 *("degree" %in% netstat))#
	b = lapply(1L:nIP, function(IP) {#
			   c(rmvnorm(1, prior.b.mean, prior.b.var))#
			   })#
	delta = pnorm(rnorm(1, prior.eta[1], prior.eta[2]))#
	currentC = sample(1L:nIP, K, replace = TRUE)	 #
	base.data = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, seed, #
							 base.edge = list(), base.text = list(), base = TRUE)#
	base.edge = base.data$edge	   #
	base.text = base.data$text#
#
	#Forward sampling#
	Forward_stats = matrix(NA, nrow = Nsamp, ncol = 21)#
	colnames(Forward_stats) = c(paste0("B_",1:length(b[[1]])), "Mean_receipients", "Mean_timediff", "Mean_TopicIP", #
						  paste0("Tokens_in_IP_", 1:nIP), paste0("Tokens_in_Topic", 1:K), #
						  paste0("Tokens_in_Word", 1:length(vocabulary)))#
	deltamat1 = rep(NA, Nsamp)#
	bmat1 = matrix(NA, nrow = Nsamp, ncol = nIP * P)#
	entropy1 = rep(NA, Nsamp)#
	for (i in 1:Nsamp) { #
		if (i %% 5000 == 0) {cat("Forward sampling", i, "\n")}#
		set.seed(i)#
		b = lapply(1:nIP, function(IP) {#
			c(rmvnorm(1, prior.b.mean, prior.b.var))#
			})#
		delta = pnorm(rnorm(1, prior.eta[1], prior.eta[2]))#
		currentC = sample(1L:nIP, K, replace = TRUE)#
		Forward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
									  base.edge = base.edge, base.text = base.text, seed, forward = TRUE) #
		Forward_stats[i, ] = GiR_stats(Forward_sample, K, currentC, vocabulary, forward = TRUE, backward = FALSE)#
		bmat1[i, ] = unlist(b)#
		deltamat1[i] = delta#
		entropy1[i] = entropy.empirical(currentC)#
	}#
	#Backward sampling#
	Backward_stats = matrix(NA, nrow = Nsamp, ncol = 21)#
	Backward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
								   base.edge = base.edge, base.text = base.text, seed, backward_init = TRUE) #
	deltamat2 = rep(NA, Nsamp)#
	bmat2 = matrix(NA, nrow = Nsamp, ncol = nIP * P)		#
	entropy2 = rep(NA, Nsamp)		   #
	for (i in 1:Nsamp) { #
		if (i %% 500 == 0) {cat("Backward sampling", i, "\n")}#
		seed = seed + 100#
		set.seed(seed)#
		Inference_samp = Inference(Backward_sample$edge, node, Backward_sample$text, vocabulary, nIP, K, sigma_Q, #
						  alpha, mvec, betas, nvec, prior.b.mean, prior.b.var, prior.eta,#
						  out = niters[1], n1 = niters[2], n2 = niters[3], n3 = niters[4], burn = niters[5], thin = niters[6], #
						  netstat, seed)#
		b = lapply(1:nIP, function(IP) {#
			rowMeans(Inference_samp$B[[IP]])#
		})#
		delta = mean(Inference_samp$D)#
		currentC = Inference_samp$C#
		topic_token_assignments = Inference_samp$Z#
		for (d in 1:length(topic_token_assignments)) {#
			names(topic_token_assignments[[d]]) = Backward_sample$text[[d + length(base.text)]]#
		}#
		topic_token_counts = tabulate(unlist(Inference_samp$Z), K)#
		word_type_topic_counts = matrix(NA , length(vocabulary), K)#
		for (w in 1:length(vocabulary)) {#
			word_type_w = which(unlist(Backward_sample$text[-(1:length(base.text))]) == w)#
			word_type_topic_counts[w, ] = tabulate(unlist(Inference_samp$Z)[word_type_w], K)#
		}#
		Backward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
								   	   base.edge = base.edge, base.text = base.text, seed, topic_token_assignments = topic_token_assignments, #
								   	   topic_token_counts = topic_token_counts, word_type_topic_counts = word_type_topic_counts, #
								   	   forward = FALSE, backward = TRUE)#
		Backward_stats[i, ] = GiR_stats(Backward_sample, K, currentC, vocabulary, forward = FALSE, backward = TRUE)#
		bmat2[i, ] = unlist(b)#
		deltamat2[i] = delta#
		entropy2[i] = entropy.empirical(currentC)#
	}#
	tstats = rep(0, ncol(Forward_stats))#
	wstats = rep(0, ncol(Forward_stats))#
	for (j in 1:ncol(Forward_stats)) {#
		thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 400)#
		Forward_test = Forward_stats[thin, j]#
		Backward_test = Backward_stats[thin, j]#
		tstats[j] = t.test(Backward_test, Forward_test)$p.value#
		wstats[j] = wilcox.test(Backward_test, Forward_test)$p.value#
	}#
	names(tstats) = names(wstats) = colnames(Forward_stats)						#
	if (generate_PP_plots) {#
		par(mfrow=c(5,5), oma = c(3,3,3,3), mar = c(2,1,1,1))#
		GiR_PP_Plots(Forward_stats, Backward_stats)#
	}			#
	return(list(Forward = Forward_stats, Backward = Backward_stats, tstats = tstats, wstats = wstats, delta = cbind(deltamat1, deltamat2),#
				b1 = bmat1, b2= bmat2, entropy = cbind(entropy1, entropy2))	#
}
# Getting_It_Right#
GiR = function(Nsamp = 5000, nDocs = 5, node = 1:4, vocabulary =  c("hi", "hello","bye", "mine", "what"), #
			   nIP = 2, K = 4, nwords = 4, alpha = 2, mvec = rep(1/4, 4), betas = 2, nvec = rep(1/5, 5), #
			   prior.b.mean = c(-3, rep(0, 6)), prior.b.var = diag(7), prior.eta = c(0, 1), sigma_Q = 0.25, #
			   niters = c(1, 1, 1, 50, 0, 1), netstat = "dyadic", generate_PP_plots = TRUE, seed = 1) {#
	set.seed(seed)#
	P = 1 + 3 * (2 * ("dyadic" %in% netstat) + 4 * ("triadic" %in% netstat) + 2 *("degree" %in% netstat))#
	b = lapply(1L:nIP, function(IP) {#
			   c(rmvnorm(1, prior.b.mean, prior.b.var))#
			   })#
	delta = pnorm(rnorm(1, prior.eta[1], prior.eta[2]))#
	currentC = sample(1L:nIP, K, replace = TRUE)	 #
	base.data = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, seed, #
							 base.edge = list(), base.text = list(), base = TRUE)#
	base.edge = base.data$edge	   #
	base.text = base.data$text#
#
	#Forward sampling#
	Forward_stats = matrix(NA, nrow = Nsamp, ncol = 21)#
	colnames(Forward_stats) = c(paste0("B_",1:length(b[[1]])), "Mean_receipients", "Mean_timediff", "Mean_TopicIP", #
						  paste0("Tokens_in_IP_", 1:nIP), paste0("Tokens_in_Topic", 1:K), #
						  paste0("Tokens_in_Word", 1:length(vocabulary)))#
	deltamat1 = rep(NA, Nsamp)#
	bmat1 = matrix(NA, nrow = Nsamp, ncol = nIP * P)#
	entropy1 = rep(NA, Nsamp)#
	for (i in 1:Nsamp) { #
		if (i %% 5000 == 0) {cat("Forward sampling", i, "\n")}#
		set.seed(i)#
		b = lapply(1:nIP, function(IP) {#
			c(rmvnorm(1, prior.b.mean, prior.b.var))#
			})#
		delta = pnorm(rnorm(1, prior.eta[1], prior.eta[2]))#
		currentC = sample(1L:nIP, K, replace = TRUE)#
		Forward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
									  base.edge = base.edge, base.text = base.text, seed, forward = TRUE) #
		Forward_stats[i, ] = GiR_stats(Forward_sample, K, currentC, vocabulary, forward = TRUE, backward = FALSE)#
		bmat1[i, ] = unlist(b)#
		deltamat1[i] = delta#
		entropy1[i] = entropy.empirical(currentC)#
	}#
	#Backward sampling#
	Backward_stats = matrix(NA, nrow = Nsamp, ncol = 21)#
	Backward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
								   base.edge = base.edge, base.text = base.text, seed, backward_init = TRUE) #
	deltamat2 = rep(NA, Nsamp)#
	bmat2 = matrix(NA, nrow = Nsamp, ncol = nIP * P)		#
	entropy2 = rep(NA, Nsamp)		   #
	for (i in 1:Nsamp) { #
		if (i %% 500 == 0) {cat("Backward sampling", i, "\n")}#
		seed = seed + 100#
		set.seed(seed)#
		Inference_samp = Inference(Backward_sample$edge, node, Backward_sample$text, vocabulary, nIP, K, sigma_Q, #
						  alpha, mvec, betas, nvec, prior.b.mean, prior.b.var, prior.eta,#
						  out = niters[1], n1 = niters[2], n2 = niters[3], n3 = niters[4], burn = niters[5], thin = niters[6], #
						  netstat, seed)#
		b = lapply(1:nIP, function(IP) {#
			rowMeans(Inference_samp$B[[IP]])#
		})#
		delta = mean(Inference_samp$D)#
		currentC = Inference_samp$C#
		topic_token_assignments = Inference_samp$Z#
		for (d in 1:length(topic_token_assignments)) {#
			names(topic_token_assignments[[d]]) = Backward_sample$text[[d + length(base.text)]]#
		}#
		topic_token_counts = tabulate(unlist(Inference_samp$Z), K)#
		word_type_topic_counts = matrix(NA , length(vocabulary), K)#
		for (w in 1:length(vocabulary)) {#
			word_type_w = which(unlist(Backward_sample$text[-(1:length(base.text))]) == w)#
			word_type_topic_counts[w, ] = tabulate(unlist(Inference_samp$Z)[word_type_w], K)#
		}#
		Backward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
								   	   base.edge = base.edge, base.text = base.text, seed, topic_token_assignments = topic_token_assignments, #
								   	   topic_token_counts = topic_token_counts, word_type_topic_counts = word_type_topic_counts, #
								   	   forward = FALSE, backward = TRUE)#
		Backward_stats[i, ] = GiR_stats(Backward_sample, K, currentC, vocabulary, forward = FALSE, backward = TRUE)#
		bmat2[i, ] = unlist(b)#
		deltamat2[i] = delta#
		entropy2[i] = entropy.empirical(currentC)#
	}#
	tstats = rep(0, ncol(Forward_stats))#
	wstats = rep(0, ncol(Forward_stats))#
	for (j in 1:ncol(Forward_stats)) {#
		thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 400)#
		Forward_test = Forward_stats[thin, j]#
		Backward_test = Backward_stats[thin, j]#
		tstats[j] = t.test(Backward_test, Forward_test)$p.value#
		wstats[j] = wilcox.test(Backward_test, Forward_test)$p.value#
	}#
	names(tstats) = names(wstats) = colnames(Forward_stats)						#
	if (generate_PP_plots) {#
		par(mfrow=c(5,5), oma = c(3,3,3,3), mar = c(2,1,1,1))#
		GiR_PP_Plots(Forward_stats, Backward_stats)#
	}			#
	return(list(Forward = Forward_stats, Backward = Backward_stats, tstats = tstats, wstats = wstats, delta = cbind(deltamat1, deltamat2),#
				b1 = bmat1, b2= bmat2, entropy = cbind(entropy1, entropy2)))	#
}#
#
unix.time(TryGiR <- GiR(5*10^3, seed = 100))
par(mfrow=c(4, 4))#
matplot(TryGiR$delta,type = 'l', col = 1:2)#
matplot(TryGiR$entropy,type = 'l', col = 1:2)#
for (p in 1:14) {#
	matplot(cbind(TryGiR$b1[,p], TryGiR$b2[,p]),type = 'l', col = 1:2)#
}
Nsamp = nrow(TryGiR$Forward)#
thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 400)#
par(mfrow=c(4, 4))#
matplot(TryGiR$delta[thin, ],type = 'l', col = 1:2)#
matplot(TryGiR$entropy[thin, ],type = 'l', col = 1:2)#
for (p in 1:14) {#
	matplot(cbind(TryGiR$b1[thin ,p], TryGiR$b2[thin,p]),type = 'l', col = 1:2)#
}
Nsamp = nrow(TryGiR$Forward)#
thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 500)#
par(mfrow=c(4, 4))#
matplot(TryGiR$delta[thin, ],type = 'l', col = 1:2)#
matplot(TryGiR$entropy[thin, ],type = 'l', col = 1:2)#
for (p in 1:14) {#
	matplot(cbind(TryGiR$b1[thin ,p], TryGiR$b2[thin,p]),type = 'l', col = 1:2)#
}
unix.time(TryGiR <- GiR(10^4, seed = 20))#
#
Nsamp = nrow(TryGiR$Forward)#
thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 500)#
par(mfrow=c(4, 4))#
matplot(TryGiR$delta[thin, ],type = 'l', col = 1:2)#
matplot(TryGiR$entropy[thin, ],type = 'l', col = 1:2)#
for (p in 1:14) {#
	matplot(cbind(TryGiR$b1[thin ,p], TryGiR$b2[thin,p]),type = 'l', col = 1:2)#
}
unix.time(TryGiR <- GiR(10^4, seed = 20))
Nsamp = nrow(TryGiR$Forward)#
thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 500)#
par(mfrow=c(4, 4))#
matplot(TryGiR$delta[thin, ],type = 'l', col = 1:2)#
matplot(TryGiR$entropy[thin, ],type = 'l', col = 1:2)#
for (p in 1:14) {#
	matplot(cbind(TryGiR$b1[thin ,p], TryGiR$b2[thin,p]),type = 'l', col = 1:2)#
}
Nsamp = nrow(TryGiR$Forward)#
thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 500)#
par(mfrow=c(4, 4))#
matplot(TryGiR$delta[thin, ],type = 'l', col = 1:2, lty = 1)#
matplot(TryGiR$entropy[thin, ],type = 'l', col = 1:2, lty =1)#
for (p in 1:14) {#
	matplot(cbind(TryGiR$b1[thin ,p], TryGiR$b2[thin,p]),type = 'l', col = 1:2, lty = 1)#
}
# Getting_It_Right#
GiR = function(Nsamp = 5000, nDocs = 5, node = 1:4, vocabulary =  c("hi", "hello","bye", "mine", "what"), #
			   nIP = 2, K = 4, nwords = 4, alpha = 2, mvec = rep(1/4, 4), betas = 2, nvec = rep(1/5, 5), #
			   prior.b.mean = c(-3, rep(0, 6)), prior.b.var = diag(7), prior.eta = c(0, 1), sigma_Q = 0.25, #
			   niters = c(1, 1, 1, 50, 0, 1), netstat = "dyadic", generate_PP_plots = TRUE, seed = 1) {#
	set.seed(seed)#
	P = 1 + 3 * (2 * ("dyadic" %in% netstat) + 4 * ("triadic" %in% netstat) + 2 *("degree" %in% netstat))#
	b = lapply(1L:nIP, function(IP) {#
			   c(rmvnorm(1, prior.b.mean, prior.b.var))#
			   })#
	delta = pnorm(rnorm(1, prior.eta[1], prior.eta[2]))#
	currentC = sample(1L:nIP, K, replace = TRUE)	 #
	base.data = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, seed, #
							 base.edge = list(), base.text = list(), base = TRUE)#
	base.edge = base.data$edge	   #
	base.text = base.data$text#
#
	#Forward sampling#
	Forward_stats = matrix(NA, nrow = Nsamp, ncol = 21)#
	colnames(Forward_stats) = c(paste0("B_",1:length(b[[1]])), "Mean_receipients", "Mean_timediff", "Mean_TopicIP", #
						  paste0("Tokens_in_IP_", 1:nIP), paste0("Tokens_in_Topic", 1:K), #
						  paste0("Tokens_in_Word", 1:length(vocabulary)))#
	deltamat1 = rep(NA, Nsamp)#
	bmat1 = matrix(NA, nrow = Nsamp, ncol = nIP * P)#
	entropy1 = rep(NA, Nsamp)#
	for (i in 1:Nsamp) { #
		if (i %% 5000 == 0) {cat("Forward sampling", i, "\n")}#
		set.seed(i)#
		b = lapply(1:nIP, function(IP) {#
			c(rmvnorm(1, prior.b.mean, prior.b.var))#
			})#
		delta = pnorm(rnorm(1, prior.eta[1], prior.eta[2]))#
		currentC = sample(1L:nIP, K, replace = TRUE)#
		Forward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
									  base.edge = base.edge, base.text = base.text, seed, forward = TRUE) #
		Forward_stats[i, ] = GiR_stats(Forward_sample, K, currentC, vocabulary, forward = TRUE, backward = FALSE)#
		bmat1[i, ] = unlist(b)#
		deltamat1[i] = delta#
		entropy1[i] = entropy.empirical(currentC)#
	}#
	#Backward sampling#
	Backward_stats = matrix(NA, nrow = Nsamp, ncol = 21)#
	Backward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
								   base.edge = base.edge, base.text = base.text, seed, backward_init = TRUE) #
	deltamat2 = rep(NA, Nsamp)#
	bmat2 = matrix(NA, nrow = Nsamp, ncol = nIP * P)		#
	entropy2 = matrix(NA, Nsamp, 2)		   #
	for (i in 1:Nsamp) { #
		if (i %% 500 == 0) {cat("Backward sampling", i, "\n")}#
		seed = seed + 100#
		set.seed(seed)#
		Inference_samp = Inference(Backward_sample$edge, node, Backward_sample$text, vocabulary, nIP, K, sigma_Q, #
						  alpha, mvec, betas, nvec, prior.b.mean, prior.b.var, prior.eta,#
						  out = niters[1], n1 = niters[2], n2 = niters[3], n3 = niters[4], burn = niters[5], thin = niters[6], #
						  netstat, seed)#
		b = lapply(1:nIP, function(IP) {#
			rowMeans(Inference_samp$B[[IP]])#
		})#
		delta = mean(Inference_samp$D)#
		currentC = Inference_samp$C#
		topic_token_assignments = Inference_samp$Z#
		for (d in 1:length(topic_token_assignments)) {#
			names(topic_token_assignments[[d]]) = Backward_sample$text[[d + length(base.text)]]#
		}#
		topic_token_counts = tabulate(unlist(Inference_samp$Z), K)#
		word_type_topic_counts = matrix(NA , length(vocabulary), K)#
		for (w in 1:length(vocabulary)) {#
			word_type_w = which(unlist(Backward_sample$text[-(1:length(base.text))]) == w)#
			word_type_topic_counts[w, ] = tabulate(unlist(Inference_samp$Z)[word_type_w], K)#
		}#
		Backward_sample = GenerateDocs(nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, #
								   	   base.edge = base.edge, base.text = base.text, seed, topic_token_assignments = topic_token_assignments, #
								   	   topic_token_counts = topic_token_counts, word_type_topic_counts = word_type_topic_counts, #
								   	   forward = FALSE, backward = TRUE)#
		Backward_stats[i, ] = GiR_stats(Backward_sample, K, currentC, vocabulary, forward = FALSE, backward = TRUE)#
		bmat2[i, ] = unlist(b)#
		deltamat2[i] = delta#
		entropy2[i, ] = c(entropy.empirical(currentC), entropy.empirical(topic_token_counts))#
	}#
	tstats = rep(0, ncol(Forward_stats))#
	wstats = rep(0, ncol(Forward_stats))#
	for (j in 1:ncol(Forward_stats)) {#
		thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 400)#
		Forward_test = Forward_stats[thin, j]#
		Backward_test = Backward_stats[thin, j]#
		tstats[j] = t.test(Backward_test, Forward_test)$p.value#
		wstats[j] = wilcox.test(Backward_test, Forward_test)$p.value#
	}#
	names(tstats) = names(wstats) = colnames(Forward_stats)						#
	if (generate_PP_plots) {#
		par(mfrow=c(5,5), oma = c(3,3,3,3), mar = c(2,1,1,1))#
		GiR_PP_Plots(Forward_stats, Backward_stats)#
	}			#
	return(list(Forward = Forward_stats, Backward = Backward_stats, tstats = tstats, wstats = wstats, delta = cbind(deltamat1, deltamat2),#
				b1 = bmat1, b2= bmat2, entropy1 = entropy1, entropy2 = entropy2))	#
}#
#
unix.time(TryGiR <- GiR(10^4, seed = 20))
Nsamp = nrow(TryGiR$Forward)#
thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 500)#
par(mfrow=c(4, 4))#
matplot(TryGiR$delta[thin, ],type = 'l', col = 1:2, lty = 1)#
matplot(TryGiR$entropy[thin, ],type = 'l', col = 1:2, lty =1)#
for (p in 1:14) {#
	matplot(cbind(TryGiR$b1[thin ,p], TryGiR$b2[thin,p]),type = 'l', col = 1:2, lty = 1)#
}
#clear memory#
rm( list=ls() )#
#load necessary libraries 						                                 #
library(foreign)#
library(Zelig)#
library(car)#
library(MASS)#
library(VGAM)#
library(plotrix)#
library(pscl)#
library(survival)#
library(msm)#
library(verification)#
library(corpcor)#
library(Design)
install.packages("Zelig")
library(foreign)
library(Zelig)
library(car)
install.packages("car")
install.packages("Zelig")
library(car)
library(MASS)
library(VGAM)
library(plotrix)
install.packages("plotrix")
library(plotrix)
library(pscl)
library(survival)
library(msm)
install.packages("msm")
library(verification)
install.packages("verificatino")
install.packages("verification")
library(verification)
library(corpcor)
library(Design)
install.packages("Design")
library(foreign)#
library(Zelig)#
library(car)#
library(MASS)#
library(VGAM)#
library(plotrix)#
library(pscl)#
library(survival)#
library(msm)#
library(verification)#
library(corpcor)#
library(Design)
hist(rgamma(10^6, 2, 0.001))
summary(rgamma(10^6, 2, 0.001))
library(devtools)
install_github('https://github.com/bomin8319/DLFM/')
install_github('https://github.com/bomin8319/DLFM/pkg')
install_github('https://github.com/bomin8319/DLFM/pkg/')
install_github('bomin8319/DLFM', subdir= "pkg")
library(DLFM)
rm(list=ls())
library(DLFM)
DLFM
DLFM_NA
library(maps)
library(fields)
Exponential(3, 1)
Exponential(3^2, 1^2)
install.packages("BayesMixSurv")
library(BayesMixSurv)
bayesmixsurv
bayesmixsurv.mcmc
library(IPTM)#
TryGiR<- GiR(10^3, nDocs = 10, niters = c(1, 1, 1, 50, 0, 1), sigma_Q = c(0.25, 2), seed = 1)
colMeans(TryGiR$accept.rate)
Nsamp = nrow(TryGiR$Forward)#
thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 500)#
par(mfrow=c(1,5))#
matplot(TryGiR$delta[thin, ],type = 'l', col = 1:2, lty = 1, xlab = "iter", ylab = "delta")#
matplot(cbind(TryGiR$entropyC1[thin,1],TryGiR$entropyC2[thin,1]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-IP)")#
matplot(cbind(TryGiR$entropyC1[thin,2],TryGiR$entropyC2[thin,2]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-token overall)")#
matplot(cbind(TryGiR$entropyZ1[thin],TryGiR$entropyZ2[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(word-token)")#
matplot(cbind(TryGiR$Zstat1[thin],TryGiR$Zstat2[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "mean(entropy(topic-token[[d]]))")
document()
library(devtools)
document()
TryGiR<- GiR(10^3, nDocs = 10, niters = c(1, 1, 1, 50, 0, 1), prior.b.mean = c(-2, rep(0, 6)), sigma_Q = c(0.25, 2), seed = 1)
Nsamp = nrow(TryGiR$Forward)#
thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 500)#
par(mfrow=c(1,5))#
matplot(TryGiR$delta[thin, ],type = 'l', col = 1:2, lty = 1, xlab = "iter", ylab = "delta")#
matplot(cbind(TryGiR$entropyC1[thin,1],TryGiR$entropyC2[thin,1]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-IP)")#
matplot(cbind(TryGiR$entropyC1[thin,2],TryGiR$entropyC2[thin,2]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-token overall)")#
matplot(cbind(TryGiR$entropyZ1[thin],TryGiR$entropyZ2[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(word-token)")#
matplot(cbind(TryGiR$Zstat1[thin],TryGiR$Zstat2[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "mean(entropy(topic-token[[d]]))")
colMeans(TryGiR$accept.rate)
TryGiR<- GiR(10^3, nDocs = 10, niters = c(1, 1, 1, 50, 0, 1), prior.b.mean = c(-1, rep(0, 6)), sigma_Q = c(0.25, 2), seed = 1)
colMeans(TryGiR$accept.rate)
Nsamp = nrow(TryGiR$Forward)#
thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 500)#
par(mfrow=c(1,5))#
matplot(TryGiR$delta[thin, ],type = 'l', col = 1:2, lty = 1, xlab = "iter", ylab = "delta")#
matplot(cbind(TryGiR$entropyC1[thin,1],TryGiR$entropyC2[thin,1]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-IP)")#
matplot(cbind(TryGiR$entropyC1[thin,2],TryGiR$entropyC2[thin,2]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-token overall)")#
matplot(cbind(TryGiR$entropyZ1[thin],TryGiR$entropyZ2[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(word-token)")#
matplot(cbind(TryGiR$Zstat1[thin],TryGiR$Zstat2[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "mean(entropy(topic-token[[d]]))")
par(mfrow=c(2,5))#
matplot(TryGiR$delta[thin, ],type = 'l', col = 1:2, lty = 1, xlab = "iter", ylab = "delta")#
matplot(cbind(TryGiR$entropyC1[thin,1],TryGiR$entropyC2[thin,1]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-IP)")#
matplot(cbind(TryGiR$entropyC1[thin,2],TryGiR$entropyC2[thin,2]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-token overall)")#
matplot(cbind(TryGiR$entropyZ1[thin],TryGiR$entropyZ2[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(word-token)")#
matplot(cbind(TryGiR$Zstat1[thin],TryGiR$Zstat2[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "mean(entropy(topic-token[[d]]))")
matplot(TryGiR$delta[thin, ],type = 'l', col = 2:1, lty = 1, xlab = "iter", ylab = "delta")#
matplot(cbind(TryGiR$entropyC2[thin,1],TryGiR$entropyC1[thin,1]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-IP)")#
matplot(cbind(TryGiR$entropyC2[thin,2],TryGiR$entropyC1[thin,2]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-token overall)")#
matplot(cbind(TryGiR$entropyZ2[thin],TryGiR$entropyZ1[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(word-token)")#
matplot(cbind(TryGiR$Zstat2[thin],TryGiR$Zstat1[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "mean(entropy(topic-token[[d]]))")
TryGiR<- GiR(10^3, nDocs = 10, niters = c(1, 1, 1, 50, 0, 1), prior.b.mean = c(-0.5, rep(0, 6)), sigma_Q = c(0.25, 2), seed = 1)
Nsamp = nrow(TryGiR$Forward)#
thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 500)#
par(mfrow=c(2,5))#
matplot(TryGiR$delta[thin, ],type = 'l', col = 1:2, lty = 1, xlab = "iter", ylab = "delta")#
matplot(cbind(TryGiR$entropyC1[thin,1],TryGiR$entropyC2[thin,1]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-IP)")#
matplot(cbind(TryGiR$entropyC1[thin,2],TryGiR$entropyC2[thin,2]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-token overall)")#
matplot(cbind(TryGiR$entropyZ1[thin],TryGiR$entropyZ2[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(word-token)")#
matplot(cbind(TryGiR$Zstat1[thin],TryGiR$Zstat2[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "mean(entropy(topic-token[[d]]))")#
matplot(TryGiR$delta[thin, ],type = 'l', col = 2:1, lty = 1, xlab = "iter", ylab = "delta")#
matplot(cbind(TryGiR$entropyC2[thin,1],TryGiR$entropyC1[thin,1]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-IP)")#
matplot(cbind(TryGiR$entropyC2[thin,2],TryGiR$entropyC1[thin,2]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-token overall)")#
matplot(cbind(TryGiR$entropyZ2[thin],TryGiR$entropyZ1[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(word-token)")#
matplot(cbind(TryGiR$Zstat2[thin],TryGiR$Zstat1[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "mean(entropy(topic-token[[d]]))")
TryGiR<- GiR(10^3, nDocs = 10, niters = c(1, 1, 1, 50, 0, 1), prior.b.mean = c(-5, rep(0, 6)), sigma_Q = c(0.25, 2), seed = 1)
TryGiR<- GiR(10^3, nDocs = 10, niters = c(1, 1, 1, 50, 0, 1), prior.b.mean = c(-1.5, rep(0, 6)), sigma_Q = c(0.25, 2), seed = 1)
Nsamp = nrow(TryGiR$Forward)#
thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 500)#
par(mfrow=c(2,5))#
matplot(TryGiR$delta[thin, ],type = 'l', col = 1:2, lty = 1, xlab = "iter", ylab = "delta")#
matplot(cbind(TryGiR$entropyC1[thin,1],TryGiR$entropyC2[thin,1]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-IP)")#
matplot(cbind(TryGiR$entropyC1[thin,2],TryGiR$entropyC2[thin,2]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-token overall)")#
matplot(cbind(TryGiR$entropyZ1[thin],TryGiR$entropyZ2[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(word-token)")#
matplot(cbind(TryGiR$Zstat1[thin],TryGiR$Zstat2[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "mean(entropy(topic-token[[d]]))")#
matplot(TryGiR$delta[thin, ],type = 'l', col = 2:1, lty = 1, xlab = "iter", ylab = "delta")#
matplot(cbind(TryGiR$entropyC2[thin,1],TryGiR$entropyC1[thin,1]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-IP)")#
matplot(cbind(TryGiR$entropyC2[thin,2],TryGiR$entropyC1[thin,2]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-token overall)")#
matplot(cbind(TryGiR$entropyZ2[thin],TryGiR$entropyZ1[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(word-token)")#
matplot(cbind(TryGiR$Zstat2[thin],TryGiR$Zstat1[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "mean(entropy(topic-token[[d]]))")
TryGiR<- GiR(10^3, nDocs = 20, niters = c(1, 1, 1, 50, 0, 1), prior.b.mean = c(-2, rep(0, 6)), sigma_Q = c(0.25, 2), seed = 1)
Nsamp = nrow(TryGiR$Forward)#
thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 500)#
par(mfrow=c(2,5))#
matplot(TryGiR$delta[thin, ],type = 'l', col = 1:2, lty = 1, xlab = "iter", ylab = "delta")#
matplot(cbind(TryGiR$entropyC1[thin,1],TryGiR$entropyC2[thin,1]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-IP)")#
matplot(cbind(TryGiR$entropyC1[thin,2],TryGiR$entropyC2[thin,2]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-token overall)")#
matplot(cbind(TryGiR$entropyZ1[thin],TryGiR$entropyZ2[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(word-token)")#
matplot(cbind(TryGiR$Zstat1[thin],TryGiR$Zstat2[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "mean(entropy(topic-token[[d]]))")#
matplot(TryGiR$delta[thin, ],type = 'l', col = 2:1, lty = 1, xlab = "iter", ylab = "delta")#
matplot(cbind(TryGiR$entropyC2[thin,1],TryGiR$entropyC1[thin,1]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-IP)")#
matplot(cbind(TryGiR$entropyC2[thin,2],TryGiR$entropyC1[thin,2]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-token overall)")#
matplot(cbind(TryGiR$entropyZ2[thin],TryGiR$entropyZ1[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(word-token)")#
matplot(cbind(TryGiR$Zstat2[thin],TryGiR$Zstat1[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "mean(entropy(topic-token[[d]]))")
colMeans(TryGiR$accept.rate)
PPPplot(GiR$Forward, GiR$Backward)
pppplot(GiR$Forward, GiR$Backward)
names(TryGiR)
pppplot(TryGiR$Forward, TryGiR$Backward)
par(mfrow=c(3,7))
pppplot(TryGiR$Forward, TryGiR$Backward)
head(TryGiR$delta)
TryGiR<- GiR(10^3, nDocs = 30, niters = c(1, 1, 1, 50, 0, 1), prior.b.mean = c(-2, rep(0, 6)), sigma_Q = c(0.25, 2), seed = 1)
Nsamp = nrow(TryGiR$Forward)#
thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 500)#
par(mfrow=c(2,5))#
matplot(TryGiR$delta[thin, ],type = 'l', col = 1:2, lty = 1, xlab = "iter", ylab = "delta")#
matplot(cbind(TryGiR$entropyC1[thin,1],TryGiR$entropyC2[thin,1]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-IP)")#
matplot(cbind(TryGiR$entropyC1[thin,2],TryGiR$entropyC2[thin,2]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-token overall)")#
matplot(cbind(TryGiR$entropyZ1[thin],TryGiR$entropyZ2[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(word-token)")#
matplot(cbind(TryGiR$Zstat1[thin],TryGiR$Zstat2[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "mean(entropy(topic-token[[d]]))")
matplot(TryGiR$delta[thin, ],type = 'l', col = 2:1, lty = 1, xlab = "iter", ylab = "delta")#
matplot(cbind(TryGiR$entropyC2[thin,1],TryGiR$entropyC1[thin,1]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-IP)")#
matplot(cbind(TryGiR$entropyC2[thin,2],TryGiR$entropyC1[thin,2]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-token overall)")#
matplot(cbind(TryGiR$entropyZ2[thin],TryGiR$entropyZ1[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(word-token)")#
matplot(cbind(TryGiR$Zstat2[thin],TryGiR$Zstat1[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "mean(entropy(topic-token[[d]]))")
Nsamp = nrow(TryGiR$Forward)#
thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 500)#
par(mfrow = c(3, 3))#
for (p in 1:9){#
matplot(cbind(TryGiR$Forward[thin,p], TryGiR$Backward[thin,p]), type = 'l', col = 1:2, lty = 1, main = colnames(TryGiR$Forward)[p], xlab = 'iter', ylab ='')#
}
TryGiR<- GiR(10^3, nDocs = 20, node = 1:5, niters = c(1, 1, 1, 50, 0, 1), prior.b.mean = c(-2, rep(0, 6)), sigma_Q = c(0.25, 2), seed = 1)
TryGiR<- GiR(10^3, nDocs = 20, node = 1:5, prior.delta = c(4, 8), niters = c(1, 1, 1, 50, 0, 1), prior.b.mean = c(-2, rep(0, 6)), sigma_Q = c(0.25, 2), seed = 1)
library(IPTM)#
TryGiR<- GiR(10^3, nDocs = 20, node = 1:4, prior.delta = c(4, 8), niters = c(1, 1, 1, 50, 0, 1), prior.b.mean = c(-2, rep(0, 6)), sigma_Q = c(0.25, 2), seed = 1)#
T
library(IPTM)#
TryGiR<- GiR(10^3, nDocs = 20, node = 1:4, prior.delta = c(4, 8), niters = c(1, 1, 1, 50, 0, 1), prior.b.mean = c(-2, rep(0, 6)), sigma_Q = c(0.25, 2), seed = 1)
Nsamp = nrow(TryGiR$Forward)#
thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 500)#
par(mfrow=c(2,5))#
matplot(TryGiR$delta[thin, ],type = 'l', col = 1:2, lty = 1, xlab = "iter", ylab = "delta")#
matplot(cbind(TryGiR$entropyC1[thin,1],TryGiR$entropyC2[thin,1]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-IP)")#
matplot(cbind(TryGiR$entropyC1[thin,2],TryGiR$entropyC2[thin,2]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-token overall)")#
matplot(cbind(TryGiR$entropyZ1[thin],TryGiR$entropyZ2[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(word-token)")#
matplot(cbind(TryGiR$Zstat1[thin],TryGiR$Zstat2[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "mean(entropy(topic-token[[d]]))")
Nsamp = nrow(TryGiR$Forward)#
thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 500)#
par(mfrow = c(3, 3))#
for (p in 1:9){#
matplot(cbind(TryGiR$Forward[thin,p], TryGiR$Backward[thin,p]), type = 'l', col = 1:2, lty = 1, main = colnames(TryGiR$Forward)[p], xlab = 'iter', ylab ='')#
}
colMeans(TryGiR$accept.rate)
head(TryGiR$delta)
cor(TryGiR$delta)
par(mfrow=c(2,5))#
matplot(TryGiR$delta[thin, ],type = 'l', col = 1:2, lty = 1, xlab = "iter", ylab = "delta")#
matplot(cbind(TryGiR$entropyC1[thin,1],TryGiR$entropyC2[thin,1]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-IP)")#
matplot(cbind(TryGiR$entropyC1[thin,2],TryGiR$entropyC2[thin,2]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-token overall)")#
matplot(cbind(TryGiR$entropyZ1[thin],TryGiR$entropyZ2[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(word-token)")#
matplot(cbind(TryGiR$Zstat1[thin],TryGiR$Zstat2[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "mean(entropy(topic-token[[d]]))")#
matplot(TryGiR$delta[thin, ],type = 'l', col = 2:1, lty = 1, xlab = "iter", ylab = "delta")#
matplot(cbind(TryGiR$entropyC2[thin,1],TryGiR$entropyC1[thin,1]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-IP)")#
matplot(cbind(TryGiR$entropyC2[thin,2],TryGiR$entropyC1[thin,2]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-token overall)")#
matplot(cbind(TryGiR$entropyZ2[thin],TryGiR$entropyZ1[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(word-token)")#
matplot(cbind(TryGiR$Zstat2[thin],TryGiR$Zstat1[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "mean(entropy(topic-token[[d]]))")
library(IPTM)#
TryGiR<- GiR(5*10^3, nDocs = 20, node = 1:4, prior.delta = c(4, 8), niters = c(1, 1, 1, 50, 0, 1), prior.b.mean = c(-2, rep(0, 6)), sigma_Q = c(0.25, 2), seed = 1)
Nsamp = nrow(TryGiR$Forward)#
thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 500)#
par(mfrow=c(2,5))#
matplot(TryGiR$delta[thin, ],type = 'l', col = 1:2, lty = 1, xlab = "iter", ylab = "delta")#
matplot(cbind(TryGiR$entropyC1[thin,1],TryGiR$entropyC2[thin,1]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-IP)")#
matplot(cbind(TryGiR$entropyC1[thin,2],TryGiR$entropyC2[thin,2]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-token overall)")#
matplot(cbind(TryGiR$entropyZ1[thin],TryGiR$entropyZ2[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(word-token)")#
matplot(cbind(TryGiR$Zstat1[thin],TryGiR$Zstat2[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "mean(entropy(topic-token[[d]]))")#
matplot(TryGiR$delta[thin, ],type = 'l', col = 2:1, lty = 1, xlab = "iter", ylab = "delta")#
matplot(cbind(TryGiR$entropyC2[thin,1],TryGiR$entropyC1[thin,1]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-IP)")#
matplot(cbind(TryGiR$entropyC2[thin,2],TryGiR$entropyC1[thin,2]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-token overall)")#
matplot(cbind(TryGiR$entropyZ2[thin],TryGiR$entropyZ1[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(word-token)")#
matplot(cbind(TryGiR$Zstat2[thin],TryGiR$Zstat1[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "mean(entropy(topic-token[[d]]))")
colMeans(TryGiR$accept.rate)
(TryGiR$accept.rate)
for (p in 1:14) {#
	matplot(cbind(TryGiR$b1[thin ,p], TryGiR$b2[thin,p]),type = 'l', col = 1:2, lty = 1)#
}
Nsamp = nrow(TryGiR$Forward)#
thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 500)#
par(mfrow = c(3, 3))#
for (p in 1:9){#
matplot(cbind(TryGiR$Forward[thin,p], TryGiR$Backward[thin,p]), type = 'l', col = 1:2, lty = 1, main = colnames(TryGiR$Forward)[p], xlab = 'iter', ylab ='')#
}
Nsamp = nrow(TryGiR$Forward)#
thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 500)#
par(mfrow = c(3, 7))#
for (p in 1:21){#
matplot(cbind(TryGiR$Forward[thin,p], TryGiR$Backward[thin,p]), type = 'l', col = 1:2, lty = 1, main = colnames(TryGiR$Forward)[p], xlab = 'iter', ylab ='')#
}
library(IPTM)#
library(mvtnorm)#
library(MCMCpack)#
nDocs = 10#
node = 1:4#
vocabulary = c("hi", "hello", "fine", "bye", "what")#
nIP = 2#
K = 4#
nwords = 4#
alpha = 2#
mvec = rep(1/4, 4)#
betas = 2#
nvec = rep(1/5, 5)#
prior.b.mean = c(-2.5, rep(0, 6))#
prior.b.var =  diag(7)#
prior.delta = c(4, 8)#
sigma_Q = c(0.25, 2)#
niters = c(1, 50, 10, 0, 1)#
netstat = c("intercept", "dyadic")#
P = 1 * ("intercept" %in% netstat) + 3 * (2 * ("dyadic" %in% netstat) + 4 * ("triadic" %in% netstat) + 2 *("degree" %in% netstat))#
b = lapply(1L:nIP, function(IP) {#
    c(rmvnorm(1,  c(-2.5, rep(0, 6)),  diag(7)))#
  })#
delta = rgamma(1, 4, 8)#
currentC = sample(1L:nIP, K, replace = TRUE)	 #
base.data = GenerateDocs(30, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, base.edge = list(),  base.text = list(), base = TRUE) #
base.edge = base.data$edge	   #
base.text = base.data$text#
TryGiR<- GiR(5*10^3, nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, #
               prior.b.mean, prior.b.var, prior.delta, sigma_Q, niters, netstat, base.edge, base.text, seed = 12345)
Nsamp = nrow(TryGiR$Forward)#
thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 500)#
par(mfrow=c(1,5))#
matplot(TryGiR$delta[thin, ],type = 'l', col = 1:2, lty = 1, xlab = "iter", ylab = "delta")#
matplot(cbind(TryGiR$entropyC1[thin,1],TryGiR$entropyC2[thin,1]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-IP)")#
matplot(cbind(TryGiR$entropyC1[thin,2],TryGiR$entropyC2[thin,2]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(topic-token overall)")#
matplot(cbind(TryGiR$entropyZ1[thin],TryGiR$entropyZ2[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "entropy(word-token)")#
matplot(cbind(TryGiR$Zstat1[thin],TryGiR$Zstat2[thin]) ,type = 'l', col = 1:2, lty =1, xlab = "iter", ylab = "mean(entropy(topic-token[[d]]))")
Nsamp = nrow(TryGiR$Forward)#
thin = seq(from = floor(Nsamp / 5), to = Nsamp, length.out = 500)#
par(mfrow = c(3, 7))#
for (p in 1:21){#
matplot(cbind(TryGiR$Forward[thin,p], TryGiR$Backward[thin,p]), type = 'l', col = 1:2, lty = 1, main = colnames(TryGiR$Forward)[p], xlab = 'iter', ylab ='')#
}
rexp(1, 0)
load("/Users/bomin8319/Desktop/DLFM/UNfit/DLFM_NA.RData")#
attach(UN_NA)#
library(ggplot2)#
library(MCMCpack)#
library(reshape2)#
library(gridExtra)#
library(ggrepel)
years = c(1983:2014)
colors = sort(rownames(U[[32]]))#
	thetanew = sapply(1:32, function(t){colMeans(UN_NA$theta[[t]])})#
	data3 = data.frame(years = years, theta = t(thetanew))
head(data3)
orders = sapply(1:23, function(n){which(colors[n]== colnames(data3)[-1])})
data3 = data3[,orders]
head(data3)
thetanew = t(sapply(1:32, function(t){colMeans(UN_NA$theta[[t]])}))
head(thetanew)
thetanew = t(sapply(1:32, function(t){colMeans(UN_NA$theta[[t]])}))#
	orders = sapply(1:23, function(n){which(colors[n]== rownames(U[[32]]))})
orders
thetanew = thetanew[,orders]
head(thetanew)
data3 = data.frame(years = years, theta = thetanew)
colnames(data3)[-1] = colors
head(data3)
data3new = melt(data3, id = "years")
colnames(data3new)[3] = "theta"
f <- ggplot(data3new, aes(years, theta, colour = variable, label = variable))
f + geom_line() + scale_x_continuous(breaks=number_ticks(8)) + scale_colour_discrete(name = "countries")
number_ticks <- function(n) {function(limits) pretty(limits, n)}
f + geom_line() + scale_x_continuous(breaks=number_ticks(8)) + scale_colour_discrete(name = "countries")
f + geom_line() + scale_x_continuous(breaks=number_ticks(8)) + scale_colour_discrete(name = "countries")+ geom_text()
data3new[data3new$year==2014,]
data3new[data3new$year==2014,3]
f + geom_line() + scale_x_continuous(breaks=number_ticks(8)) + scale_colour_discrete(name = "countries")+ annotate("text", x = rep(2014, 23), y = data3new[data3new$year==2014,3], label = colors)
f + geom_line() + scale_x_continuous(breaks=number_ticks(8)) + scale_colour_discrete(name = "countries")+ annotate("text", x = rep(2014, 23), y = data3new[data3new$year==2014,3], label = colors, colour = colors)
f + geom_line() + scale_x_continuous(breaks=number_ticks(8)) + scale_colour_discrete(name = "countries")+ annotate("text", x = rep(2014, 23), y = data3new[data3new$year==2014,3], label = colors, colour = factor(colors))
f + geom_line() + scale_x_continuous(breaks=number_ticks(8)) + scale_colour_discrete(name = "countries")+ annotate("text", x = rep(2014, 23), y = data3new[data3new$year==2014,3], label = colors, aes(color = colors))
f + geom_line() + scale_x_continuous(breaks=number_ticks(8)) + scale_colour_discrete(name = "countries")+ annotate("text", x = rep(2014, 23), y = data3new[data3new$year==2014,3], label = colors, color = factor(1:23))
f + geom_line() + scale_x_continuous(breaks=number_ticks(8)) + scale_colour_discrete(name = "countries")+ annotate("text", x = rep(2014, 23), y = data3new[data3new$year==2014,3], label = colors, fill = factor(1:23))
f + geom_line() + scale_x_continuous(breaks=number_ticks(8)) + scale_colour_discrete(name = "countries")+ geom_text(aes(x = rep(2014, 23), y = data3new[data3new$year==2014,3], label = colors, colour = colors))
f + geom_line() + scale_x_continuous(breaks=number_ticks(8)) + scale_colour_discrete(name = "countries")+ geom_text(aes(x = rep(2014, 23), y = data3new[data3new$year==2014,3], label = colors))
eom_text(x = rep(2014, 23), y = data3new[data3new$year==2014,3], label = colors))
f + geom_line() + scale_x_continuous(breaks=number_ticks(8)) + scale_colour_discrete(name = "countries")+ geom_text(x = rep(2014, 23), y = data3new[data3new$year==2014,3], label = colors))
f + geom_line() + scale_x_continuous(breaks=number_ticks(8)) + scale_colour_discrete(name = "countries")+ annotate("text", x = rep(2014, 23), y = data3new[data3new$year==2014,3], label = colors, fill = factor(1:23))
f + geom_line() + scale_x_continuous(breaks=number_ticks(8)) + scale_colour_discrete(name = "countries")
load("/Users/bomin8319/Desktop/DLFM/UNfit/UNdatafull.RData")
load(DLFM)#
reduced <-which(dimnames(Y)[[2]] %in% c("USA", "CHN", "IND", "UKG", "FRN", "GFR", "TUR", "JPN", "ISR", "SYR", "LEB", "SUD", "IRN", "AUL", "PAK", "EGY", "AFG", "PRK", "RUS", "GRG", "UKR", "ROK", "IRQ"))#
#
#reduced <-which(dimnames(Y)[[2]] %in% c("USA", "CHN", "IND", "UKG", "FRN", "GFR", "TUR", "JPN", "ISR", "SYR", "LEB", "SUD", "IRN", "AUL", "PAK", "EGY"))#
Y = Y[, reduced, reduced]#
X = X[, reduced, reduced, 1:5]#
#
for(tp in 1:32){#
	diag(X[tp,,,2]) = 0#
}#
# not existing countries -> all missing values imputed using model (biased)#
avail1 = matrix(1, 32, 23)#
avail1[1:4, 21:23] = 0#
avail1[5:6, 21:22] = 0#
avail1[7, 21] = 0#
avail1[13:21, 9] = 0
50000/250
50000/200
50000/100
UN_NA = DLFM_NA(Y[1:32,,], X[1:32,,,1:5], R = 2, avail = avail1, burn = 40000, prescan = 10000, nscan = 50000, odens = 100, seed = 1)
attach(UNdatafull)#
library(FastGP)#
library(mvtnorm)#
library(fields)#
library(reshape)#
library(MCMCpack)
library(expm)
UN_NA = DLFM_NA(Y[1:32,,], X[1:32,,,1:5], R = 2, avail = avail1, burn = 40000, prescan = 10000, nscan = 50000, odens = 100, seed = 1)
UN_NA2 = DLFM_NA(Y[1:32,,], X[1:32,,,1], R = 2, avail = avail1, burn = 40000, prescan = 10000, nscan = 50000, odens = 100, seed = 1)
UN_NA1 = DLFM_NA(Y[1:32,,], X[1:32,,,1], R = 2, avail = avail1, burn = 15000, prescan = 5000, nscan = 30000, odens = 100, seed = 1)#
UN_NA2 = DLFM_NA(Y[1:32,,], X[1:32,,,1:2], R = 2, avail = avail1, burn = 15000, prescan = 5000, nscan = 30000, odens = 100, seed = 1)#
UN_NA3 = DLFM_NA(Y[1:32,,], X[1:32,,,1:3], R = 2, avail = avail1, burn = 15000, prescan = 5000, nscan = 30000, odens = 100, seed = 1)#
UN_NA4 = DLFM_NA(Y[1:32,,], X[1:32,,,1:4], R = 2, avail = avail1, burn = 15000, prescan = 5000, nscan = 30000, odens = 100, seed = 1)#
UN_NA5 = DLFM_NA(Y[1:32,,], X[1:32,,,1:5], R = 2, avail = avail1, burn = 15000, prescan = 5000, nscan = 30000, odens = 100, seed = 1)
names(UN_NA5)
UN_NA1 = DLFM_NA(Y[1:32,,], X[1:32,,,1], R = 2, avail = avail1, burn = 15000, prescan = 5000, nscan = 30000, odens = 100, seed = 1, plot = FALSE)#
UN_NA2 = DLFM_NA(Y[1:32,,], X[1:32,,,1:2], R = 2, avail = avail1, burn = 15000, prescan = 5000, nscan = 30000, odens = 100, seed = 1, plot = FALSE)#
UN_NA3 = DLFM_NA(Y[1:32,,], X[1:32,,,1:3], R = 2, avail = avail1, burn = 15000, prescan = 5000, nscan = 30000, odens = 100, seed = 1, plot = FALSE)#
UN_NA4 = DLFM_NA(Y[1:32,,], X[1:32,,,1:4], R = 2, avail = avail1, burn = 15000, prescan = 5000, nscan = 30000, odens = 100, seed = 1, plot = FALSE)#
UN_NA5 = DLFM_NA(Y[1:32,,], X[1:32,,,1:5], R = 2, avail = avail1, burn = 15000, prescan = 5000, nscan = 30000, odens = 100, seed = 1, plot = FALSE)
names(UN_NA1)
DLFM_NA
library(DLFM)
UN_NA
DLFM_NA
Rprof('test.out')
library(mvtnorm)#
library(MCMCpack)#
library(BayesOFsurv)#
#
n = 1000#
nsims = 1#
#create covariates#
x<-runif(n, min=-2.5, max=12)#
z<-log(runif(n, min=1, max=100))#
tru.est<-matrix(NA,nrow=nsims,ncol=8)#
i = 1#
  #Assign parameter values#
  tru.est[i,1]<-1#
  tru.est[i,2]<-3.5#
  tru.est[i,3]<--2#
  tru.est[i,4]<-2#
  tru.est[i,5]<-3#
  tru.est[i,6]<-1#
  myrates <- exp(tru.est[i,1]+(tru.est[i,2]*x)) #
  y <- rexp(n, rate = myrates) # generates the r.v.#
  cen <- rexp(n, rate = 1 )#
  ycen <- pmin(y, cen)#
  di <- as.numeric(y <= cen)#
  tru.est[i,7]<-table(di)[1]#
  #create parameters for ZG#
  alpha<-1/(1+exp(-(tru.est[i,3]+tru.est[i,4]*z+tru.est[i,5]*x)))#
  print(mean(alpha))#
  yzero<-matrix(1,n,1)#
  error<--1*rlogis(n)#
  flag<-error<qlogis(alpha)#
  yzero[flag]<-error[flag]#
  flag<-yzero==1#
  di[flag]<-ifelse(di[flag]==0,yzero[flag],di[flag])#
  tru.est[i,8]<-table(di)[1]#
  data<-cbind(ycen,di,x,z)#
  data<-data#
  Y<-ycen#
  C<-di#
  X<-cbind(1,x)#
  Z<-cbind(1,z,x)#
######### Try fit the data using Bayesian OF model #############  #
Weibull = mcmcOF(Y, C, X, Z, N = 10000, burn = 5000, thin = 20,  w = c(1, 1, 1), m = 10, form = "Weibull", seed = 100)
document()
library(devtools)
document()
setwd('/Users/bomin8319/Desktop/BayesOFsurv/pkg/R')
document()
check()
install()
Rprof('try.out')
library(mvtnorm)#
library(MCMCpack)#
library(BayesOFsurv)#
#
n = 1000#
nsims = 1#
#create covariates#
x<-runif(n, min=-2.5, max=12)#
z<-log(runif(n, min=1, max=100))#
tru.est<-matrix(NA,nrow=nsims,ncol=8)#
i = 1#
  #Assign parameter values#
  tru.est[i,1]<-1#
  tru.est[i,2]<-3.5#
  tru.est[i,3]<--2#
  tru.est[i,4]<-2#
  tru.est[i,5]<-3#
  tru.est[i,6]<-1#
  myrates <- exp(tru.est[i,1]+(tru.est[i,2]*x)) #
  y <- rexp(n, rate = myrates) # generates the r.v.#
  cen <- rexp(n, rate = 1 )#
  ycen <- pmin(y, cen)#
  di <- as.numeric(y <= cen)#
  tru.est[i,7]<-table(di)[1]#
  #create parameters for ZG#
  alpha<-1/(1+exp(-(tru.est[i,3]+tru.est[i,4]*z+tru.est[i,5]*x)))#
  print(mean(alpha))#
  yzero<-matrix(1,n,1)#
  error<--1*rlogis(n)#
  flag<-error<qlogis(alpha)#
  yzero[flag]<-error[flag]#
  flag<-yzero==1#
  di[flag]<-ifelse(di[flag]==0,yzero[flag],di[flag])#
  tru.est[i,8]<-table(di)[1]#
  data<-cbind(ycen,di,x,z)#
  data<-data#
  Y<-ycen#
  C<-di#
  X<-cbind(1,x)#
  Z<-cbind(1,z,x)#
######### Try fit the data using Bayesian OF model #############  #
Weibull = mcmcOF(Y, C, X, Z, N = 10000, burn = 5000, thin = 20,  w = c(1, 1, 1), m = 10, form = "Weibull", seed = 100)
Weibull = mcmcOF(Y, C, X, Z, N = 5000, burn = 1000, thin = 10,  w = c(1, 1, 1), m = 10, form = "Weibull", seed = 100)
summryRprof('Try.out')
summaryRprof('Try.out')
library(profr)
install.packages("profr")
library(profr)
par(mfrow = c(2,4))#
plot(Weibull$loglike, type = 'l')#
for (p in 1:2) {#
  plot(Weibull$beta[,p], type = 'l')#
}#
for (p in 1:3) {#
  plot(Weibull$gamma[,p], type = 'l')#
}#
plot(Weibull$lambda, type = 'l')
beta
betas
beta
data
head(data)
head(tru.est)
llikWeibull
library(BayesOFsurv)
llikWeibull
library(devtools)
document()
document()
check()
install()
#clear memory#
rm( list=ls() )#
#load necessary libraries 						                                 #
library(foreign)#
library(Zelig)#
library(car)#
library(MASS)#
library(VGAM)#
library(plotrix)#
library(pscl)#
library(survival)#
library(msm)#
library(verification)#
library(corpcor)#
library(Design)#
library(mvtnorm)#
library(MCMCpack)#
library(BayesOFsurv)#
#set working directory#
setwd("/Users/bomin8319/Desktop/BayesOFsurv/coding material/Monte Carlos/Mixture DGP/")#
###########################################################################
###########################################################################
############################Monte Carlo####################################
###########################################################################
#set seed#
set.seed(300)   #
#set the number of observations#
n<-100#
#set the number of simulations, and create matrices to store the results#
nsims<-2#
#history matrix for true estimates#
tru.est<-matrix(NA,nrow=nsims,ncol=8)#
#history matrix for cox estimates#
cox.est<-matrix(NA,nrow=nsims,ncol=2)#
#history matrix for exp estimates#
exp.est<-matrix(NA,nrow=nsims,ncol=24)#
#history matrix for weibull estimates#
weib.est<-matrix(NA,nrow=nsims,ncol=30)#
#history matrix for cox RMSE#
cox.rmse<-matrix(NA,nrow=nsims,ncol=1)#
#history matrix for exp RMSE#
exp.rmse<-matrix(NA,nrow=nsims,ncol=12)#
#history matrix for exp RMSE#
weib.rmse<-matrix(NA,nrow=nsims,ncol=15)#
#history matrix for cox CP#
cox.cp<-matrix(NA,nrow=nsims,ncol=1)#
#history matrix for exp CP#
exp.cp<-matrix(NA,nrow=nsims,ncol=12)#
#history matrix for exp CP#
weib.cp<-matrix(NA,nrow=nsims,ncol=15)#
#create covariates#
x<-runif(n, min=-2.5, max=12)#
z<-log(runif(n, min=1, max=100))#
#create a dependent variable, begin the simmulations#
for(i in 1:nsims){#
#Assign parameter values#
tru.est[i,1]<-1#
tru.est[i,2]<-3.5#
tru.est[i,3]<--2#
tru.est[i,4]<-2#
tru.est[i,5]<-3#
tru.est[i,6]<-1#
myrates <- exp(tru.est[i,1]+(tru.est[i,2]*x)) #
y <- rexp(n, rate = myrates) # generates the r.v.#
cen <- rexp(n, rate = 1 )#
ycen <- pmin(y, cen)#
di <- as.numeric(y <= cen)#
tru.est[i,7]<-table(di)[1]#
#create parameters for ZG#
phi<-1/(1+exp(-(tru.est[i,3]+tru.est[i,4]*z+tru.est[i,5]*x)))#
print(mean(phi))#
yzero<-matrix(1,n,1)#
error<--1*rlogis(n)#
flag<-error<qlogis(phi)#
yzero[flag]<-error[flag]#
flag<-yzero==1#
di[flag]<-ifelse(di[flag]==0,yzero[flag],di[flag])#
tru.est[i,8]<-table(di)[1]#
data<-cbind(ycen,di,x,z)#
######################################################################################
###################################COX Model##########################################
######################################################################################
#store estimate and se#
cox.est[i,1]<-summary(coxph(Surv(ycen, di)~x,coxph.control(iter.max = 10000)))$coef[1]#
cox.est[i,2]<-summary(coxph(Surv(ycen, di)~x,coxph.control(iter.max = 10000)))$coef[3]#
#store rmse#
cox.rmse[i,1]<-sqrt((tru.est[i,2]-cox.est[i,1])^2)#
#calculate upper and lower 95% CI's#
b1.lower<-cox.est[i,1]-(1.959964*cox.est[i,2])#
b1.upper<-cox.est[i,1]+(1.959964*cox.est[i,2])#
#store coverage parameters#
cox.cp[i,1]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
##############################################################################
########################Simple Exponential Model##############################
##############################################################################
Exponential<- function(est,Y,C,X,data) {					      #
	n=nrow(data)							      					  #
	llik <- matrix(0, nrow=n, ncol = 1)#
	beta<-est[1:length(est)]#
	XB<-X%*%beta#
	llik<-C*(XB-exp(XB)*Y)+(1-C)*(-exp(XB)*Y)#
	llik<--1*sum(llik)#
	return(llik)#
	}#
#set starting parameters#
est<-rbind(.01,.01)#
#set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
#optimize#
output.Exponential<-try(optim(f=Exponential,  p=est, X=X,Y=Y,C=C, method="BFGS", control=list(maxit=10000),  data=data, hessian=TRUE), TRUE)#
if(class(output.Exponential)=="list"){#
	ifelse(is.positive.definite(output.Exponential$hessian)==TRUE,vcv<-solve(output.Exponential$hessian),vcv<-matrix(data=NA,nrow=2,ncol=2))#
#store betas and ses#
exp.est[i,1]<-output.Exponential$par[1]#
exp.est[i,2]<-sqrt(vcv[1,1])#
exp.est[i,3]<-output.Exponential$par[2]#
exp.est[i,4]<-sqrt(vcv[2,2])#
#store rmse#
exp.rmse[i,1]<-sqrt((tru.est[i,1]-exp.est[i,1])^2)#
exp.rmse[i,2]<-sqrt((tru.est[i,2]-exp.est[i,3])^2)#
#calculate upper and lower 95% CI's#
b0.lower<-exp.est[i,1]-(1.959964*exp.est[i,2])#
b0.upper<-exp.est[i,1]+(1.959964*exp.est[i,2])#
b1.lower<-exp.est[i,3]-(1.959964*exp.est[i,4])#
b1.upper<-exp.est[i,3]+(1.959964*exp.est[i,4])#
#store coverage parameters#
exp.cp[i,1]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
exp.cp[i,2]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
}#
#################################################################################
#########################Simple Weibull Model ###################################
#################################################################################
#Note this estiamtes the model via hazard rates, a la Stata#
test<-survreg(Surv(ycen, di)~x, dist="weibull")#
summary(test)#
Weibull<- function(est,Y,C,X,data) {					      #
	n=nrow(data)							      					  #
	llik <- matrix(0, nrow=n, ncol = 1)#
	beta<-est[1:length(est)-1]#
	p<-est[length(est)]#
	p<-exp(p)#
	XB<-X%*%beta#
	llik<-C*(log(exp(XB+1/p)*p*((exp(XB+1/p)*Y)^(p-1))*exp(-(exp(XB+1/p)*Y)^p)))+(1-C)*log(exp(-(exp(XB+1/p)*Y)^p))#
	llik<--1*sum(llik)#
	return(llik)#
	}#
#set starting parameters#
est<-rbind(exp.est[i,1],exp.est[i,3],.01)#
#set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
#optimize#
output.Weibull<-try(optim(f=Weibull,  p=est, X=X,Y=Y,C=C, method="BFGS", control=list(maxit=10000),  data=data, hessian=TRUE), TRUE)#
if(class(output.Weibull)=="list"){#
	ifelse(is.positive.definite(output.Weibull$hessian)==TRUE,vcv<-solve(output.Weibull$hessian),vcv<-matrix(data=NA,nrow=3,ncol=3))#
#store betas and ses#
weib.est[i,1]<-output.Weibull$par[1]+1/exp(output.Weibull$par[3])#
coeff<-c(weib.est[i,1],output.Weibull$par[3])#
varcov<-matrix(NA,2,2)#
varcov[1,1]<-vcv[1,1]#
varcov[1,2]<-vcv[1,3]#
varcov[2,1]<-vcv[3,1]#
varcov[2,2]<-vcv[3,3]#
weib.est[i,2]<-deltamethod(~(x1+1/exp(x2)), coeff, varcov, ses=TRUE)#
weib.est[i,3]<-output.Weibull$par[2]#
weib.est[i,4]<-sqrt(vcv[2,2])#
weib.est[i,5]<-exp(output.Weibull$par[3])#
coeff<-c(weib.est[i,5])#
varcov<-matrix(NA,1,1)#
varcov[1,1]<-vcv[3,3]#
weib.est[i,6]<-deltamethod(~(exp(x1)), coeff, varcov, ses=TRUE)#
#store rmse#
weib.rmse[i,1]<-sqrt((tru.est[i,1]-weib.est[i,1])^2)#
weib.rmse[i,2]<-sqrt((tru.est[i,2]-weib.est[i,3])^2)#
weib.rmse[i,3]<-sqrt((tru.est[i,6]-weib.est[i,5])^2)#
#calculate upper and lower 95% CI's#
b0.lower<-weib.est[i,1]-(1.959964*weib.est[i,2])#
b0.upper<-weib.est[i,1]+(1.959964*weib.est[i,2])#
b1.lower<-weib.est[i,3]-(1.959964*weib.est[i,4])#
b1.upper<-weib.est[i,3]+(1.959964*weib.est[i,4])#
p.lower<-weib.est[i,5]-(1.959964*weib.est[i,6])#
p.upper<-weib.est[i,5]+(1.959964*weib.est[i,6])#
#store coverage parameters#
weib.cp[i,1]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
weib.cp[i,2]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
weib.cp[i,3]<-ifelse(tru.est[i,6]>p.lower & tru.est[i,6]<p.upper, 1,0)#
}#
###logit estimates####
dataset<-as.data.frame(data)#
logitcoef1<-glm(di~ z+x, data = dataset, family = "binomial")$coef[1]#
logitcoef2<-glm(di~ z+x, data = dataset, family = "binomial")$coef[2]#
logitcoef3<-glm(di~ z+x, data = dataset, family = "binomial")$coef[3]#
################################################################################
##########################Zombie Exponential Model##############################
################################################################################
#This program estimates the Exponential loglikelihood function returning hazard rate form coefficients#
ZExponential<- function(est,Y,C,X,Z,data) {					      #
	n=nrow(data)							      					  #
	llik <- matrix(0, nrow=n, ncol = 1)#
	gamma<-est[1:ncol(Z)]#
	beta<-est[(ncol(Z)+1):length(est)]#
	XB<-X%*%beta#
	ZG<-Z%*%gamma#
	phi<-1/(1+exp(-ZG))#
	llik<-C*(log(phi*exp(-exp(XB)*Y)+(1-phi)*exp(XB)*exp(-exp(XB)*Y)))+(1-C)*(log(phi)+-exp(XB)*Y)#
	llik<--1*sum(llik)#
	return(llik)#
	}#
#set starting parameters#
est<-rbind(.01,.01,.01,exp.est[i,1],exp.est[i,3])#
#set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
Z<-cbind(1,z,x)#
#optimize#
output.ZExponential<-try(optim(f=ZExponential,  p=est, X=X,Y=Y,C=C,Z=Z, method="BFGS", control=list(maxit=10000),  data=data, hessian=TRUE), TRUE)#
if(class(output.ZExponential)=="list"){#
	ifelse(is.positive.definite(output.ZExponential$hessian)==TRUE,vcv<-solve(output.ZExponential$hessian),vcv<-matrix(data=NA,nrow=5,ncol=5))#
#store betas and ses#
exp.est[i,5]<-output.ZExponential$par[1]#
exp.est[i,6]<-sqrt(vcv[1,1])#
exp.est[i,7]<-output.ZExponential$par[2]#
exp.est[i,8]<-sqrt(vcv[2,2])#
exp.est[i,9]<-output.ZExponential$par[3]#
exp.est[i,10]<-sqrt(vcv[3,3])#
exp.est[i,11]<-output.ZExponential$par[4]#
exp.est[i,12]<-sqrt(vcv[4,4])#
exp.est[i,13]<-output.ZExponential$par[5]#
exp.est[i,14]<-sqrt(vcv[5,5])#
#store rmse#
exp.rmse[i,3]<-sqrt((tru.est[i,3]-exp.est[i,5])^2)#
exp.rmse[i,4]<-sqrt((tru.est[i,4]-exp.est[i,7])^2)#
exp.rmse[i,5]<-sqrt((tru.est[i,5]-exp.est[i,9])^2)#
exp.rmse[i,6]<-sqrt((tru.est[i,1]-exp.est[i,11])^2)#
exp.rmse[i,7]<-sqrt((tru.est[i,2]-exp.est[i,13])^2)#
#calculate upper and lower 95% CI's#
g0.lower<-exp.est[i,5]-(1.959964*exp.est[i,6])#
g0.upper<-exp.est[i,5]+(1.959964*exp.est[i,6])#
g1.lower<-exp.est[i,7]-(1.959964*exp.est[i,8])#
g1.upper<-exp.est[i,7]+(1.959964*exp.est[i,8])#
g2.lower<-exp.est[i,9]-(1.959964*exp.est[i,10])#
g2.upper<-exp.est[i,9]+(1.959964*exp.est[i,10])#
b0.lower<-exp.est[i,11]-(1.959964*exp.est[i,12])#
b0.upper<-exp.est[i,11]+(1.959964*exp.est[i,12])#
b1.lower<-exp.est[i,13]-(1.959964*exp.est[i,14])#
b1.upper<-exp.est[i,13]+(1.959964*exp.est[i,14])#
#store coverage parameters#
exp.cp[i,3]<-ifelse(tru.est[i,3]>g0.lower & tru.est[i,3]<g0.upper, 1,0)#
exp.cp[i,4]<-ifelse(tru.est[i,4]>g1.lower & tru.est[i,4]<g1.upper, 1,0)#
exp.cp[i,5]<-ifelse(tru.est[i,5]>g2.lower & tru.est[i,5]<g2.upper, 1,0)#
exp.cp[i,6]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
exp.cp[i,7]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
}#
######################################################################################
##########################Zombie Weibull Model #######################################
######################################################################################
#This program estimates the Exponential loglikelihood function returning hazard rate form coefficients#
ZWeibull<- function(est,Y,C,X,Z,data) {					      #
	n=nrow(data)							      					  #
	llik <- matrix(0, nrow=n, ncol = 1)#
	gamma<-est[1:ncol(Z)]#
	beta<-est[(ncol(Z)+1):(length(est)-1)]#
	p<-est[length(est)]#
	p<-exp(p)#
	XB<-X%*%beta#
	ZG<-Z%*%gamma#
	phi<-1/(1+exp(-(ZG+1/p)))#
	llik<-C*(log(phi*exp(-(exp(XB+1/p)*Y)^p)+(1-phi)*exp(XB+1/p)*p*((exp(XB+1/p)*Y)^(p-1))*exp(-(exp(XB+1/p)*Y)^p)))+(1-C)*(log(phi)+-(exp(XB+1/p)*Y)^p)#
	llik<--1*sum(llik)#
	return(llik)#
	}#
#set starting parameters#
est<-rbind(.01,.01,.01,output.Weibull$par[1],output.Weibull$par[2],output.Weibull$par[3])#
#set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
Z<-cbind(1,z,x)#
#optimize#
output.ZWeibull<-try(optim(f=ZWeibull,  p=est, X=X,Y=Y,C=C,Z=Z, method="BFGS", control=list(maxit=10000),  data=data, hessian=TRUE), TRUE)#
if(class(output.ZWeibull)=="list"){#
	ifelse(is.positive.definite(output.ZWeibull$hessian)==TRUE,vcv<-solve(output.ZWeibull$hessian),vcv<-matrix(data=NA,nrow=6,ncol=6))#
#store betas and ses#
weib.est[i,7]<-output.ZWeibull$par[1]+1/exp(output.ZWeibull$par[6])#
coeff<-c(weib.est[i,7],output.ZWeibull$par[6])#
varcov<-matrix(NA,2,2)#
varcov[1,1]<-vcv[1,1]#
varcov[1,2]<-vcv[1,6]#
varcov[2,1]<-vcv[6,1]#
varcov[2,2]<-vcv[6,6]#
weib.est[i,8]<-deltamethod(~(x1+1/exp(x2)), coeff, varcov, ses=TRUE)#
weib.est[i,9]<-output.ZWeibull$par[2]#
weib.est[i,10]<-sqrt(vcv[2,2])#
weib.est[i,11]<-output.ZWeibull$par[3]#
weib.est[i,12]<-sqrt(vcv[3,3])#
weib.est[i,13]<-output.ZWeibull$par[4]+1/exp(output.ZWeibull$par[6])#
coeff<-c(weib.est[i,13],output.ZWeibull$par[6])#
varcov<-matrix(NA,2,2)#
varcov[1,1]<-vcv[4,4]#
varcov[1,2]<-vcv[4,6]#
varcov[2,1]<-vcv[6,4]#
varcov[2,2]<-vcv[6,6]#
weib.est[i,14]<-deltamethod(~(x1+1/exp(x2)), coeff, varcov, ses=TRUE)#
weib.est[i,15]<-output.ZWeibull$par[5]#
weib.est[i,16]<-sqrt(vcv[5,5])#
weib.est[i,17]<-exp(output.ZWeibull$par[6])#
coeff<-c(weib.est[i,17])#
varcov<-matrix(NA,1,1)#
varcov[1,1]<-vcv[6,6]#
weib.est[i,18]<-deltamethod(~(exp(x1)), coeff, varcov, ses=TRUE)#
#store rmse#
weib.rmse[i,4]<-sqrt((tru.est[i,3]-weib.est[i,7])^2)#
weib.rmse[i,5]<-sqrt((tru.est[i,4]-weib.est[i,9])^2)#
weib.rmse[i,6]<-sqrt((tru.est[i,5]-weib.est[i,11])^2)#
weib.rmse[i,7]<-sqrt((tru.est[i,1]-weib.est[i,13])^2)#
weib.rmse[i,8]<-sqrt((tru.est[i,2]-weib.est[i,15])^2)#
weib.rmse[i,9]<-sqrt((tru.est[i,6]-weib.est[i,17])^2)#
#calculate upper and lower 95% CI's#
g0.lower<-weib.est[i,7]-(1.959964*weib.est[i,8])#
g0.upper<-weib.est[i,7]+(1.959964*weib.est[i,8])#
g1.lower<-weib.est[i,9]-(1.959964*weib.est[i,10])#
g1.upper<-weib.est[i,9]+(1.959964*weib.est[i,10])#
g2.lower<-weib.est[i,11]-(1.959964*weib.est[i,12])#
g2.upper<-weib.est[i,11]+(1.959964*weib.est[i,12])#
b0.lower<-weib.est[i,13]-(1.959964*weib.est[i,14])#
b0.upper<-weib.est[i,13]+(1.959964*weib.est[i,14])#
b1.lower<-weib.est[i,15]-(1.959964*weib.est[i,16])#
b1.upper<-weib.est[i,15]+(1.959964*weib.est[i,16])#
p.lower<-weib.est[i,17]-(1.959964*weib.est[i,18])#
p.upper<-weib.est[i,17]+(1.959964*weib.est[i,18])#
#store coverage parameters#
weib.cp[i,4]<-ifelse(tru.est[i,3]>g0.lower & tru.est[i,3]<g0.upper, 1,0)#
weib.cp[i,5]<-ifelse(tru.est[i,4]>g1.lower & tru.est[i,4]<g1.upper, 1,0)#
weib.cp[i,6]<-ifelse(tru.est[i,5]>g2.lower & tru.est[i,5]<g2.upper, 1,0)#
weib.cp[i,7]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
weib.cp[i,8]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
weib.cp[i,9]<-ifelse(tru.est[i,6]>p.lower & tru.est[i,6]<p.upper, 1,0)#
}#
################################################################################
######################Bayesian Zombie Exponential Model#########################
################################################################################
#set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
Z<-cbind(1,z,x)#
BayesZExponential = mcmcOF(Y, C, X, Z, N = 3000, burn = 1000, thin = 20,  w = c(1, 1, 1), m = 10, form = "Exponential")#
output.BayesZExponential = list(par = c(summary(mcmc(BayesZExponential$beta))[[1]][,1], summary(mcmc(BayesZExponential$gamma))[[1]][,1]), #
								se = c(summary(mcmc(BayesZExponential$beta))[[1]][,2], summary(mcmc(BayesZExponential$gamma))[[1]][,2]),#
								CI = rbind(summary(mcmc(BayesZExponential$beta))[[2]], summary(mcmc(BayesZExponential$gamma))[[2]]))#
exp.est[i,15]<-output.BayesZExponential$par[1]#
exp.est[i,16]<-output.BayesZExponential$se[1]#
exp.est[i,17]<-output.BayesZExponential$par[2]#
exp.est[i,18]<-output.BayesZExponential$se[2]#
exp.est[i,19]<-output.BayesZExponential$par[3]#
exp.est[i,20]<-output.BayesZExponential$se[3]#
exp.est[i,21]<-output.BayesZExponential$par[4]#
exp.est[i,22]<-output.BayesZExponential$se[4]#
exp.est[i,23]<-output.BayesZExponential$par[5]#
exp.est[i,24]<-output.BayesZExponential$se[5]#
#
#store rmse#
exp.rmse[i,8]<-sqrt((tru.est[i,3]-exp.est[i,15])^2)#
exp.rmse[i,9]<-sqrt((tru.est[i,4]-exp.est[i,17])^2)#
exp.rmse[i,10]<-sqrt((tru.est[i,5]-exp.est[i,19])^2)#
exp.rmse[i,11]<-sqrt((tru.est[i,1]-exp.est[i,21])^2)#
exp.rmse[i,12]<-sqrt((tru.est[i,2]-exp.est[i,23])^2)#
#
#calculate upper and lower 95% CI's#
# b0.lower<-output.BayesZExponential$CI[1,1]#
# b0.upper<-output.BayesZExponential$CI[1,5]#
# b1.lower<-output.BayesZExponential$CI[2,1]#
# b1.upper<-output.BayesZExponential$CI[2,5]#
# g0.lower<-output.BayesZExponential$CI[3,1]#
# g0.upper<-output.BayesZExponential$CI[3,5]#
# g1.lower<-output.BayesZExponential$CI[4,1]#
# g1.upper<-output.BayesZExponential$CI[4,5]#
# g2.lower<-output.BayesZExponential$CI[5,1]#
# g2.upper<-output.BayesZExponential$CI[5,5]#
b0.lower<-exp.est[i,15]-(1.959964*exp.est[i,16])#
b0.upper<-exp.est[i,15]+(1.959964*exp.est[i,16])#
b1.lower<-exp.est[i,17]-(1.959964*exp.est[i,18])#
b1.upper<-exp.est[i,17]+(1.959964*exp.est[i,18])#
g0.lower<-exp.est[i,19]-(1.959964*exp.est[i,20])#
g0.upper<-exp.est[i,19]+(1.959964*exp.est[i,20])#
g1.lower<-exp.est[i,21]-(1.959964*exp.est[i,22])#
g1.upper<-exp.est[i,21]+(1.959964*exp.est[i,22])#
g2.lower<-exp.est[i,23]-(1.959964*exp.est[i,24])#
g2.upper<-exp.est[i,23]+(1.959964*exp.est[i,24])#
#store coverage parameters#
exp.cp[i,8]<-ifelse(tru.est[i,3]>g0.lower & tru.est[i,3]<g0.upper, 1,0)#
exp.cp[i,9]<-ifelse(tru.est[i,4]>g1.lower & tru.est[i,4]<g1.upper, 1,0)#
exp.cp[i,10]<-ifelse(tru.est[i,5]>g2.lower & tru.est[i,5]<g2.upper, 1,0)#
exp.cp[i,11]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
exp.cp[i,12]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
#
################################################################################
########################Bayesian Zombie Weibull Model###########################
################################################################################
#set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
Z<-cbind(1,z,x)#
BayesZWeibull = mcmcOF(Y, C, X, Z, N = 3000, burn = 1000, thin = 20,  w = c(1, 1, 1), m = 10, form = "Weibull")#
output.BayesZWeibull = list(par = c(summary(mcmc(BayesZWeibull$beta))[[1]][,1], summary(mcmc(BayesZWeibull$gamma))[[1]][,1], #
									summary(mcmc(BayesZWeibull$lambda))[[1]][1]), #
								se = c(summary(mcmc(BayesZWeibull$beta))[[1]][,2], summary(mcmc(BayesZWeibull$gamma))[[1]][,2], #
									   summary(mcmc(BayesZWeibull$lambda))[[1]][2]),#
								CI = rbind(summary(mcmc(BayesZWeibull$beta))[[2]], summary(mcmc(BayesZWeibull$gamma))[[2]], #
										summary(mcmc(BayesZWeibull$lambda))[[2]]))#
#
weib.est[i,19]<-output.BayesZWeibull$par[1]#
weib.est[i,20]<-output.BayesZWeibull$se[1]#
weib.est[i,21]<-output.BayesZWeibull$par[2]#
weib.est[i,22]<-output.BayesZWeibull$se[2]#
weib.est[i,23]<-output.BayesZWeibull$par[3]#
weib.est[i,24]<-output.BayesZWeibull$se[3]#
weib.est[i,25]<-output.BayesZWeibull$par[4]#
weib.est[i,26]<-output.BayesZWeibull$se[4]#
weib.est[i,27]<-output.BayesZWeibull$par[5]#
weib.est[i,28]<-output.BayesZWeibull$se[5]#
weib.est[i,29]<-output.BayesZWeibull$par[6]#
weib.est[i,30]<-output.BayesZWeibull$se[6]#
#
#store rmse#
weib.rmse[i,10]<-sqrt((tru.est[i,3]-weib.est[i,19])^2)#
weib.rmse[i,11]<-sqrt((tru.est[i,4]-weib.est[i,21])^2)#
weib.rmse[i,12]<-sqrt((tru.est[i,5]-weib.est[i,23])^2)#
weib.rmse[i,13]<-sqrt((tru.est[i,1]-weib.est[i,25])^2)#
weib.rmse[i,14]<-sqrt((tru.est[i,2]-weib.est[i,27])^2)#
weib.rmse[i,15]<-sqrt((tru.est[i,6]-weib.est[i,29])^2)#
#
#calculate upper and lower 95% CI's#
# b0.lower<-output.BayesZWeibull$CI[1,1]#
# b0.upper<-output.BayesZWeibull$CI[1,5]#
# b1.lower<-output.BayesZWeibull$CI[2,1]#
# b1.upper<-output.BayesZWeibull$CI[2,5]#
# g0.lower<-output.BayesZWeibull$CI[3,1]#
# g0.upper<-output.BayesZWeibull$CI[3,5]#
# g1.lower<-output.BayesZWeibull$CI[4,1]#
# g1.upper<-output.BayesZWeibull$CI[4,5]#
# g2.lower<-output.BayesZWeibull$CI[5,1]#
# g2.upper<-output.BayesZWeibull$CI[5,5]#
# p.lower<-output.BayesZWeibull$CI[6,1]#
# p.upper<-output.BayesZWeibull$CI[6,2]#
g0.lower<-weib.est[i,19]-(1.959964*weib.est[i,20])#
g0.upper<-weib.est[i,19]+(1.959964*weib.est[i,20])#
g1.lower<-weib.est[i,21]-(1.959964*weib.est[i,22])#
g1.upper<-weib.est[i,21]+(1.959964*weib.est[i,22])#
g2.lower<-weib.est[i,23]-(1.959964*weib.est[i,24])#
g2.upper<-weib.est[i,23]+(1.959964*weib.est[i,24])#
b0.lower<-weib.est[i,25]-(1.959964*weib.est[i,26])#
b0.upper<-weib.est[i,25]+(1.959964*weib.est[i,26])#
b1.lower<-weib.est[i,27]-(1.959964*weib.est[i,28])#
b1.upper<-weib.est[i,27]+(1.959964*weib.est[i,28])#
p.lower<-weib.est[i,29]-(1.959964*weib.est[i,30])#
p.upper<-weib.est[i,29]+(1.959964*weib.est[i,30])#
#store coverage parameters#
weib.cp[i,10]<-ifelse(tru.est[i,3]>g0.lower & tru.est[i,3]<g0.upper, 1,0)#
weib.cp[i,11]<-ifelse(tru.est[i,4]>g1.lower & tru.est[i,4]<g1.upper, 1,0)#
weib.cp[i,12]<-ifelse(tru.est[i,5]>g2.lower & tru.est[i,5]<g2.upper, 1,0)#
weib.cp[i,13]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
weib.cp[i,14]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
weib.cp[i,15]<-ifelse(tru.est[i,6]>p.lower & tru.est[i,6]<p.upper, 1,0)#
}#
#combine matrices and label variables#
main.data<-cbind(tru.est, cox.est, exp.est, weib.est, cox.rmse, exp.rmse, weib.rmse, cox.cp, exp.cp, weib.cp)#
colnames(main.data)<-c("true.x0","true.x1","true.z0","true.z1","true.z2","true.p","cen.lat","cen.obs",#
	"cox.x1","cox.x1.se",#
	"exp.x0","exp.x0.se","exp.x1","exp.x1.se",#
	"zexp.z0","zexp.z0.se","zexp.z1","zexp.z1.se","zexp.z2","zexp.z2.se","zexp.x0","zexp.x0.se","zexp.x1","zexp.x1.se",#
	"bzexp.x0","zexp.x0.se","bzexp.x1","bzexp.x1.se","bzexp.z0","bzexp.z0.se","bzexp.z1","bzexp.z1.se","bzexp.z2","bzexp.z2.se",#
	"wei.x0","wei.x0.se","wei.x1","wei.x1.se","wei.p","wei.p.se",#
	"zwei.z0","zwei.z0.se","zwei.z1","zwei.z1.se","zwei.z2","zwei.z2.se","zwei.x0","zwei.x0.se","zwei.x1","zwei.x1.se","zwei.p","zwei.p.se",#
	"bzwei.x0","bzwei.x0.se","bzwei.x1","bzwei.x1.se","bzwei.z0","bzwei.z0.se","bzwei.z1","bzwei.z1.se","bzwei.z2","bzwei.z2.se","bzwei.p","bzwei.p.se",#
	"cox.x1.rmse",#
	"exp.x0.rmse","exp.x1.rmse","zexp.z0.rmse","zexp.z1.rmse","zexp.z2.rmse","zexp.x0.rmse","zexp.x1.rmse","bzexp.x0.rmse","bzexp.x1.rmse","bzexp.z0.rmse","bzexp.z1.rmse","bzexp.z2.rmse",#
	"wei.x0.rmse","wei.x1.rmse","wei.p.rmse","zwei.z0.rmse","zwei.z1.rmse","zwei.z2.rmse",#
	"zwei.x0.rmse","zwei.x1.rmse","zwei.p.rmse", "bzwei.x0.rmse","bzwei.x1.rmse","bzwei.z0.rmse","bzwei.z1.rmse","bzwei.z2.rmse","bzwei.p.rmse",#
	"cox.x1.cp","exp.x0.cp","exp.x1.cp","zexp.z0.cp","zexp.z1.cp","zexp.z2.cp","zexp.x0.cp","zexp.x1.cp","bzexp.x0.cp","bzexp.x1.cp","bzexp.z0.cp","bzexp.z1.cp","bzexp.z2.cp",#
	"wei.x0.cp","wei.x1.cp","wei.p.cp",#
	"zwei.z0.cp","zwei.z1.cp","zwei.z2.cp","zwei.x0.cp","zwei.x1.cp","zwei.p.cp", "bzwei.x0.cp","bzwei.x1.cp","bzwei.z0.cp","bzwei.z1.cp","bzwei.z2.cp","bzwei.p.cp")#
#save dataset#
main.data2<-as.data.frame(main.data)
main.data2
#clear memory#
rm( list=ls() )#
#load necessary libraries 						                                 #
library(foreign)#
library(Zelig)#
library(car)#
library(MASS)#
library(VGAM)#
library(plotrix)#
library(pscl)#
library(survival)#
library(msm)#
library(verification)#
library(corpcor)#
library(Design)#
library(mvtnorm)#
library(MCMCpack)#
library(BayesOFsurv)#
#set working directory#
setwd("/Users/bomin8319/Desktop/BayesOFsurv/coding material/Monte Carlos/Mixture DGP/")#
###########################################################################
###########################################################################
############################Monte Carlo####################################
###########################################################################
#set seed#
set.seed(3)   #
#set the number of observations#
n<-1000#
#set the number of simulations, and create matrices to store the results#
nsims<-1000#
#history matrix for true estimates#
tru.est<-matrix(NA,nrow=nsims,ncol=8)#
#history matrix for cox estimates#
cox.est<-matrix(NA,nrow=nsims,ncol=2)#
#history matrix for exp estimates#
exp.est<-matrix(NA,nrow=nsims,ncol=24)#
#history matrix for weibull estimates#
weib.est<-matrix(NA,nrow=nsims,ncol=30)#
#history matrix for cox RMSE#
cox.rmse<-matrix(NA,nrow=nsims,ncol=1)#
#history matrix for exp RMSE#
exp.rmse<-matrix(NA,nrow=nsims,ncol=12)#
#history matrix for exp RMSE#
weib.rmse<-matrix(NA,nrow=nsims,ncol=15)#
#history matrix for cox CP#
cox.cp<-matrix(NA,nrow=nsims,ncol=1)#
#history matrix for exp CP#
exp.cp<-matrix(NA,nrow=nsims,ncol=12)#
#history matrix for exp CP#
weib.cp<-matrix(NA,nrow=nsims,ncol=15)#
#create covariates#
x<-runif(n, min=-2.5, max=12)#
z<-log(runif(n, min=1, max=100))#
#create a dependent variable, begin the simmulations#
for(i in 1:nsims){#
#Assign parameter values#
tru.est[i,1]<-1#
tru.est[i,2]<-3.5#
tru.est[i,3]<--2#
tru.est[i,4]<-2#
tru.est[i,5]<-3#
tru.est[i,6]<-1#
myrates <- exp(tru.est[i,1]+(tru.est[i,2]*x)) #
y <- rexp(n, rate = myrates) # generates the r.v.#
cen <- rexp(n, rate = 1 )#
ycen <- pmin(y, cen)#
di <- as.numeric(y <= cen)#
tru.est[i,7]<-table(di)[1]#
#create parameters for ZG#
phi<-1/(1+exp(-(tru.est[i,3]+tru.est[i,4]*z+tru.est[i,5]*x)))#
print(mean(phi))#
yzero<-matrix(1,n,1)#
error<--1*rlogis(n)#
flag<-error<qlogis(phi)#
yzero[flag]<-error[flag]#
flag<-yzero==1#
di[flag]<-ifelse(di[flag]==0,yzero[flag],di[flag])#
tru.est[i,8]<-table(di)[1]#
data<-cbind(ycen,di,x,z)#
######################################################################################
###################################COX Model##########################################
######################################################################################
#store estimate and se#
cox.est[i,1]<-summary(coxph(Surv(ycen, di)~x,coxph.control(iter.max = 10000)))$coef[1]#
cox.est[i,2]<-summary(coxph(Surv(ycen, di)~x,coxph.control(iter.max = 10000)))$coef[3]#
#store rmse#
cox.rmse[i,1]<-sqrt((tru.est[i,2]-cox.est[i,1])^2)#
#calculate upper and lower 95% CI's#
b1.lower<-cox.est[i,1]-(1.959964*cox.est[i,2])#
b1.upper<-cox.est[i,1]+(1.959964*cox.est[i,2])#
#store coverage parameters#
cox.cp[i,1]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
##############################################################################
########################Simple Exponential Model##############################
##############################################################################
Exponential<- function(est,Y,C,X,data) {					      #
	n=nrow(data)							      					  #
	llik <- matrix(0, nrow=n, ncol = 1)#
	beta<-est[1:length(est)]#
	XB<-X%*%beta#
	llik<-C*(XB-exp(XB)*Y)+(1-C)*(-exp(XB)*Y)#
	llik<--1*sum(llik)#
	return(llik)#
	}#
#set starting parameters#
est<-rbind(.01,.01)#
#set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
#optimize#
output.Exponential<-try(optim(f=Exponential,  p=est, X=X,Y=Y,C=C, method="BFGS", control=list(maxit=10000),  data=data, hessian=TRUE), TRUE)#
if(class(output.Exponential)=="list"){#
	ifelse(is.positive.definite(output.Exponential$hessian)==TRUE,vcv<-solve(output.Exponential$hessian),vcv<-matrix(data=NA,nrow=2,ncol=2))#
#store betas and ses#
exp.est[i,1]<-output.Exponential$par[1]#
exp.est[i,2]<-sqrt(vcv[1,1])#
exp.est[i,3]<-output.Exponential$par[2]#
exp.est[i,4]<-sqrt(vcv[2,2])#
#store rmse#
exp.rmse[i,1]<-sqrt((tru.est[i,1]-exp.est[i,1])^2)#
exp.rmse[i,2]<-sqrt((tru.est[i,2]-exp.est[i,3])^2)#
#calculate upper and lower 95% CI's#
b0.lower<-exp.est[i,1]-(1.959964*exp.est[i,2])#
b0.upper<-exp.est[i,1]+(1.959964*exp.est[i,2])#
b1.lower<-exp.est[i,3]-(1.959964*exp.est[i,4])#
b1.upper<-exp.est[i,3]+(1.959964*exp.est[i,4])#
#store coverage parameters#
exp.cp[i,1]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
exp.cp[i,2]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
}#
#################################################################################
#########################Simple Weibull Model ###################################
#################################################################################
#Note this estiamtes the model via hazard rates, a la Stata#
test<-survreg(Surv(ycen, di)~x, dist="weibull")#
summary(test)#
Weibull<- function(est,Y,C,X,data) {					      #
	n=nrow(data)							      					  #
	llik <- matrix(0, nrow=n, ncol = 1)#
	beta<-est[1:length(est)-1]#
	p<-est[length(est)]#
	p<-exp(p)#
	XB<-X%*%beta#
	llik<-C*(log(exp(XB+1/p)*p*((exp(XB+1/p)*Y)^(p-1))*exp(-(exp(XB+1/p)*Y)^p)))+(1-C)*log(exp(-(exp(XB+1/p)*Y)^p))#
	llik<--1*sum(llik)#
	return(llik)#
	}#
#set starting parameters#
est<-rbind(exp.est[i,1],exp.est[i,3],.01)#
#set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
#optimize#
output.Weibull<-try(optim(f=Weibull,  p=est, X=X,Y=Y,C=C, method="BFGS", control=list(maxit=10000),  data=data, hessian=TRUE), TRUE)#
if(class(output.Weibull)=="list"){#
	ifelse(is.positive.definite(output.Weibull$hessian)==TRUE,vcv<-solve(output.Weibull$hessian),vcv<-matrix(data=NA,nrow=3,ncol=3))#
#store betas and ses#
weib.est[i,1]<-output.Weibull$par[1]+1/exp(output.Weibull$par[3])#
coeff<-c(weib.est[i,1],output.Weibull$par[3])#
varcov<-matrix(NA,2,2)#
varcov[1,1]<-vcv[1,1]#
varcov[1,2]<-vcv[1,3]#
varcov[2,1]<-vcv[3,1]#
varcov[2,2]<-vcv[3,3]#
weib.est[i,2]<-deltamethod(~(x1+1/exp(x2)), coeff, varcov, ses=TRUE)#
weib.est[i,3]<-output.Weibull$par[2]#
weib.est[i,4]<-sqrt(vcv[2,2])#
weib.est[i,5]<-exp(output.Weibull$par[3])#
coeff<-c(weib.est[i,5])#
varcov<-matrix(NA,1,1)#
varcov[1,1]<-vcv[3,3]#
weib.est[i,6]<-deltamethod(~(exp(x1)), coeff, varcov, ses=TRUE)#
#store rmse#
weib.rmse[i,1]<-sqrt((tru.est[i,1]-weib.est[i,1])^2)#
weib.rmse[i,2]<-sqrt((tru.est[i,2]-weib.est[i,3])^2)#
weib.rmse[i,3]<-sqrt((tru.est[i,6]-weib.est[i,5])^2)#
#calculate upper and lower 95% CI's#
b0.lower<-weib.est[i,1]-(1.959964*weib.est[i,2])#
b0.upper<-weib.est[i,1]+(1.959964*weib.est[i,2])#
b1.lower<-weib.est[i,3]-(1.959964*weib.est[i,4])#
b1.upper<-weib.est[i,3]+(1.959964*weib.est[i,4])#
p.lower<-weib.est[i,5]-(1.959964*weib.est[i,6])#
p.upper<-weib.est[i,5]+(1.959964*weib.est[i,6])#
#store coverage parameters#
weib.cp[i,1]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
weib.cp[i,2]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
weib.cp[i,3]<-ifelse(tru.est[i,6]>p.lower & tru.est[i,6]<p.upper, 1,0)#
}#
###logit estimates####
dataset<-as.data.frame(data)#
logitcoef1<-glm(di~ z+x, data = dataset, family = "binomial")$coef[1]#
logitcoef2<-glm(di~ z+x, data = dataset, family = "binomial")$coef[2]#
logitcoef3<-glm(di~ z+x, data = dataset, family = "binomial")$coef[3]#
################################################################################
##########################Zombie Exponential Model##############################
################################################################################
#This program estimates the Exponential loglikelihood function returning hazard rate form coefficients#
ZExponential<- function(est,Y,C,X,Z,data) {					      #
	n=nrow(data)							      					  #
	llik <- matrix(0, nrow=n, ncol = 1)#
	gamma<-est[1:ncol(Z)]#
	beta<-est[(ncol(Z)+1):length(est)]#
	XB<-X%*%beta#
	ZG<-Z%*%gamma#
	phi<-1/(1+exp(-ZG))#
	llik<-C*(log(phi*exp(-exp(XB)*Y)+(1-phi)*exp(XB)*exp(-exp(XB)*Y)))+(1-C)*(log(phi)+-exp(XB)*Y)#
	llik<--1*sum(llik)#
	return(llik)#
	}#
#set starting parameters#
est<-rbind(.01,.01,.01,exp.est[i,1],exp.est[i,3])#
#set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
Z<-cbind(1,z,x)#
#optimize#
output.ZExponential<-try(optim(f=ZExponential,  p=est, X=X,Y=Y,C=C,Z=Z, method="BFGS", control=list(maxit=10000),  data=data, hessian=TRUE), TRUE)#
if(class(output.ZExponential)=="list"){#
	ifelse(is.positive.definite(output.ZExponential$hessian)==TRUE,vcv<-solve(output.ZExponential$hessian),vcv<-matrix(data=NA,nrow=5,ncol=5))#
#store betas and ses#
exp.est[i,5]<-output.ZExponential$par[1]#
exp.est[i,6]<-sqrt(vcv[1,1])#
exp.est[i,7]<-output.ZExponential$par[2]#
exp.est[i,8]<-sqrt(vcv[2,2])#
exp.est[i,9]<-output.ZExponential$par[3]#
exp.est[i,10]<-sqrt(vcv[3,3])#
exp.est[i,11]<-output.ZExponential$par[4]#
exp.est[i,12]<-sqrt(vcv[4,4])#
exp.est[i,13]<-output.ZExponential$par[5]#
exp.est[i,14]<-sqrt(vcv[5,5])#
#store rmse#
exp.rmse[i,3]<-sqrt((tru.est[i,3]-exp.est[i,5])^2)#
exp.rmse[i,4]<-sqrt((tru.est[i,4]-exp.est[i,7])^2)#
exp.rmse[i,5]<-sqrt((tru.est[i,5]-exp.est[i,9])^2)#
exp.rmse[i,6]<-sqrt((tru.est[i,1]-exp.est[i,11])^2)#
exp.rmse[i,7]<-sqrt((tru.est[i,2]-exp.est[i,13])^2)#
#calculate upper and lower 95% CI's#
g0.lower<-exp.est[i,5]-(1.959964*exp.est[i,6])#
g0.upper<-exp.est[i,5]+(1.959964*exp.est[i,6])#
g1.lower<-exp.est[i,7]-(1.959964*exp.est[i,8])#
g1.upper<-exp.est[i,7]+(1.959964*exp.est[i,8])#
g2.lower<-exp.est[i,9]-(1.959964*exp.est[i,10])#
g2.upper<-exp.est[i,9]+(1.959964*exp.est[i,10])#
b0.lower<-exp.est[i,11]-(1.959964*exp.est[i,12])#
b0.upper<-exp.est[i,11]+(1.959964*exp.est[i,12])#
b1.lower<-exp.est[i,13]-(1.959964*exp.est[i,14])#
b1.upper<-exp.est[i,13]+(1.959964*exp.est[i,14])#
#store coverage parameters#
exp.cp[i,3]<-ifelse(tru.est[i,3]>g0.lower & tru.est[i,3]<g0.upper, 1,0)#
exp.cp[i,4]<-ifelse(tru.est[i,4]>g1.lower & tru.est[i,4]<g1.upper, 1,0)#
exp.cp[i,5]<-ifelse(tru.est[i,5]>g2.lower & tru.est[i,5]<g2.upper, 1,0)#
exp.cp[i,6]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
exp.cp[i,7]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
}#
######################################################################################
##########################Zombie Weibull Model #######################################
######################################################################################
#This program estimates the Exponential loglikelihood function returning hazard rate form coefficients#
ZWeibull<- function(est,Y,C,X,Z,data) {					      #
	n=nrow(data)							      					  #
	llik <- matrix(0, nrow=n, ncol = 1)#
	gamma<-est[1:ncol(Z)]#
	beta<-est[(ncol(Z)+1):(length(est)-1)]#
	p<-est[length(est)]#
	p<-exp(p)#
	XB<-X%*%beta#
	ZG<-Z%*%gamma#
	phi<-1/(1+exp(-(ZG+1/p)))#
	llik<-C*(log(phi*exp(-(exp(XB+1/p)*Y)^p)+(1-phi)*exp(XB+1/p)*p*((exp(XB+1/p)*Y)^(p-1))*exp(-(exp(XB+1/p)*Y)^p)))+(1-C)*(log(phi)+-(exp(XB+1/p)*Y)^p)#
	llik<--1*sum(llik)#
	return(llik)#
	}#
#set starting parameters#
est<-rbind(.01,.01,.01,output.Weibull$par[1],output.Weibull$par[2],output.Weibull$par[3])#
#set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
Z<-cbind(1,z,x)#
#optimize#
output.ZWeibull<-try(optim(f=ZWeibull,  p=est, X=X,Y=Y,C=C,Z=Z, method="BFGS", control=list(maxit=10000),  data=data, hessian=TRUE), TRUE)#
if(class(output.ZWeibull)=="list"){#
	ifelse(is.positive.definite(output.ZWeibull$hessian)==TRUE,vcv<-solve(output.ZWeibull$hessian),vcv<-matrix(data=NA,nrow=6,ncol=6))#
#store betas and ses#
weib.est[i,7]<-output.ZWeibull$par[1]+1/exp(output.ZWeibull$par[6])#
coeff<-c(weib.est[i,7],output.ZWeibull$par[6])#
varcov<-matrix(NA,2,2)#
varcov[1,1]<-vcv[1,1]#
varcov[1,2]<-vcv[1,6]#
varcov[2,1]<-vcv[6,1]#
varcov[2,2]<-vcv[6,6]#
weib.est[i,8]<-deltamethod(~(x1+1/exp(x2)), coeff, varcov, ses=TRUE)#
weib.est[i,9]<-output.ZWeibull$par[2]#
weib.est[i,10]<-sqrt(vcv[2,2])#
weib.est[i,11]<-output.ZWeibull$par[3]#
weib.est[i,12]<-sqrt(vcv[3,3])#
weib.est[i,13]<-output.ZWeibull$par[4]+1/exp(output.ZWeibull$par[6])#
coeff<-c(weib.est[i,13],output.ZWeibull$par[6])#
varcov<-matrix(NA,2,2)#
varcov[1,1]<-vcv[4,4]#
varcov[1,2]<-vcv[4,6]#
varcov[2,1]<-vcv[6,4]#
varcov[2,2]<-vcv[6,6]#
weib.est[i,14]<-deltamethod(~(x1+1/exp(x2)), coeff, varcov, ses=TRUE)#
weib.est[i,15]<-output.ZWeibull$par[5]#
weib.est[i,16]<-sqrt(vcv[5,5])#
weib.est[i,17]<-exp(output.ZWeibull$par[6])#
coeff<-c(weib.est[i,17])#
varcov<-matrix(NA,1,1)#
varcov[1,1]<-vcv[6,6]#
weib.est[i,18]<-deltamethod(~(exp(x1)), coeff, varcov, ses=TRUE)#
#store rmse#
weib.rmse[i,4]<-sqrt((tru.est[i,3]-weib.est[i,7])^2)#
weib.rmse[i,5]<-sqrt((tru.est[i,4]-weib.est[i,9])^2)#
weib.rmse[i,6]<-sqrt((tru.est[i,5]-weib.est[i,11])^2)#
weib.rmse[i,7]<-sqrt((tru.est[i,1]-weib.est[i,13])^2)#
weib.rmse[i,8]<-sqrt((tru.est[i,2]-weib.est[i,15])^2)#
weib.rmse[i,9]<-sqrt((tru.est[i,6]-weib.est[i,17])^2)#
#calculate upper and lower 95% CI's#
g0.lower<-weib.est[i,7]-(1.959964*weib.est[i,8])#
g0.upper<-weib.est[i,7]+(1.959964*weib.est[i,8])#
g1.lower<-weib.est[i,9]-(1.959964*weib.est[i,10])#
g1.upper<-weib.est[i,9]+(1.959964*weib.est[i,10])#
g2.lower<-weib.est[i,11]-(1.959964*weib.est[i,12])#
g2.upper<-weib.est[i,11]+(1.959964*weib.est[i,12])#
b0.lower<-weib.est[i,13]-(1.959964*weib.est[i,14])#
b0.upper<-weib.est[i,13]+(1.959964*weib.est[i,14])#
b1.lower<-weib.est[i,15]-(1.959964*weib.est[i,16])#
b1.upper<-weib.est[i,15]+(1.959964*weib.est[i,16])#
p.lower<-weib.est[i,17]-(1.959964*weib.est[i,18])#
p.upper<-weib.est[i,17]+(1.959964*weib.est[i,18])#
#store coverage parameters#
weib.cp[i,4]<-ifelse(tru.est[i,3]>g0.lower & tru.est[i,3]<g0.upper, 1,0)#
weib.cp[i,5]<-ifelse(tru.est[i,4]>g1.lower & tru.est[i,4]<g1.upper, 1,0)#
weib.cp[i,6]<-ifelse(tru.est[i,5]>g2.lower & tru.est[i,5]<g2.upper, 1,0)#
weib.cp[i,7]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
weib.cp[i,8]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
weib.cp[i,9]<-ifelse(tru.est[i,6]>p.lower & tru.est[i,6]<p.upper, 1,0)#
}#
################################################################################
######################Bayesian Zombie Exponential Model#########################
################################################################################
#set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
Z<-cbind(1,z,x)#
BayesZExponential = mcmcOF(Y, C, X, Z, N = 3000, burn = 1000, thin = 20,  w = c(1, 1, 1), m = 10, form = "Exponential")#
output.BayesZExponential = list(par = c(summary(mcmc(BayesZExponential$beta))[[1]][,1], summary(mcmc(BayesZExponential$gamma))[[1]][,1]), #
								se = c(summary(mcmc(BayesZExponential$beta))[[1]][,2], summary(mcmc(BayesZExponential$gamma))[[1]][,2]),#
								CI = rbind(summary(mcmc(BayesZExponential$beta))[[2]], summary(mcmc(BayesZExponential$gamma))[[2]]))#
exp.est[i,15]<-output.BayesZExponential$par[1]#
exp.est[i,16]<-output.BayesZExponential$se[1]#
exp.est[i,17]<-output.BayesZExponential$par[2]#
exp.est[i,18]<-output.BayesZExponential$se[2]#
exp.est[i,19]<-output.BayesZExponential$par[3]#
exp.est[i,20]<-output.BayesZExponential$se[3]#
exp.est[i,21]<-output.BayesZExponential$par[4]#
exp.est[i,22]<-output.BayesZExponential$se[4]#
exp.est[i,23]<-output.BayesZExponential$par[5]#
exp.est[i,24]<-output.BayesZExponential$se[5]#
#
#store rmse#
exp.rmse[i,8]<-sqrt((tru.est[i,3]-exp.est[i,15])^2)#
exp.rmse[i,9]<-sqrt((tru.est[i,4]-exp.est[i,17])^2)#
exp.rmse[i,10]<-sqrt((tru.est[i,5]-exp.est[i,19])^2)#
exp.rmse[i,11]<-sqrt((tru.est[i,1]-exp.est[i,21])^2)#
exp.rmse[i,12]<-sqrt((tru.est[i,2]-exp.est[i,23])^2)#
#
#calculate upper and lower 95% CI's#
# b0.lower<-output.BayesZExponential$CI[1,1]#
# b0.upper<-output.BayesZExponential$CI[1,5]#
# b1.lower<-output.BayesZExponential$CI[2,1]#
# b1.upper<-output.BayesZExponential$CI[2,5]#
# g0.lower<-output.BayesZExponential$CI[3,1]#
# g0.upper<-output.BayesZExponential$CI[3,5]#
# g1.lower<-output.BayesZExponential$CI[4,1]#
# g1.upper<-output.BayesZExponential$CI[4,5]#
# g2.lower<-output.BayesZExponential$CI[5,1]#
# g2.upper<-output.BayesZExponential$CI[5,5]#
b0.lower<-exp.est[i,15]-(1.959964*exp.est[i,16])#
b0.upper<-exp.est[i,15]+(1.959964*exp.est[i,16])#
b1.lower<-exp.est[i,17]-(1.959964*exp.est[i,18])#
b1.upper<-exp.est[i,17]+(1.959964*exp.est[i,18])#
g0.lower<-exp.est[i,19]-(1.959964*exp.est[i,20])#
g0.upper<-exp.est[i,19]+(1.959964*exp.est[i,20])#
g1.lower<-exp.est[i,21]-(1.959964*exp.est[i,22])#
g1.upper<-exp.est[i,21]+(1.959964*exp.est[i,22])#
g2.lower<-exp.est[i,23]-(1.959964*exp.est[i,24])#
g2.upper<-exp.est[i,23]+(1.959964*exp.est[i,24])#
#store coverage parameters#
exp.cp[i,8]<-ifelse(tru.est[i,3]>g0.lower & tru.est[i,3]<g0.upper, 1,0)#
exp.cp[i,9]<-ifelse(tru.est[i,4]>g1.lower & tru.est[i,4]<g1.upper, 1,0)#
exp.cp[i,10]<-ifelse(tru.est[i,5]>g2.lower & tru.est[i,5]<g2.upper, 1,0)#
exp.cp[i,11]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
exp.cp[i,12]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
#
################################################################################
########################Bayesian Zombie Weibull Model###########################
################################################################################
#set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
Z<-cbind(1,z,x)#
BayesZWeibull = mcmcOF(Y, C, X, Z, N = 3000, burn = 1000, thin = 20,  w = c(1, 1, 1), m = 10, form = "Weibull")#
output.BayesZWeibull = list(par = c(summary(mcmc(BayesZWeibull$beta))[[1]][,1], summary(mcmc(BayesZWeibull$gamma))[[1]][,1], #
									summary(mcmc(BayesZWeibull$lambda))[[1]][1]), #
								se = c(summary(mcmc(BayesZWeibull$beta))[[1]][,2], summary(mcmc(BayesZWeibull$gamma))[[1]][,2], #
									   summary(mcmc(BayesZWeibull$lambda))[[1]][2]),#
								CI = rbind(summary(mcmc(BayesZWeibull$beta))[[2]], summary(mcmc(BayesZWeibull$gamma))[[2]], #
										summary(mcmc(BayesZWeibull$lambda))[[2]]))#
#
weib.est[i,19]<-output.BayesZWeibull$par[1]#
weib.est[i,20]<-output.BayesZWeibull$se[1]#
weib.est[i,21]<-output.BayesZWeibull$par[2]#
weib.est[i,22]<-output.BayesZWeibull$se[2]#
weib.est[i,23]<-output.BayesZWeibull$par[3]#
weib.est[i,24]<-output.BayesZWeibull$se[3]#
weib.est[i,25]<-output.BayesZWeibull$par[4]#
weib.est[i,26]<-output.BayesZWeibull$se[4]#
weib.est[i,27]<-output.BayesZWeibull$par[5]#
weib.est[i,28]<-output.BayesZWeibull$se[5]#
weib.est[i,29]<-output.BayesZWeibull$par[6]#
weib.est[i,30]<-output.BayesZWeibull$se[6]#
#
#store rmse#
weib.rmse[i,10]<-sqrt((tru.est[i,3]-weib.est[i,19])^2)#
weib.rmse[i,11]<-sqrt((tru.est[i,4]-weib.est[i,21])^2)#
weib.rmse[i,12]<-sqrt((tru.est[i,5]-weib.est[i,23])^2)#
weib.rmse[i,13]<-sqrt((tru.est[i,1]-weib.est[i,25])^2)#
weib.rmse[i,14]<-sqrt((tru.est[i,2]-weib.est[i,27])^2)#
weib.rmse[i,15]<-sqrt((tru.est[i,6]-weib.est[i,29])^2)#
#
#calculate upper and lower 95% CI's#
# b0.lower<-output.BayesZWeibull$CI[1,1]#
# b0.upper<-output.BayesZWeibull$CI[1,5]#
# b1.lower<-output.BayesZWeibull$CI[2,1]#
# b1.upper<-output.BayesZWeibull$CI[2,5]#
# g0.lower<-output.BayesZWeibull$CI[3,1]#
# g0.upper<-output.BayesZWeibull$CI[3,5]#
# g1.lower<-output.BayesZWeibull$CI[4,1]#
# g1.upper<-output.BayesZWeibull$CI[4,5]#
# g2.lower<-output.BayesZWeibull$CI[5,1]#
# g2.upper<-output.BayesZWeibull$CI[5,5]#
# p.lower<-output.BayesZWeibull$CI[6,1]#
# p.upper<-output.BayesZWeibull$CI[6,2]#
g0.lower<-weib.est[i,19]-(1.959964*weib.est[i,20])#
g0.upper<-weib.est[i,19]+(1.959964*weib.est[i,20])#
g1.lower<-weib.est[i,21]-(1.959964*weib.est[i,22])#
g1.upper<-weib.est[i,21]+(1.959964*weib.est[i,22])#
g2.lower<-weib.est[i,23]-(1.959964*weib.est[i,24])#
g2.upper<-weib.est[i,23]+(1.959964*weib.est[i,24])#
b0.lower<-weib.est[i,25]-(1.959964*weib.est[i,26])#
b0.upper<-weib.est[i,25]+(1.959964*weib.est[i,26])#
b1.lower<-weib.est[i,27]-(1.959964*weib.est[i,28])#
b1.upper<-weib.est[i,27]+(1.959964*weib.est[i,28])#
p.lower<-weib.est[i,29]-(1.959964*weib.est[i,30])#
p.upper<-weib.est[i,29]+(1.959964*weib.est[i,30])#
#store coverage parameters#
weib.cp[i,10]<-ifelse(tru.est[i,3]>g0.lower & tru.est[i,3]<g0.upper, 1,0)#
weib.cp[i,11]<-ifelse(tru.est[i,4]>g1.lower & tru.est[i,4]<g1.upper, 1,0)#
weib.cp[i,12]<-ifelse(tru.est[i,5]>g2.lower & tru.est[i,5]<g2.upper, 1,0)#
weib.cp[i,13]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
weib.cp[i,14]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
weib.cp[i,15]<-ifelse(tru.est[i,6]>p.lower & tru.est[i,6]<p.upper, 1,0)#
}#
#combine matrices and label variables#
main.data<-cbind(tru.est, cox.est, exp.est, weib.est, cox.rmse, exp.rmse, weib.rmse, cox.cp, exp.cp, weib.cp)#
colnames(main.data)<-c("true.x0","true.x1","true.z0","true.z1","true.z2","true.p","cen.lat","cen.obs",#
	"cox.x1","cox.x1.se",#
	"exp.x0","exp.x0.se","exp.x1","exp.x1.se",#
	"zexp.z0","zexp.z0.se","zexp.z1","zexp.z1.se","zexp.z2","zexp.z2.se","zexp.x0","zexp.x0.se","zexp.x1","zexp.x1.se",#
	"bzexp.x0","zexp.x0.se","bzexp.x1","bzexp.x1.se","bzexp.z0","bzexp.z0.se","bzexp.z1","bzexp.z1.se","bzexp.z2","bzexp.z2.se",#
	"wei.x0","wei.x0.se","wei.x1","wei.x1.se","wei.p","wei.p.se",#
	"zwei.z0","zwei.z0.se","zwei.z1","zwei.z1.se","zwei.z2","zwei.z2.se","zwei.x0","zwei.x0.se","zwei.x1","zwei.x1.se","zwei.p","zwei.p.se",#
	"bzwei.x0","bzwei.x0.se","bzwei.x1","bzwei.x1.se","bzwei.z0","bzwei.z0.se","bzwei.z1","bzwei.z1.se","bzwei.z2","bzwei.z2.se","bzwei.p","bzwei.p.se",#
	"cox.x1.rmse",#
	"exp.x0.rmse","exp.x1.rmse","zexp.z0.rmse","zexp.z1.rmse","zexp.z2.rmse","zexp.x0.rmse","zexp.x1.rmse","bzexp.x0.rmse","bzexp.x1.rmse","bzexp.z0.rmse","bzexp.z1.rmse","bzexp.z2.rmse",#
	"wei.x0.rmse","wei.x1.rmse","wei.p.rmse","zwei.z0.rmse","zwei.z1.rmse","zwei.z2.rmse",#
	"zwei.x0.rmse","zwei.x1.rmse","zwei.p.rmse", "bzwei.x0.rmse","bzwei.x1.rmse","bzwei.z0.rmse","bzwei.z1.rmse","bzwei.z2.rmse","bzwei.p.rmse",#
	"cox.x1.cp","exp.x0.cp","exp.x1.cp","zexp.z0.cp","zexp.z1.cp","zexp.z2.cp","zexp.x0.cp","zexp.x1.cp","bzexp.x0.cp","bzexp.x1.cp","bzexp.z0.cp","bzexp.z1.cp","bzexp.z2.cp",#
	"wei.x0.cp","wei.x1.cp","wei.p.cp",#
	"zwei.z0.cp","zwei.z1.cp","zwei.z2.cp","zwei.x0.cp","zwei.x1.cp","zwei.p.cp", "bzwei.x0.cp","bzwei.x1.cp","bzwei.z0.cp","bzwei.z1.cp","bzwei.z2.cp","bzwei.p.cp")#
#save dataset#
main.data2<-as.data.frame(main.data)#
write.dta(main.data2,"main.data2.dta", )
#clear memory#
rm( list=ls() )#
#load necessary libraries 						                                 #
library(foreign)#
library(Zelig)#
library(car)#
library(MASS)#
library(VGAM)#
library(plotrix)#
library(pscl)#
library(survival)#
library(msm)#
library(verification)#
library(corpcor)#
library(Design)#
library(mvtnorm)#
library(MCMCpack)#
library(BayesOFsurv)#
#set working directory#
setwd("/Users/bomin8319/Desktop/BayesOFsurv/coding material/Monte Carlos/Mixture DGP/")#
###########################################################################
###########################################################################
############################Monte Carlo####################################
###########################################################################
#set seed#
set.seed(3)   #
#set the number of observations#
n<-100#
#set the number of simulations, and create matrices to store the results#
nsims<-1000#
#history matrix for true estimates#
tru.est<-matrix(NA,nrow=nsims,ncol=8)#
#history matrix for cox estimates#
cox.est<-matrix(NA,nrow=nsims,ncol=2)#
#history matrix for exp estimates#
exp.est<-matrix(NA,nrow=nsims,ncol=24)#
#history matrix for weibull estimates#
weib.est<-matrix(NA,nrow=nsims,ncol=30)#
#history matrix for cox RMSE#
cox.rmse<-matrix(NA,nrow=nsims,ncol=1)#
#history matrix for exp RMSE#
exp.rmse<-matrix(NA,nrow=nsims,ncol=12)#
#history matrix for exp RMSE#
weib.rmse<-matrix(NA,nrow=nsims,ncol=15)#
#history matrix for cox CP#
cox.cp<-matrix(NA,nrow=nsims,ncol=1)#
#history matrix for exp CP#
exp.cp<-matrix(NA,nrow=nsims,ncol=12)#
#history matrix for exp CP#
weib.cp<-matrix(NA,nrow=nsims,ncol=15)#
#create covariates#
x<-runif(n, min=-2.5, max=12)#
z<-log(runif(n, min=1, max=100))#
#create a dependent variable, begin the simmulations#
for(i in 1:nsims){#
#Assign parameter values#
tru.est[i,1]<-1#
tru.est[i,2]<-3.5#
tru.est[i,3]<--2#
tru.est[i,4]<-2#
tru.est[i,5]<-3#
tru.est[i,6]<-1#
myrates <- exp(tru.est[i,1]+(tru.est[i,2]*x)) #
y <- rexp(n, rate = myrates) # generates the r.v.#
cen <- rexp(n, rate = 1 )#
ycen <- pmin(y, cen)#
di <- as.numeric(y <= cen)#
tru.est[i,7]<-table(di)[1]#
#create parameters for ZG#
phi<-1/(1+exp(-(tru.est[i,3]+tru.est[i,4]*z+tru.est[i,5]*x)))#
print(mean(phi))#
yzero<-matrix(1,n,1)#
error<--1*rlogis(n)#
flag<-error<qlogis(phi)#
yzero[flag]<-error[flag]#
flag<-yzero==1#
di[flag]<-ifelse(di[flag]==0,yzero[flag],di[flag])#
tru.est[i,8]<-table(di)[1]#
data<-cbind(ycen,di,x,z)#
######################################################################################
###################################COX Model##########################################
######################################################################################
#store estimate and se#
cox.est[i,1]<-summary(coxph(Surv(ycen, di)~x,coxph.control(iter.max = 10000)))$coef[1]#
cox.est[i,2]<-summary(coxph(Surv(ycen, di)~x,coxph.control(iter.max = 10000)))$coef[3]#
#store rmse#
cox.rmse[i,1]<-sqrt((tru.est[i,2]-cox.est[i,1])^2)#
#calculate upper and lower 95% CI's#
b1.lower<-cox.est[i,1]-(1.959964*cox.est[i,2])#
b1.upper<-cox.est[i,1]+(1.959964*cox.est[i,2])#
#store coverage parameters#
cox.cp[i,1]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
##############################################################################
########################Simple Exponential Model##############################
##############################################################################
Exponential<- function(est,Y,C,X,data) {					      #
	n=nrow(data)							      					  #
	llik <- matrix(0, nrow=n, ncol = 1)#
	beta<-est[1:length(est)]#
	XB<-X%*%beta#
	llik<-C*(XB-exp(XB)*Y)+(1-C)*(-exp(XB)*Y)#
	llik<--1*sum(llik)#
	return(llik)#
	}#
#set starting parameters#
est<-rbind(.01,.01)#
#set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
#optimize#
output.Exponential<-try(optim(f=Exponential,  p=est, X=X,Y=Y,C=C, method="BFGS", control=list(maxit=10000),  data=data, hessian=TRUE), TRUE)#
if(class(output.Exponential)=="list"){#
	ifelse(is.positive.definite(output.Exponential$hessian)==TRUE,vcv<-solve(output.Exponential$hessian),vcv<-matrix(data=NA,nrow=2,ncol=2))#
#store betas and ses#
exp.est[i,1]<-output.Exponential$par[1]#
exp.est[i,2]<-sqrt(vcv[1,1])#
exp.est[i,3]<-output.Exponential$par[2]#
exp.est[i,4]<-sqrt(vcv[2,2])#
#store rmse#
exp.rmse[i,1]<-sqrt((tru.est[i,1]-exp.est[i,1])^2)#
exp.rmse[i,2]<-sqrt((tru.est[i,2]-exp.est[i,3])^2)#
#calculate upper and lower 95% CI's#
b0.lower<-exp.est[i,1]-(1.959964*exp.est[i,2])#
b0.upper<-exp.est[i,1]+(1.959964*exp.est[i,2])#
b1.lower<-exp.est[i,3]-(1.959964*exp.est[i,4])#
b1.upper<-exp.est[i,3]+(1.959964*exp.est[i,4])#
#store coverage parameters#
exp.cp[i,1]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
exp.cp[i,2]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
}#
#################################################################################
#########################Simple Weibull Model ###################################
#################################################################################
#Note this estiamtes the model via hazard rates, a la Stata#
test<-survreg(Surv(ycen, di)~x, dist="weibull")#
summary(test)#
Weibull<- function(est,Y,C,X,data) {					      #
	n=nrow(data)							      					  #
	llik <- matrix(0, nrow=n, ncol = 1)#
	beta<-est[1:length(est)-1]#
	p<-est[length(est)]#
	p<-exp(p)#
	XB<-X%*%beta#
	llik<-C*(log(exp(XB+1/p)*p*((exp(XB+1/p)*Y)^(p-1))*exp(-(exp(XB+1/p)*Y)^p)))+(1-C)*log(exp(-(exp(XB+1/p)*Y)^p))#
	llik<--1*sum(llik)#
	return(llik)#
	}#
#set starting parameters#
est<-rbind(exp.est[i,1],exp.est[i,3],.01)#
#set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
#optimize#
output.Weibull<-try(optim(f=Weibull,  p=est, X=X,Y=Y,C=C, method="BFGS", control=list(maxit=10000),  data=data, hessian=TRUE), TRUE)#
if(class(output.Weibull)=="list"){#
	ifelse(is.positive.definite(output.Weibull$hessian)==TRUE,vcv<-solve(output.Weibull$hessian),vcv<-matrix(data=NA,nrow=3,ncol=3))#
#store betas and ses#
weib.est[i,1]<-output.Weibull$par[1]+1/exp(output.Weibull$par[3])#
coeff<-c(weib.est[i,1],output.Weibull$par[3])#
varcov<-matrix(NA,2,2)#
varcov[1,1]<-vcv[1,1]#
varcov[1,2]<-vcv[1,3]#
varcov[2,1]<-vcv[3,1]#
varcov[2,2]<-vcv[3,3]#
weib.est[i,2]<-deltamethod(~(x1+1/exp(x2)), coeff, varcov, ses=TRUE)#
weib.est[i,3]<-output.Weibull$par[2]#
weib.est[i,4]<-sqrt(vcv[2,2])#
weib.est[i,5]<-exp(output.Weibull$par[3])#
coeff<-c(weib.est[i,5])#
varcov<-matrix(NA,1,1)#
varcov[1,1]<-vcv[3,3]#
weib.est[i,6]<-deltamethod(~(exp(x1)), coeff, varcov, ses=TRUE)#
#store rmse#
weib.rmse[i,1]<-sqrt((tru.est[i,1]-weib.est[i,1])^2)#
weib.rmse[i,2]<-sqrt((tru.est[i,2]-weib.est[i,3])^2)#
weib.rmse[i,3]<-sqrt((tru.est[i,6]-weib.est[i,5])^2)#
#calculate upper and lower 95% CI's#
b0.lower<-weib.est[i,1]-(1.959964*weib.est[i,2])#
b0.upper<-weib.est[i,1]+(1.959964*weib.est[i,2])#
b1.lower<-weib.est[i,3]-(1.959964*weib.est[i,4])#
b1.upper<-weib.est[i,3]+(1.959964*weib.est[i,4])#
p.lower<-weib.est[i,5]-(1.959964*weib.est[i,6])#
p.upper<-weib.est[i,5]+(1.959964*weib.est[i,6])#
#store coverage parameters#
weib.cp[i,1]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
weib.cp[i,2]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
weib.cp[i,3]<-ifelse(tru.est[i,6]>p.lower & tru.est[i,6]<p.upper, 1,0)#
}#
###logit estimates####
dataset<-as.data.frame(data)#
logitcoef1<-glm(di~ z+x, data = dataset, family = "binomial")$coef[1]#
logitcoef2<-glm(di~ z+x, data = dataset, family = "binomial")$coef[2]#
logitcoef3<-glm(di~ z+x, data = dataset, family = "binomial")$coef[3]#
################################################################################
##########################Zombie Exponential Model##############################
################################################################################
#This program estimates the Exponential loglikelihood function returning hazard rate form coefficients#
ZExponential<- function(est,Y,C,X,Z,data) {					      #
	n=nrow(data)							      					  #
	llik <- matrix(0, nrow=n, ncol = 1)#
	gamma<-est[1:ncol(Z)]#
	beta<-est[(ncol(Z)+1):length(est)]#
	XB<-X%*%beta#
	ZG<-Z%*%gamma#
	phi<-1/(1+exp(-ZG))#
	llik<-C*(log(phi*exp(-exp(XB)*Y)+(1-phi)*exp(XB)*exp(-exp(XB)*Y)))+(1-C)*(log(phi)+-exp(XB)*Y)#
	llik<--1*sum(llik)#
	return(llik)#
	}#
#set starting parameters#
est<-rbind(.01,.01,.01,exp.est[i,1],exp.est[i,3])#
#set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
Z<-cbind(1,z,x)#
#optimize#
output.ZExponential<-try(optim(f=ZExponential,  p=est, X=X,Y=Y,C=C,Z=Z, method="BFGS", control=list(maxit=10000),  data=data, hessian=TRUE), TRUE)#
if(class(output.ZExponential)=="list"){#
	ifelse(is.positive.definite(output.ZExponential$hessian)==TRUE,vcv<-solve(output.ZExponential$hessian),vcv<-matrix(data=NA,nrow=5,ncol=5))#
#store betas and ses#
exp.est[i,5]<-output.ZExponential$par[1]#
exp.est[i,6]<-sqrt(vcv[1,1])#
exp.est[i,7]<-output.ZExponential$par[2]#
exp.est[i,8]<-sqrt(vcv[2,2])#
exp.est[i,9]<-output.ZExponential$par[3]#
exp.est[i,10]<-sqrt(vcv[3,3])#
exp.est[i,11]<-output.ZExponential$par[4]#
exp.est[i,12]<-sqrt(vcv[4,4])#
exp.est[i,13]<-output.ZExponential$par[5]#
exp.est[i,14]<-sqrt(vcv[5,5])#
#store rmse#
exp.rmse[i,3]<-sqrt((tru.est[i,3]-exp.est[i,5])^2)#
exp.rmse[i,4]<-sqrt((tru.est[i,4]-exp.est[i,7])^2)#
exp.rmse[i,5]<-sqrt((tru.est[i,5]-exp.est[i,9])^2)#
exp.rmse[i,6]<-sqrt((tru.est[i,1]-exp.est[i,11])^2)#
exp.rmse[i,7]<-sqrt((tru.est[i,2]-exp.est[i,13])^2)#
#calculate upper and lower 95% CI's#
g0.lower<-exp.est[i,5]-(1.959964*exp.est[i,6])#
g0.upper<-exp.est[i,5]+(1.959964*exp.est[i,6])#
g1.lower<-exp.est[i,7]-(1.959964*exp.est[i,8])#
g1.upper<-exp.est[i,7]+(1.959964*exp.est[i,8])#
g2.lower<-exp.est[i,9]-(1.959964*exp.est[i,10])#
g2.upper<-exp.est[i,9]+(1.959964*exp.est[i,10])#
b0.lower<-exp.est[i,11]-(1.959964*exp.est[i,12])#
b0.upper<-exp.est[i,11]+(1.959964*exp.est[i,12])#
b1.lower<-exp.est[i,13]-(1.959964*exp.est[i,14])#
b1.upper<-exp.est[i,13]+(1.959964*exp.est[i,14])#
#store coverage parameters#
exp.cp[i,3]<-ifelse(tru.est[i,3]>g0.lower & tru.est[i,3]<g0.upper, 1,0)#
exp.cp[i,4]<-ifelse(tru.est[i,4]>g1.lower & tru.est[i,4]<g1.upper, 1,0)#
exp.cp[i,5]<-ifelse(tru.est[i,5]>g2.lower & tru.est[i,5]<g2.upper, 1,0)#
exp.cp[i,6]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
exp.cp[i,7]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
}#
######################################################################################
##########################Zombie Weibull Model #######################################
######################################################################################
#This program estimates the Exponential loglikelihood function returning hazard rate form coefficients#
ZWeibull<- function(est,Y,C,X,Z,data) {					      #
	n=nrow(data)							      					  #
	llik <- matrix(0, nrow=n, ncol = 1)#
	gamma<-est[1:ncol(Z)]#
	beta<-est[(ncol(Z)+1):(length(est)-1)]#
	p<-est[length(est)]#
	p<-exp(p)#
	XB<-X%*%beta#
	ZG<-Z%*%gamma#
	phi<-1/(1+exp(-(ZG+1/p)))#
	llik<-C*(log(phi*exp(-(exp(XB+1/p)*Y)^p)+(1-phi)*exp(XB+1/p)*p*((exp(XB+1/p)*Y)^(p-1))*exp(-(exp(XB+1/p)*Y)^p)))+(1-C)*(log(phi)+-(exp(XB+1/p)*Y)^p)#
	llik<--1*sum(llik)#
	return(llik)#
	}#
#set starting parameters#
est<-rbind(.01,.01,.01,output.Weibull$par[1],output.Weibull$par[2],output.Weibull$par[3])#
#set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
Z<-cbind(1,z,x)#
#optimize#
output.ZWeibull<-try(optim(f=ZWeibull,  p=est, X=X,Y=Y,C=C,Z=Z, method="BFGS", control=list(maxit=10000),  data=data, hessian=TRUE), TRUE)#
if(class(output.ZWeibull)=="list"){#
	ifelse(is.positive.definite(output.ZWeibull$hessian)==TRUE,vcv<-solve(output.ZWeibull$hessian),vcv<-matrix(data=NA,nrow=6,ncol=6))#
#store betas and ses#
weib.est[i,7]<-output.ZWeibull$par[1]+1/exp(output.ZWeibull$par[6])#
coeff<-c(weib.est[i,7],output.ZWeibull$par[6])#
varcov<-matrix(NA,2,2)#
varcov[1,1]<-vcv[1,1]#
varcov[1,2]<-vcv[1,6]#
varcov[2,1]<-vcv[6,1]#
varcov[2,2]<-vcv[6,6]#
weib.est[i,8]<-deltamethod(~(x1+1/exp(x2)), coeff, varcov, ses=TRUE)#
weib.est[i,9]<-output.ZWeibull$par[2]#
weib.est[i,10]<-sqrt(vcv[2,2])#
weib.est[i,11]<-output.ZWeibull$par[3]#
weib.est[i,12]<-sqrt(vcv[3,3])#
weib.est[i,13]<-output.ZWeibull$par[4]+1/exp(output.ZWeibull$par[6])#
coeff<-c(weib.est[i,13],output.ZWeibull$par[6])#
varcov<-matrix(NA,2,2)#
varcov[1,1]<-vcv[4,4]#
varcov[1,2]<-vcv[4,6]#
varcov[2,1]<-vcv[6,4]#
varcov[2,2]<-vcv[6,6]#
weib.est[i,14]<-deltamethod(~(x1+1/exp(x2)), coeff, varcov, ses=TRUE)#
weib.est[i,15]<-output.ZWeibull$par[5]#
weib.est[i,16]<-sqrt(vcv[5,5])#
weib.est[i,17]<-exp(output.ZWeibull$par[6])#
coeff<-c(weib.est[i,17])#
varcov<-matrix(NA,1,1)#
varcov[1,1]<-vcv[6,6]#
weib.est[i,18]<-deltamethod(~(exp(x1)), coeff, varcov, ses=TRUE)#
#store rmse#
weib.rmse[i,4]<-sqrt((tru.est[i,3]-weib.est[i,7])^2)#
weib.rmse[i,5]<-sqrt((tru.est[i,4]-weib.est[i,9])^2)#
weib.rmse[i,6]<-sqrt((tru.est[i,5]-weib.est[i,11])^2)#
weib.rmse[i,7]<-sqrt((tru.est[i,1]-weib.est[i,13])^2)#
weib.rmse[i,8]<-sqrt((tru.est[i,2]-weib.est[i,15])^2)#
weib.rmse[i,9]<-sqrt((tru.est[i,6]-weib.est[i,17])^2)#
#calculate upper and lower 95% CI's#
g0.lower<-weib.est[i,7]-(1.959964*weib.est[i,8])#
g0.upper<-weib.est[i,7]+(1.959964*weib.est[i,8])#
g1.lower<-weib.est[i,9]-(1.959964*weib.est[i,10])#
g1.upper<-weib.est[i,9]+(1.959964*weib.est[i,10])#
g2.lower<-weib.est[i,11]-(1.959964*weib.est[i,12])#
g2.upper<-weib.est[i,11]+(1.959964*weib.est[i,12])#
b0.lower<-weib.est[i,13]-(1.959964*weib.est[i,14])#
b0.upper<-weib.est[i,13]+(1.959964*weib.est[i,14])#
b1.lower<-weib.est[i,15]-(1.959964*weib.est[i,16])#
b1.upper<-weib.est[i,15]+(1.959964*weib.est[i,16])#
p.lower<-weib.est[i,17]-(1.959964*weib.est[i,18])#
p.upper<-weib.est[i,17]+(1.959964*weib.est[i,18])#
#store coverage parameters#
weib.cp[i,4]<-ifelse(tru.est[i,3]>g0.lower & tru.est[i,3]<g0.upper, 1,0)#
weib.cp[i,5]<-ifelse(tru.est[i,4]>g1.lower & tru.est[i,4]<g1.upper, 1,0)#
weib.cp[i,6]<-ifelse(tru.est[i,5]>g2.lower & tru.est[i,5]<g2.upper, 1,0)#
weib.cp[i,7]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
weib.cp[i,8]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
weib.cp[i,9]<-ifelse(tru.est[i,6]>p.lower & tru.est[i,6]<p.upper, 1,0)#
}#
################################################################################
######################Bayesian Zombie Exponential Model#########################
################################################################################
#set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
Z<-cbind(1,z,x)#
BayesZExponential = mcmcOF(Y, C, X, Z, N = 3000, burn = 1000, thin = 20,  w = c(1, 1, 1), m = 10, form = "Exponential")#
output.BayesZExponential = list(par = c(summary(mcmc(BayesZExponential$beta))[[1]][,1], summary(mcmc(BayesZExponential$gamma))[[1]][,1]), #
								se = c(summary(mcmc(BayesZExponential$beta))[[1]][,2], summary(mcmc(BayesZExponential$gamma))[[1]][,2]),#
								CI = rbind(summary(mcmc(BayesZExponential$beta))[[2]], summary(mcmc(BayesZExponential$gamma))[[2]]))#
exp.est[i,15]<-output.BayesZExponential$par[1]#
exp.est[i,16]<-output.BayesZExponential$se[1]#
exp.est[i,17]<-output.BayesZExponential$par[2]#
exp.est[i,18]<-output.BayesZExponential$se[2]#
exp.est[i,19]<-output.BayesZExponential$par[3]#
exp.est[i,20]<-output.BayesZExponential$se[3]#
exp.est[i,21]<-output.BayesZExponential$par[4]#
exp.est[i,22]<-output.BayesZExponential$se[4]#
exp.est[i,23]<-output.BayesZExponential$par[5]#
exp.est[i,24]<-output.BayesZExponential$se[5]#
#
#store rmse#
exp.rmse[i,8]<-sqrt((tru.est[i,3]-exp.est[i,15])^2)#
exp.rmse[i,9]<-sqrt((tru.est[i,4]-exp.est[i,17])^2)#
exp.rmse[i,10]<-sqrt((tru.est[i,5]-exp.est[i,19])^2)#
exp.rmse[i,11]<-sqrt((tru.est[i,1]-exp.est[i,21])^2)#
exp.rmse[i,12]<-sqrt((tru.est[i,2]-exp.est[i,23])^2)#
#
#calculate upper and lower 95% CI's#
# b0.lower<-output.BayesZExponential$CI[1,1]#
# b0.upper<-output.BayesZExponential$CI[1,5]#
# b1.lower<-output.BayesZExponential$CI[2,1]#
# b1.upper<-output.BayesZExponential$CI[2,5]#
# g0.lower<-output.BayesZExponential$CI[3,1]#
# g0.upper<-output.BayesZExponential$CI[3,5]#
# g1.lower<-output.BayesZExponential$CI[4,1]#
# g1.upper<-output.BayesZExponential$CI[4,5]#
# g2.lower<-output.BayesZExponential$CI[5,1]#
# g2.upper<-output.BayesZExponential$CI[5,5]#
b0.lower<-exp.est[i,15]-(1.959964*exp.est[i,16])#
b0.upper<-exp.est[i,15]+(1.959964*exp.est[i,16])#
b1.lower<-exp.est[i,17]-(1.959964*exp.est[i,18])#
b1.upper<-exp.est[i,17]+(1.959964*exp.est[i,18])#
g0.lower<-exp.est[i,19]-(1.959964*exp.est[i,20])#
g0.upper<-exp.est[i,19]+(1.959964*exp.est[i,20])#
g1.lower<-exp.est[i,21]-(1.959964*exp.est[i,22])#
g1.upper<-exp.est[i,21]+(1.959964*exp.est[i,22])#
g2.lower<-exp.est[i,23]-(1.959964*exp.est[i,24])#
g2.upper<-exp.est[i,23]+(1.959964*exp.est[i,24])#
#store coverage parameters#
exp.cp[i,8]<-ifelse(tru.est[i,3]>g0.lower & tru.est[i,3]<g0.upper, 1,0)#
exp.cp[i,9]<-ifelse(tru.est[i,4]>g1.lower & tru.est[i,4]<g1.upper, 1,0)#
exp.cp[i,10]<-ifelse(tru.est[i,5]>g2.lower & tru.est[i,5]<g2.upper, 1,0)#
exp.cp[i,11]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
exp.cp[i,12]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
#
################################################################################
########################Bayesian Zombie Weibull Model###########################
################################################################################
#set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
Z<-cbind(1,z,x)#
BayesZWeibull = mcmcOF(Y, C, X, Z, N = 3000, burn = 1000, thin = 20,  w = c(1, 1, 1), m = 10, form = "Weibull")#
output.BayesZWeibull = list(par = c(summary(mcmc(BayesZWeibull$beta))[[1]][,1], summary(mcmc(BayesZWeibull$gamma))[[1]][,1], #
									summary(mcmc(BayesZWeibull$lambda))[[1]][1]), #
								se = c(summary(mcmc(BayesZWeibull$beta))[[1]][,2], summary(mcmc(BayesZWeibull$gamma))[[1]][,2], #
									   summary(mcmc(BayesZWeibull$lambda))[[1]][2]),#
								CI = rbind(summary(mcmc(BayesZWeibull$beta))[[2]], summary(mcmc(BayesZWeibull$gamma))[[2]], #
										summary(mcmc(BayesZWeibull$lambda))[[2]]))#
#
weib.est[i,19]<-output.BayesZWeibull$par[1]#
weib.est[i,20]<-output.BayesZWeibull$se[1]#
weib.est[i,21]<-output.BayesZWeibull$par[2]#
weib.est[i,22]<-output.BayesZWeibull$se[2]#
weib.est[i,23]<-output.BayesZWeibull$par[3]#
weib.est[i,24]<-output.BayesZWeibull$se[3]#
weib.est[i,25]<-output.BayesZWeibull$par[4]#
weib.est[i,26]<-output.BayesZWeibull$se[4]#
weib.est[i,27]<-output.BayesZWeibull$par[5]#
weib.est[i,28]<-output.BayesZWeibull$se[5]#
weib.est[i,29]<-output.BayesZWeibull$par[6]#
weib.est[i,30]<-output.BayesZWeibull$se[6]#
#
#store rmse#
weib.rmse[i,10]<-sqrt((tru.est[i,3]-weib.est[i,19])^2)#
weib.rmse[i,11]<-sqrt((tru.est[i,4]-weib.est[i,21])^2)#
weib.rmse[i,12]<-sqrt((tru.est[i,5]-weib.est[i,23])^2)#
weib.rmse[i,13]<-sqrt((tru.est[i,1]-weib.est[i,25])^2)#
weib.rmse[i,14]<-sqrt((tru.est[i,2]-weib.est[i,27])^2)#
weib.rmse[i,15]<-sqrt((tru.est[i,6]-weib.est[i,29])^2)#
#
#calculate upper and lower 95% CI's#
# b0.lower<-output.BayesZWeibull$CI[1,1]#
# b0.upper<-output.BayesZWeibull$CI[1,5]#
# b1.lower<-output.BayesZWeibull$CI[2,1]#
# b1.upper<-output.BayesZWeibull$CI[2,5]#
# g0.lower<-output.BayesZWeibull$CI[3,1]#
# g0.upper<-output.BayesZWeibull$CI[3,5]#
# g1.lower<-output.BayesZWeibull$CI[4,1]#
# g1.upper<-output.BayesZWeibull$CI[4,5]#
# g2.lower<-output.BayesZWeibull$CI[5,1]#
# g2.upper<-output.BayesZWeibull$CI[5,5]#
# p.lower<-output.BayesZWeibull$CI[6,1]#
# p.upper<-output.BayesZWeibull$CI[6,2]#
g0.lower<-weib.est[i,19]-(1.959964*weib.est[i,20])#
g0.upper<-weib.est[i,19]+(1.959964*weib.est[i,20])#
g1.lower<-weib.est[i,21]-(1.959964*weib.est[i,22])#
g1.upper<-weib.est[i,21]+(1.959964*weib.est[i,22])#
g2.lower<-weib.est[i,23]-(1.959964*weib.est[i,24])#
g2.upper<-weib.est[i,23]+(1.959964*weib.est[i,24])#
b0.lower<-weib.est[i,25]-(1.959964*weib.est[i,26])#
b0.upper<-weib.est[i,25]+(1.959964*weib.est[i,26])#
b1.lower<-weib.est[i,27]-(1.959964*weib.est[i,28])#
b1.upper<-weib.est[i,27]+(1.959964*weib.est[i,28])#
p.lower<-weib.est[i,29]-(1.959964*weib.est[i,30])#
p.upper<-weib.est[i,29]+(1.959964*weib.est[i,30])#
#store coverage parameters#
weib.cp[i,10]<-ifelse(tru.est[i,3]>g0.lower & tru.est[i,3]<g0.upper, 1,0)#
weib.cp[i,11]<-ifelse(tru.est[i,4]>g1.lower & tru.est[i,4]<g1.upper, 1,0)#
weib.cp[i,12]<-ifelse(tru.est[i,5]>g2.lower & tru.est[i,5]<g2.upper, 1,0)#
weib.cp[i,13]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
weib.cp[i,14]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
weib.cp[i,15]<-ifelse(tru.est[i,6]>p.lower & tru.est[i,6]<p.upper, 1,0)#
}#
#combine matrices and label variables#
main.data<-cbind(tru.est, cox.est, exp.est, weib.est, cox.rmse, exp.rmse, weib.rmse, cox.cp, exp.cp, weib.cp)#
colnames(main.data)<-c("true.x0","true.x1","true.z0","true.z1","true.z2","true.p","cen.lat","cen.obs",#
	"cox.x1","cox.x1.se",#
	"exp.x0","exp.x0.se","exp.x1","exp.x1.se",#
	"zexp.z0","zexp.z0.se","zexp.z1","zexp.z1.se","zexp.z2","zexp.z2.se","zexp.x0","zexp.x0.se","zexp.x1","zexp.x1.se",#
	"bzexp.x0","zexp.x0.se","bzexp.x1","bzexp.x1.se","bzexp.z0","bzexp.z0.se","bzexp.z1","bzexp.z1.se","bzexp.z2","bzexp.z2.se",#
	"wei.x0","wei.x0.se","wei.x1","wei.x1.se","wei.p","wei.p.se",#
	"zwei.z0","zwei.z0.se","zwei.z1","zwei.z1.se","zwei.z2","zwei.z2.se","zwei.x0","zwei.x0.se","zwei.x1","zwei.x1.se","zwei.p","zwei.p.se",#
	"bzwei.x0","bzwei.x0.se","bzwei.x1","bzwei.x1.se","bzwei.z0","bzwei.z0.se","bzwei.z1","bzwei.z1.se","bzwei.z2","bzwei.z2.se","bzwei.p","bzwei.p.se",#
	"cox.x1.rmse",#
	"exp.x0.rmse","exp.x1.rmse","zexp.z0.rmse","zexp.z1.rmse","zexp.z2.rmse","zexp.x0.rmse","zexp.x1.rmse","bzexp.x0.rmse","bzexp.x1.rmse","bzexp.z0.rmse","bzexp.z1.rmse","bzexp.z2.rmse",#
	"wei.x0.rmse","wei.x1.rmse","wei.p.rmse","zwei.z0.rmse","zwei.z1.rmse","zwei.z2.rmse",#
	"zwei.x0.rmse","zwei.x1.rmse","zwei.p.rmse", "bzwei.x0.rmse","bzwei.x1.rmse","bzwei.z0.rmse","bzwei.z1.rmse","bzwei.z2.rmse","bzwei.p.rmse",#
	"cox.x1.cp","exp.x0.cp","exp.x1.cp","zexp.z0.cp","zexp.z1.cp","zexp.z2.cp","zexp.x0.cp","zexp.x1.cp","bzexp.x0.cp","bzexp.x1.cp","bzexp.z0.cp","bzexp.z1.cp","bzexp.z2.cp",#
	"wei.x0.cp","wei.x1.cp","wei.p.cp",#
	"zwei.z0.cp","zwei.z1.cp","zwei.z2.cp","zwei.x0.cp","zwei.x1.cp","zwei.p.cp", "bzwei.x0.cp","bzwei.x1.cp","bzwei.z0.cp","bzwei.z1.cp","bzwei.z2.cp","bzwei.p.cp")#
#save dataset#
main.data2<-as.data.frame(main.data)#
write.dta(main.data2,"main.data2.dta", )
warnings()
main.data2
main.data2 = main.data2[1:391,]
colMeans(main.data2, na.rm = TRUE)
write.dta(main.data2,"main.data2.dta", )
