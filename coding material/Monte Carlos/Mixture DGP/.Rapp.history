1/(1+exp(delta+lambda[1]))
prod(exp(-(delta+lambdai))/(exp(-(delta+lambdai))+1))
lambdai = lambda[i]
lambdai = lambda[1]
lambdai = lambda[i]
prod(exp(-(delta+lambdai))/(exp(-(delta+lambdai))+1))
exp(-(delta+lambdai))/(exp(-(delta+lambdai))+1)
1/prod(exp(-(delta+lambdai))/(exp(-(delta+lambdai))+1))
lambdai = lambda
exp(-(delta+lambdai))/(exp(-(delta+lambdai))+1)
1/prod(exp(-(delta+lambdai))/(exp(-(delta+lambdai))+1))
prod(exp(-(delta+lambdai))/(exp(-(delta+lambdai))+1))
# Test#
delta = -0.75#
lambda = runif(4)#
gibbsNormalizer(delta,lambda)#
#
support <- gibbs.measure.support(4)#
#
gibbsNormalizerBrute(delta,lambda,support)
logitNormalizingConstant <- function(delta,lambdai){#
	# lambdaij is the vector of element-specific coefficients#
	# delta is the shared intercept#
	1/prod(exp(-(delta+lambdai))/(exp(-(delta+lambdai))+1))#
}#
#
gibbsLogitProb <- function(Ji,delta,lambdai){#
	logitNumerator(Ji,delta,lambdai)/logitNormalizingConstant(delta,lambdai)#
}#
#
LogitProb <- function(Ji,delta,lambdai){#
	prob1 <- exp(delta+lambdai)/(1+exp(delta+lambdai))#
	prob0 <- 1-prob1#
	prod(prob1^Ji*prob0^(1-Ji))#
}#
#
# Calulate the normalizing constant without enumerating support#
gibbsNormalizer <- function(delta,lambdai){#
	logitNormalizingConstant(delta,lambdai)-1#
}#
logitNumerator <- function(Ji,delta,lambdai){#
	exp(sum((delta+lambdai)*Ji))#
}#
#
gibbs.measure.support <- function(n){#
	require(combinat)#
	# returns a 2^n x n binary matrix representing#
	# the support of the binary Gibbs measure in n elements#
	gibbs.support <- rbind(rep(1,n))#
	for(i in 1:(n-1)){#
		gibbs.mat.i <- do.call('rbind',permn(c(rep(1,i),rep(0,n-i))))#
		gibbs.support <- rbind(gibbs.support,gibbs.mat.i)#
	}#
	as.matrix(unique(gibbs.support))#
}#
#
gibbsNormalizerBrute <- function(delta,lambdai,support){#
	normalizer=0#
	for(i in 1:nrow(support)){#
		normalizer = normalizer + logitNumerator(support[i,],delta,lambdai)#
	}#
	normalizer#
}#
#
# Test#
delta = -0.75#
lambda = runif(4)#
gibbsNormalizer(delta,lambda)#
#
support <- gibbs.measure.support(4)#
#
gibbsNormalizerBrute(delta,lambda,support)
# Test#
delta = -0.75#
lambda = runif(10)#
gibbsNormalizer(delta,lambda)#
#
support <- gibbs.measure.support(10)#
#
gibbsNormalizerBrute(delta,lambda,support)
library(IPTM)
hi = gibbs.measure.support(10)
dim(hi)
head(hi)
devtools::install_github('bomin8319/IPTM/pkg')
library(IPTM)#
library(mvtnorm)#
library(MCMCpack)#
set.seed(100)#
nDocs = 5#
node = 1:4#
vocabulary = c("hi", "hello", "fine", "bye", "what")#
nIP = 2#
K = 4#
nwords = 4#
alpha = 2#
mvec = rep(1/4, 4)#
betas = 2#
nvec = rep(1/5, 5)#
netstat = c("intercept", "dyadic")#
P = 1 * ("intercept" %in% netstat) + 3 * (2 * ("dyadic" %in% netstat) + 4 * ("triadic" %in% netstat) + 2 *("degree" %in% netstat))#
prior.b.mean = c(-3, rep(0, P-1))#
prior.b.var = 0.05 * diag(P)#
prior.delta = c(2.5, 0.1)#
sigma_Q = c(0.05, 2)#
niters = c(2, 2200, 100, 200, 10)#
b = lapply(1:nIP, function(IP) {#
    c(rmvnorm(1,  prior.b.mean, prior.b.var))#
  })#
delta = rnorm(1, prior.delta[1], sqrt(prior.delta[2]))#
currentC = sample(1:nIP, K, replace = TRUE)	 #
supportD = gibbs.measure.support(length(node) - 1)#
base.data = GenerateDocs.Gibbs(100, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, base.edge = list(),  base.text = list(), base = TRUE, support = supportD) #
base.edge = base.data$edge	   #
base.text = base.data$text#
TryGiR<- GiR.Gibbs(20, nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, #
					prior.b.mean, prior.b.var, prior.delta, sigma_Q, niters, netstat, base.edge, base.text, seed = 12345)
TryGiR$accept.rate
TryGiR$geweke.1
TryGiR$geweke1
summary(TryGiR$geweke1)
summary(TryGiR$geweke2)
devtools::install_github('bomin8319/IPTM/pkg')#
library(IPTM)#
library(mvtnorm)#
library(MCMCpack)#
set.seed(100)#
nDocs = 5#
node = 1:4#
vocabulary = c("hi", "hello", "fine", "bye", "what")#
nIP = 2#
K = 4#
nwords = 4#
alpha = 2#
mvec = rep(1/4, 4)#
betas = 2#
nvec = rep(1/5, 5)#
netstat = c("intercept", "dyadic")#
P = 1 * ("intercept" %in% netstat) + 3 * (2 * ("dyadic" %in% netstat) + 4 * ("triadic" %in% netstat) + 2 *("degree" %in% netstat))#
prior.b.mean = c(-3, rep(0, P-1))#
prior.b.var = 0.05 * diag(P)#
prior.delta = c(2.5, 0.1)#
sigma_Q = c(0.05, 2)#
niters = c(2, 3300, 100, 300, 10)#
b = lapply(1:nIP, function(IP) {#
    c(rmvnorm(1,  prior.b.mean, prior.b.var))#
  })#
delta = rnorm(1, prior.delta[1], sqrt(prior.delta[2]))#
currentC = sample(1:nIP, K, replace = TRUE)	 #
supportD = gibbs.measure.support(length(node) - 1)#
base.data = GenerateDocs.Gibbs(100, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, base.edge = list(),  base.text = list(), base = TRUE, support = supportD) #
base.edge = base.data$edge	   #
base.text = base.data$text#
TryGiR<- GiR.Gibbs(20, nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, #
					prior.b.mean, prior.b.var, prior.delta, sigma_Q, niters, netstat, base.edge, base.text, seed = 12345)
library(IPTM)#
library(mvtnorm)#
library(MCMCpack)#
set.seed(100)#
nDocs = 5#
node = 1:4#
vocabulary = c("hi", "hello", "fine", "bye", "what")#
nIP = 2#
K = 4#
nwords = 4#
alpha = 2#
mvec = rep(1/4, 4)#
betas = 2#
nvec = rep(1/5, 5)#
netstat = c("intercept", "dyadic")#
P = 1 * ("intercept" %in% netstat) + 3 * (2 * ("dyadic" %in% netstat) + 4 * ("triadic" %in% netstat) + 2 *("degree" %in% netstat))#
prior.b.mean = c(-3, rep(0, P-1))#
prior.b.var = 0.05 * diag(P)#
prior.delta = c(2.5, 0.1)#
sigma_Q = c(0.05, 2)#
niters = c(2, 3300, 100, 300, 10)#
b = lapply(1:nIP, function(IP) {#
    c(rmvnorm(1,  prior.b.mean, prior.b.var))#
  })#
delta = rnorm(1, prior.delta[1], sqrt(prior.delta[2]))#
currentC = sample(1:nIP, K, replace = TRUE)	 #
supportD = gibbs.measure.support(length(node) - 1)#
base.data = GenerateDocs.Gibbs(100, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, b, delta, currentC, netstat, base.edge = list(),  base.text = list(), base = TRUE, support = supportD) #
base.edge = base.data$edge	   #
base.text = base.data$text#
TryGiR<- GiR.Gibbs(20, nDocs, node, vocabulary, nIP, K, nwords, alpha, mvec, betas, nvec, #
					prior.b.mean, prior.b.var, prior.delta, sigma_Q, niters, netstat, base.edge, base.text, seed = 12345)
document()
library(IPTM)#
load('/Users/bomin8319/Desktop/IPTM/VanceServer/Vancenew.RData')#
attach(Vance)#
Vance$node = 1:nrow(Vance$node)#
mintime = Vance$edge[[1]][[3]]#
for (n in 1:length(Vance$edge)){#
  Vance$edge[[n]][3] = (Vance$edge[[n]][[3]] - mintime) / 3600#
}#
Vance$edge = lapply(Vance$edge, function(x){x[1:3]})#
set.seed(1)#
#
#Vance$node = Vance$node[-c(5, 8, 16, 17)]#
#delete = sapply(1:length(Vance$edge), function(d){(unlist(Vance$edge[[d]][1:2]) %in% Vance$node)})#
#Vance$edge = Vance$edge[- which(sapply(1:length(delete), function(d){"FALSE" %in% delete[[d]]})>0)]#
Vancetest <- IPTM_inference.data(Vance$edge, Vance$node, Vance$text, Vance$vocab, nIP = 2, K = 5, sigma_Q = c(0.01, 1),#
                       alpha = 2, mvec = rep(1/5, 5), betas = 2, nvec = rep(1/620, 620), prior.b.mean = c(-3, rep(0, 24)), #
                       prior.b.var = 0.1 * diag(25), prior.delta = c(0, 1), out = 20, n_B = 11000, n_d = 500, burn = 1000, #
                       thinning = 20, netstat = c("intercept", "dyadic", "degree", "triadic"), plot = TRUE, optimize = TRUE)
library(IPTM)#
load('/Users/bomin8319/Desktop/IPTM/VanceServer/Vancenew.RData')#
attach(Vance)#
Vance$node = 1:nrow(Vance$node)#
mintime = Vance$edge[[1]][[3]]#
for (n in 1:length(Vance$edge)){#
  Vance$edge[[n]][3] = (Vance$edge[[n]][[3]] - mintime) / 3600#
}#
Vance$edge = lapply(Vance$edge, function(x){x[1:3]})#
set.seed(1)#
#
#Vance$node = Vance$node[-c(5, 8, 16, 17)]#
#delete = sapply(1:length(Vance$edge), function(d){(unlist(Vance$edge[[d]][1:2]) %in% Vance$node)})#
#Vance$edge = Vance$edge[- which(sapply(1:length(delete), function(d){"FALSE" %in% delete[[d]]})>0)]#
Vancetest <- IPTM_inference.data(Vance$edge, Vance$node, Vance$text, Vance$vocab, nIP = 2, K = 5, sigma_Q = c(0.01, 1),#
                       alpha = 2, mvec = rep(1/5, 5), betas = 2, nvec = rep(1/620, 620), prior.b.mean = c(-3, rep(0, 24)), #
                       prior.b.var = 0.1 * diag(25), prior.delta = c(0, 1), out = 20, n_B = 5500, n_d = 500, burn = 500, #
                       thinning = 10, netstat = c("intercept", "dyadic", "degree", "triadic"), plot = TRUE, optimize = TRUE)
Vancetest$C
TablebetaIP(Vancetest)
PlotbetaIP(Vancetest$B)
rm(list=ls())
library(IPTM)#
load('/Users/bomin8319/Desktop/IPTM/VanceServer/Vancenew.RData')#
attach(Vance)#
Vance$node = 1:nrow(Vance$node)#
mintime = Vance$edge[[1]][[3]]#
for (n in 1:length(Vance$edge)){#
  Vance$edge[[n]][3] = (Vance$edge[[n]][[3]] - mintime) / 3600#
}#
Vance$edge = lapply(Vance$edge, function(x){x[1:3]})#
set.seed(1)#
#
#Vance$node = Vance$node[-c(5, 8, 16, 17)]#
#delete = sapply(1:length(Vance$edge), function(d){(unlist(Vance$edge[[d]][1:2]) %in% Vance$node)})#
#Vance$edge = Vance$edge[- which(sapply(1:length(delete), function(d){"FALSE" %in% delete[[d]]})>0)]#
Vancetest <- IPTM_inference.data(Vance$edge, Vance$node, Vance$text, Vance$vocab, nIP = 2, K = 5, sigma_Q = c(0.01, 1),#
                       alpha = 2, mvec = rep(1/5, 5), betas = 2, nvec = rep(1/620, 620), prior.b.mean = c(-3, rep(0, 24)), #
                       prior.b.var = 0.1 * diag(25), prior.delta = c(0, 1), out = 1000, n_B = 5500, n_d = 500, burn = 500, #
                       thinning = 10, netstat = c("intercept", "dyadic", "degree", "triadic"), plot = TRUE, optimize = TRUE)
library(IPTM)#
load('/Users/bomin8319/Desktop/IPTM/VanceServer/Vancenew.RData')#
attach(Vance)#
Vance$node = 1:nrow(Vance$node)#
mintime = Vance$edge[[1]][[3]]#
for (n in 1:length(Vance$edge)){#
  Vance$edge[[n]][3] = (Vance$edge[[n]][[3]] - mintime) / 3600#
}#
Vance$edge = lapply(Vance$edge, function(x){x[1:3]})#
set.seed(1)#
#
#Vance$node = Vance$node[-c(5, 8, 16, 17)]#
#delete = sapply(1:length(Vance$edge), function(d){(unlist(Vance$edge[[d]][1:2]) %in% Vance$node)})#
#Vance$edge = Vance$edge[- which(sapply(1:length(delete), function(d){"FALSE" %in% delete[[d]]})>0)]#
Vancetest <- IPTM_inference.data(Vance$edge, Vance$node, Vance$text, Vance$vocab, nIP = 2, K = 5, sigma_Q = c(0.01, 1),#
                       alpha = 2, mvec = rep(1/5, 5), betas = 2, nvec = rep(1/620, 620), prior.b.mean = c(-3, rep(0, 24)), #
                       prior.b.var = 0.1 * diag(25), prior.delta = c(0, 1), out = 800, n_B = 5500, n_d = 500, burn = 500, #
                       thinning = 10, netstat = c("intercept", "dyadic", "degree", "triadic"), plot = TRUE, optimize = TRUE)
library(Rcpp)
library(RcppArmadillo)
sourceCpp('~/Desktop/sampler2.cpp')
load('~/Desktop/IPTM/paper/TryGiR.RData')
names(TryGiR)
dim(TryGiR$Forward)
library(IPTM)
par(mfrow=c(5,5), oma = c(3,3,3,3), mar = c(2,1,1,1))#
GiR_PP_Plots(TryGiR$Forward, TryGiR$Backward)
par(mfrow=c(5,5), oma = c(2,2,2,2), mar = c(1,1,1,1))#
GiR_PP_Plots(TryGiR$Forward, TryGiR$Backward)
par(mfrow=c(5,5), oma = c(1,1,1,1), mar = c(1,1,1,1))#
GiR_PP_Plots(TryGiR$Forward, TryGiR$Backward)
par(mfrow=c(5,5), oma = c(0.1,0.1,0.1,0.1), mar = c(2,1,1,1))#
GiR_PP_Plots(TryGiR$Forward, TryGiR$Backward)
GiR_PP_Plots = function(Forward_stats, Backward_stats) {#
  nms = colnames(Forward_stats)#
  for (i in 1L:ncol(Forward_stats)) {#
    all = c(Backward_stats[, i], Forward_stats[, i])#
    quantiles = 100#
    if (grepl("B_", nms[i]) ) {#
      quantiles = 1000#
    }#
    if (grepl("delta", nms[i]) ) {#
      quantiles = 1000#
    }#
    if (grepl("Mean_timediff", nms[i]) ) {#
      quantiles = 1000#
    }#
    uniqueValues = quantile(all,seq(0, 1, length = quantiles))#
    qx1 = numeric(length(uniqueValues))#
  	qx2 = numeric(length(uniqueValues))#
  	for (j in 1:length(uniqueValues)) {#
  		qx1[j] = mean(Forward_stats[, i] <= uniqueValues[j])#
  		qx2[j] = mean(Backward_stats[, i] <= uniqueValues[j])#
  	}#
    qqplot(x = qx1,#
           y = qx2,#
           ylim = c(0, 1),#
           xlim = c(0, 1),#
           ylab = "Backward",#
           xlab = "Forward",#
           col = "blue",#
           pch = 19,#
           main = nms[i],#
           cex.lab = 0.5,#
           cex.axis = 0.5,#
           cex.main = 0.5)#
    abline(0, 1, lty = 1, col = "red", lwd = 0.5)#
    if (nrow(Forward_stats) > 1000) {#
    thinning2 = seq(from = floor(nrow(Forward_stats) / 10), to = nrow(Forward_stats), length.out = 1000)#
    Forward_test2 = Forward_stats[thinning2, i]#
    Backward_test2 = Backward_stats[thinning2, i]#
    } else {#
      Forward_test2 = Forward_stats[, i]#
      Backward_test2 = Backward_stats[, i]    	#
    }#
    text(paste("Backward Mean:", round(mean(Backward_stats[, i]), 4),#
               "\nForward Mean:", round(mean(Forward_stats[, i]), 4),#
               "\nt-test p-value:", round(t.test(Backward_test2, Forward_test2)$p.value, 4),#
               "\nMann-Whitney p-value:", round(wilcox.test(Backward_test2, Forward_test2)$p.value, 4)),#
         x = 0.65,#
         y = 0.15,#
         cex = 0.4)#
  }#
}
GiR_PP_Plots(TryGiR$Forward, TryGiR$Backward)
par(mfrow=c(5,5), oma = c(0.1,0.1,0.1,0.1), mar = c(2,1,1,1))#
GiR_PP_Plots(TryGiR$Forward, TryGiR$Backward)
GiR_PP_Plots = function(Forward_stats, Backward_stats) {#
  nms = colnames(Forward_stats)#
  for (i in 1L:ncol(Forward_stats)) {#
    all = c(Backward_stats[, i], Forward_stats[, i])#
    quantiles = 100#
    if (grepl("B_", nms[i]) ) {#
      quantiles = 1000#
    }#
    if (grepl("delta", nms[i]) ) {#
      quantiles = 1000#
    }#
    if (grepl("Mean_timediff", nms[i]) ) {#
      quantiles = 1000#
    }#
    uniqueValues = quantile(all,seq(0, 1, length = quantiles))#
    qx1 = numeric(length(uniqueValues))#
  	qx2 = numeric(length(uniqueValues))#
  	for (j in 1:length(uniqueValues)) {#
  		qx1[j] = mean(Forward_stats[, i] <= uniqueValues[j])#
  		qx2[j] = mean(Backward_stats[, i] <= uniqueValues[j])#
  	}#
    qqplot(x = qx1,#
           y = qx2,#
           ylim = c(0, 1),#
           xlim = c(0, 1),#
           ylab = "Backward",#
           xlab = "Forward",#
           col = "blue",#
           pch = 19,#
           main = nms[i],#
           cex.lab = 0.5,#
           cex.axis = 0.5,#
           cex.main = 0.1)#
    abline(0, 1, lty = 1, col = "red", lwd = 1)#
    if (nrow(Forward_stats) > 1000) {#
    thinning2 = seq(from = floor(nrow(Forward_stats) / 10), to = nrow(Forward_stats), length.out = 1000)#
    Forward_test2 = Forward_stats[thinning2, i]#
    Backward_test2 = Backward_stats[thinning2, i]#
    } else {#
      Forward_test2 = Forward_stats[, i]#
      Backward_test2 = Backward_stats[, i]    	#
    }#
    text(paste("Backward Mean:", round(mean(Backward_stats[, i]), 4),#
               "\nForward Mean:", round(mean(Forward_stats[, i]), 4),#
               "\nt-test p-value:", round(t.test(Backward_test2, Forward_test2)$p.value, 4),#
               "\nMann-Whitney p-value:", round(wilcox.test(Backward_test2, Forward_test2)$p.value, 4)),#
         x = 0.65,#
         y = 0.15,#
         cex = 0.4)#
  }#
}
par(mfrow=c(5,5), oma = c(0.1,0.1,0.1,0.1), mar = c(2,1,1,1))#
GiR_PP_Plots(TryGiR$Forward, TryGiR$Backward)
GiR_PP_Plots = function(Forward_stats, Backward_stats) {#
  nms = colnames(Forward_stats)#
  for (i in 1L:ncol(Forward_stats)) {#
    all = c(Backward_stats[, i], Forward_stats[, i])#
    quantiles = 100#
    if (grepl("B_", nms[i]) ) {#
      quantiles = 1000#
    }#
    if (grepl("delta", nms[i]) ) {#
      quantiles = 1000#
    }#
    if (grepl("Mean_timediff", nms[i]) ) {#
      quantiles = 1000#
    }#
    uniqueValues = quantile(all,seq(0, 1, length = quantiles))#
    qx1 = numeric(length(uniqueValues))#
  	qx2 = numeric(length(uniqueValues))#
  	for (j in 1:length(uniqueValues)) {#
  		qx1[j] = mean(Forward_stats[, i] <= uniqueValues[j])#
  		qx2[j] = mean(Backward_stats[, i] <= uniqueValues[j])#
  	}#
    qqplot(x = qx1,#
           y = qx2,#
           ylim = c(0, 1),#
           xlim = c(0, 1),#
           ylab = "Backward",#
           xlab = "Forward",#
           col = "blue",#
           pch = 19, cex = 0.5#
           main = nms[i],#
           cex.lab = 0.2,#
           cex.axis = 0.2,#
           cex.main = 0.5)#
    abline(0, 1, lty = 1, col = "red", lwd = 1)#
    if (nrow(Forward_stats) > 1000) {#
    thinning2 = seq(from = floor(nrow(Forward_stats) / 10), to = nrow(Forward_stats), length.out = 1000)#
    Forward_test2 = Forward_stats[thinning2, i]#
    Backward_test2 = Backward_stats[thinning2, i]#
    } else {#
      Forward_test2 = Forward_stats[, i]#
      Backward_test2 = Backward_stats[, i]    	#
    }#
    text(paste("Backward Mean:", round(mean(Backward_stats[, i]), 4),#
               "\nForward Mean:", round(mean(Forward_stats[, i]), 4),#
               "\nt-test p-value:", round(t.test(Backward_test2, Forward_test2)$p.value, 4),#
               "\nMann-Whitney p-value:", round(wilcox.test(Backward_test2, Forward_test2)$p.value, 4)),#
         x = 0.65,#
         y = 0.15,#
         cex = 0.4)#
  }#
}
GiR_PP_Plots = function(Forward_stats, Backward_stats) {#
  nms = colnames(Forward_stats)#
  for (i in 1L:ncol(Forward_stats)) {#
    all = c(Backward_stats[, i], Forward_stats[, i])#
    quantiles = 100#
    if (grepl("B_", nms[i]) ) {#
      quantiles = 1000#
    }#
    if (grepl("delta", nms[i]) ) {#
      quantiles = 1000#
    }#
    if (grepl("Mean_timediff", nms[i]) ) {#
      quantiles = 1000#
    }#
    uniqueValues = quantile(all,seq(0, 1, length = quantiles))#
    qx1 = numeric(length(uniqueValues))#
  	qx2 = numeric(length(uniqueValues))#
  	for (j in 1:length(uniqueValues)) {#
  		qx1[j] = mean(Forward_stats[, i] <= uniqueValues[j])#
  		qx2[j] = mean(Backward_stats[, i] <= uniqueValues[j])#
  	}#
    qqplot(x = qx1,#
           y = qx2,#
           ylim = c(0, 1),#
           xlim = c(0, 1),#
           ylab = "Backward",#
           xlab = "Forward",#
           col = "blue",#
           pch = 19, cex = 0.5,#
           main = nms[i],#
           cex.lab = 0.2,#
           cex.axis = 0.2,#
           cex.main = 0.5)#
    abline(0, 1, lty = 1, col = "red", lwd = 1)#
    if (nrow(Forward_stats) > 1000) {#
    thinning2 = seq(from = floor(nrow(Forward_stats) / 10), to = nrow(Forward_stats), length.out = 1000)#
    Forward_test2 = Forward_stats[thinning2, i]#
    Backward_test2 = Backward_stats[thinning2, i]#
    } else {#
      Forward_test2 = Forward_stats[, i]#
      Backward_test2 = Backward_stats[, i]    	#
    }#
    text(paste("Backward Mean:", round(mean(Backward_stats[, i]), 4),#
               "\nForward Mean:", round(mean(Forward_stats[, i]), 4),#
               "\nt-test p-value:", round(t.test(Backward_test2, Forward_test2)$p.value, 4),#
               "\nMann-Whitney p-value:", round(wilcox.test(Backward_test2, Forward_test2)$p.value, 4)),#
         x = 0.65,#
         y = 0.15,#
         cex = 0.4)#
  }#
}
par(mfrow=c(5,5), oma = c(0.1,0.1,0.1,0.1), mar = c(2,1,1,1))#
GiR_PP_Plots(TryGiR$Forward, TryGiR$Backward)
GiR_PP_Plots = function(Forward_stats, Backward_stats) {#
  nms = colnames(Forward_stats)#
  for (i in 1L:ncol(Forward_stats)) {#
    all = c(Backward_stats[, i], Forward_stats[, i])#
    quantiles = 100#
    if (grepl("B_", nms[i]) ) {#
      quantiles = 1000#
    }#
    if (grepl("delta", nms[i]) ) {#
      quantiles = 1000#
    }#
    if (grepl("Mean_timediff", nms[i]) ) {#
      quantiles = 1000#
    }#
    uniqueValues = quantile(all,seq(0, 1, length = quantiles))#
    qx1 = numeric(length(uniqueValues))#
  	qx2 = numeric(length(uniqueValues))#
  	for (j in 1:length(uniqueValues)) {#
  		qx1[j] = mean(Forward_stats[, i] <= uniqueValues[j])#
  		qx2[j] = mean(Backward_stats[, i] <= uniqueValues[j])#
  	}#
    qqplot(x = qx1,#
           y = qx2,#
           ylim = c(0, 1),#
           xlim = c(0, 1),#
           ylab = "Backward",#
           xlab = "Forward",#
           col = "blue",#
           pch = 19, cex = 0.5,#
           main = nms[i],#
           cex.lab = 0.4,#
           cex.axis = 0.4,#
           cex.main = 0.5)#
    abline(0, 1, lty = 1, col = "red", lwd = 1)#
    if (nrow(Forward_stats) > 1000) {#
    thinning2 = seq(from = floor(nrow(Forward_stats) / 10), to = nrow(Forward_stats), length.out = 1000)#
    Forward_test2 = Forward_stats[thinning2, i]#
    Backward_test2 = Backward_stats[thinning2, i]#
    } else {#
      Forward_test2 = Forward_stats[, i]#
      Backward_test2 = Backward_stats[, i]    	#
    }#
    text(paste("Backward Mean:", round(mean(Backward_stats[, i]), 4),#
               "\nForward Mean:", round(mean(Forward_stats[, i]), 4),#
               "\nt-test p-value:", round(t.test(Backward_test2, Forward_test2)$p.value, 4),#
               "\nMann-Whitney p-value:", round(wilcox.test(Backward_test2, Forward_test2)$p.value, 4)),#
         x = 0.65,#
         y = 0.15,#
         cex = 0.4)#
  }#
}
par(mfrow=c(5,5), oma = c(0.1,0.1,0.1,0.1), mar = c(2,1,1,1))#
GiR_PP_Plots(TryGiR$Forward, TryGiR$Backward)
GiR_PP_Plots = function(Forward_stats, Backward_stats) {#
  nms = colnames(Forward_stats)#
  for (i in 1L:ncol(Forward_stats)) {#
    all = c(Backward_stats[, i], Forward_stats[, i])#
    quantiles = 100#
    if (grepl("B_", nms[i]) ) {#
      quantiles = 1000#
    }#
    if (grepl("delta", nms[i]) ) {#
      quantiles = 1000#
    }#
    if (grepl("Mean_timediff", nms[i]) ) {#
      quantiles = 1000#
    }#
    uniqueValues = quantile(all,seq(0, 1, length = quantiles))#
    qx1 = numeric(length(uniqueValues))#
  	qx2 = numeric(length(uniqueValues))#
  	for (j in 1:length(uniqueValues)) {#
  		qx1[j] = mean(Forward_stats[, i] <= uniqueValues[j])#
  		qx2[j] = mean(Backward_stats[, i] <= uniqueValues[j])#
  	}#
    qqplot(x = qx1,#
           y = qx2,#
           ylim = c(0, 1),#
           xlim = c(0, 1),#
           ylab = "Backward",#
           xlab = "Forward",#
           col = "blue",#
           pch = 19, cex = 0.3,#
           main = nms[i],#
           cex.lab = 0.4,#
           cex.axis = 0.4,#
           cex.main = 0.5)#
    abline(0, 1, lty = 1, col = "red", lwd = 1)#
    if (nrow(Forward_stats) > 1000) {#
    thinning2 = seq(from = floor(nrow(Forward_stats) / 10), to = nrow(Forward_stats), length.out = 1000)#
    Forward_test2 = Forward_stats[thinning2, i]#
    Backward_test2 = Backward_stats[thinning2, i]#
    } else {#
      Forward_test2 = Forward_stats[, i]#
      Backward_test2 = Backward_stats[, i]    	#
    }#
    text(paste("Backward Mean:", round(mean(Backward_stats[, i]), 4),#
               "\nForward Mean:", round(mean(Forward_stats[, i]), 4),#
               "\nt-test p-value:", round(t.test(Backward_test2, Forward_test2)$p.value, 4),#
               "\nMann-Whitney p-value:", round(wilcox.test(Backward_test2, Forward_test2)$p.value, 4)),#
         x = 0.65,#
         y = 0.15,#
         cex = 0.4)#
  }#
}
par(mfrow=c(5,5), oma = c(0.1,0.1,0.1,0.1), mar = c(2,1,1,1))#
GiR_PP_Plots(TryGiR$Forward, TryGiR$Backward)
library(devtools)
install_github("bomin8319/DLFM/pkg")
library(devtools)
docuemnt()
#Dare EDA#
library(anytime)#
library(ggplot2)#
library(MCMCpack)#
library(reshape2)#
library(gridExtra)#
library(ggrepel)#
library(RColorBrewer)#
library(lda)
load('/Users/bomin8319/Desktop/IPTM/paper/Darenew.RData')
#IPTM model results#
load('/Users/bomin8319/Desktop/IPTM/paper/Darenew.RData')#
# 762 - #
attach(Dare)#
Dare$node = 1:nrow(Dare$node)#
Dare$text = Dare$text[762:length(Dare$edge)]#
Dare$edge = Dare$edge[762:length(Dare$edge)]#
Dare$edge = Dare$edge[-which(sapply(Dare$text, function(d){length(d)})==0)]#
Dare$text = Dare$text[-which(sapply(Dare$text, function(d){length(d)})==0)]#
mintime = Dare$edge[[1]][[3]]#
for (n in 1:length(Dare$edge)){#
  Dare$edge[[n]][3] = (Dare$edge[[n]][[3]] - mintime) / 3600#
}#
Dare$edge = lapply(Dare$edge, function(x){x[1:3]})
load("/Users/bomin8319/Desktop/IPTM/presentations/polNet/Daretest.RData")
Daretest1 = Daretest
Daretest1$C
TableWord = function(Zchain, K, textlist, vocabulary) {#
  # Generate a table of token-topic assignments with high probabilities for each IP#
  ##
  # Args #
  #  Zchain summary of Z obtained using MCMC function#
  #  K total number of topics specified by the user#
  #  textlist list of text containing the words in each document#
  #  vocabulary all vocabularies used over the corpus#
  ##
  # Returns#
  #  List of table that summarize token-topic assignments for each IP#
  W = length(vocabulary)#
    Zsummary = list()#
    topic.word = matrix(0, nrow = K, ncol = W)#
    colnames(topic.word) = vocabulary#
    iter = 1#
    for (d in seq(along = textlist)) {#
      if (length(Zchain[[d]]) > 0){#
        Zsummary[[iter]] = Zchain[[d]]#
        names(Zsummary[[iter]])<- vocabulary[textlist[[d]]]#
        iter = iter+1#
      }#
    }#
    topic.dist = t(tabulate(unlist(Zsummary), K)/length(unlist(Zsummary)))#
    colnames(topic.dist) = c(1L:K)#
    top.topic = topic.dist[, order(topic.dist, decreasing = TRUE)]#
    all.word = unlist(Zsummary)#
    for (i in seq(along = all.word)){#
      matchWZ = which(colnames(topic.word) == names(all.word[i]))#
      topic.word[all.word[i], matchWZ] = topic.word[all.word[i], matchWZ] + 1#
    }#
    table.word = top.topic.words(topic.word, num.words = 15, by.score = TRUE)#
    colnames(table.word) = names(top.topic)#
  return(table.word)#
}
which(Sandy$date %in% unique(Sandy$date)[23:27])#
TableWord(Daretest1$Z[216:418], 20, Dare$text[534:736], Dare$vocab)#
table(unlist(Daretest1$Z[216:418])) / sum(table(unlist(Daretest1$Z[534:736])))
Daretest1$C
names(Daretest1$C)
names(Daretest1$C) = 1:20
Daretest1$C
library(reshape)#
#
DareB = matrix(NA, 500, 25)#
DareB[,1:25] = t(Daretest1$B[[1]])#
colnames(DareB)= c( "intercept",#
"outdegree1", "outdegree2", "outdegree3", "indegree1", "indegree2", "indegree3",#
"send1", "send2", "send3" ,"receive1", "receive2", "receive3",#
"2-send1", "2-send2", "2-send3", "2-receive1", "2-receive2" ,"2-receive3",#
"sibling1", "sibling2" ,"sibling3", "cosibling1", "cosibling2", "cosibling3")#
DareB = melt(DareB)#
#
DareB2 = matrix(NA, 500, 25)#
DareB2[,1:25] = t(Daretest1$B[[2]])#
colnames(DareB2)= c( "intercept",#
"outdegree1", "outdegree2", "outdegree3", "indegree1", "indegree2", "indegree3",#
"send1", "send2", "send3" ,"receive1", "receive2", "receive3",#
"2-send1", "2-send2", "2-send3", "2-receive1", "2-receive2" ,"2-receive3",#
"sibling1", "sibling2" ,"sibling3", "cosibling1", "cosibling2", "cosibling3")#
DareB2 = melt(DareB2)#
#
DareB$IP = 1#
DareB2$IP = 2#
DareBnew = rbind(DareB, DareB2)[,-1]#
DareBnew$IP = as.factor(DareBnew$IP)#
colnames(DareBnew) = c("Netstat", "Estimate", "IP")#
DareBnew$Netstat = factor(DareBnew$Netstat, levels =  c( "intercept",#
"outdegree1", "outdegree2", "outdegree3", "indegree1", "indegree2", "indegree3",#
"send1", "send2", "send3" ,"receive1", "receive2", "receive3",#
"2-send1", "2-send2", "2-send3", "2-receive1", "2-receive2" ,"2-receive3",#
"sibling1", "sibling2" ,"sibling3", "cosibling1", "cosibling2", "cosibling3"))#
colnames(DareBnew) = c("Netstat", "Estimate", "IP")#
p <- ggplot(DareBnew, aes(Netstat, Estimate, colour = IP)) + geom_boxplot(aes(colour = factor(IP)), position = position_dodge()) + coord_flip() + geom_hline(yintercept = 0.0, colour = "black", size = 0.5)
ã…”
p
TableWord(Daretest1$Z[72:418], 20, Dare$text[390:736], Dare$vocab)
Daretest1$C
load("/Users/bomin8319/Desktop/IPTM/presentations/polNet/Daretest1.RData")
library(reshape)#
#
DareB = matrix(NA, 500, 25)#
DareB[,1:25] = t(Daretest1$B[[1]])#
colnames(DareB)= c( "intercept",#
"outdegree1", "outdegree2", "outdegree3", "indegree1", "indegree2", "indegree3",#
"send1", "send2", "send3" ,"receive1", "receive2", "receive3",#
"2-send1", "2-send2", "2-send3", "2-receive1", "2-receive2" ,"2-receive3",#
"sibling1", "sibling2" ,"sibling3", "cosibling1", "cosibling2", "cosibling3")#
DareB = melt(DareB)#
#
DareB2 = matrix(NA, 500, 25)#
DareB2[,1:25] = t(Daretest1$B[[2]])#
colnames(DareB2)= c( "intercept",#
"outdegree1", "outdegree2", "outdegree3", "indegree1", "indegree2", "indegree3",#
"send1", "send2", "send3" ,"receive1", "receive2", "receive3",#
"2-send1", "2-send2", "2-send3", "2-receive1", "2-receive2" ,"2-receive3",#
"sibling1", "sibling2" ,"sibling3", "cosibling1", "cosibling2", "cosibling3")#
DareB2 = melt(DareB2)#
#
DareB$IP = 1#
DareB2$IP = 2#
DareBnew = rbind(DareB, DareB2)[,-1]#
DareBnew$IP = as.factor(DareBnew$IP)#
colnames(DareBnew) = c("Netstat", "Estimate", "IP")#
DareBnew$Netstat = factor(DareBnew$Netstat, levels =  c( "intercept",#
"outdegree1", "outdegree2", "outdegree3", "indegree1", "indegree2", "indegree3",#
"send1", "send2", "send3" ,"receive1", "receive2", "receive3",#
"2-send1", "2-send2", "2-send3", "2-receive1", "2-receive2" ,"2-receive3",#
"sibling1", "sibling2" ,"sibling3", "cosibling1", "cosibling2", "cosibling3"))#
colnames(DareBnew) = c("Netstat", "Estimate", "IP")#
p <- ggplot(DareBnew, aes(Netstat, Estimate, colour = IP)) + geom_boxplot(aes(colour = factor(IP)), position = position_dodge()) + coord_flip() + geom_hline(yintercept = 0.0, colour = "black", size = 0.5)
p
load("/Users/bomin8319/Desktop/IPTM/presentations/polNet/Daretest.RData")
load("/Users/bomin8319/Desktop/IPTM/presentations/polNet/data/Daretest.RData")
Daretest1 = Daretest#
Daretest1$C
library(reshape)#
#
DareB = matrix(NA, 500, 25)#
DareB[,1:25] = t(Daretest1$B[[1]])#
colnames(DareB)= c( "intercept",#
"outdegree1", "outdegree2", "outdegree3", "indegree1", "indegree2", "indegree3",#
"send1", "send2", "send3" ,"receive1", "receive2", "receive3",#
"2-send1", "2-send2", "2-send3", "2-receive1", "2-receive2" ,"2-receive3",#
"sibling1", "sibling2" ,"sibling3", "cosibling1", "cosibling2", "cosibling3")#
DareB = melt(DareB)#
#
DareB2 = matrix(NA, 500, 25)#
DareB2[,1:25] = t(Daretest1$B[[2]])#
colnames(DareB2)= c( "intercept",#
"outdegree1", "outdegree2", "outdegree3", "indegree1", "indegree2", "indegree3",#
"send1", "send2", "send3" ,"receive1", "receive2", "receive3",#
"2-send1", "2-send2", "2-send3", "2-receive1", "2-receive2" ,"2-receive3",#
"sibling1", "sibling2" ,"sibling3", "cosibling1", "cosibling2", "cosibling3")#
DareB2 = melt(DareB2)#
#
DareB$IP = 1#
DareB2$IP = 2#
DareBnew = rbind(DareB, DareB2)[,-1]#
DareBnew$IP = as.factor(DareBnew$IP)#
colnames(DareBnew) = c("Netstat", "Estimate", "IP")#
DareBnew$Netstat = factor(DareBnew$Netstat, levels =  c( "intercept",#
"outdegree1", "outdegree2", "outdegree3", "indegree1", "indegree2", "indegree3",#
"send1", "send2", "send3" ,"receive1", "receive2", "receive3",#
"2-send1", "2-send2", "2-send3", "2-receive1", "2-receive2" ,"2-receive3",#
"sibling1", "sibling2" ,"sibling3", "cosibling1", "cosibling2", "cosibling3"))#
colnames(DareBnew) = c("Netstat", "Estimate", "IP")#
p <- ggplot(DareBnew, aes(Netstat, Estimate, colour = IP)) + geom_boxplot(aes(colour = factor(IP)), position = position_dodge()) + coord_flip() + geom_hline(yintercept = 0.0, colour = "black", size = 0.5)
p
Daretest1$C
names(Daretest1$C) = 1:20
Daretest1$C
TableWord(Daretest1$Z[216:418], 20, Dare$text[534:736], Dare$vocab)#
table(unlist(Daretest1$Z[216:418])) / sum(table(unlist(Daretest1$Z[534:736])))
load('~/Desktop/IPTM/pkg/R/output.RData')
output$C
output$Z
output$D
plot(output$D, type = 'l')
plot(output$B[[1]][1,], type = 'l')
plot(output$B[[1]][2,], type = 'l')
plot(output$B[[1]][3,], type = 'l')
plot(output$B[[1]][4,], type = 'l')
plot(output$B[[2]][4,], type = 'l')
plot(output$B[[2]][25,], type = 'l')
plot(output$B[[2]][24,], type = 'l')
plot(output$B[[2]][23,], type = 'l')
load('~/Desktop/output.RData')
plot(output$B[[2]][23,], type = 'l')
#Dare EDA#
library(anytime)#
library(ggplot2)#
library(MCMCpack)#
library(reshape2)#
library(gridExtra)#
library(ggrepel)#
library(RColorBrewer)#
library(lda)
length(output$Z)
load('/Users/bomin8319/Desktop/IPTM/paper/Darenew.RData')
load('/Users/bomin8319/Desktop/IPTM/paper/code/Darenew.RData')
length(Dare$edge)
load('~/Desktop/IPTM/paper/code/Daretest.RData')
output$Z
# 762 - #
attach(Dare)#
Dare$node = 1:nrow(Dare$node)#
Dare$text = Dare$text[1:length(Dare$edge)]#
Dare$edge = Dare$edge[762:length(Dare$edge)]#
Dare$edge = Dare$edge[-which(sapply(Dare$text, function(d){length(d)})==0)]#
Dare$text = Dare$text[-which(sapply(Dare$text, function(d){length(d)})==0)]#
mintime = Dare$edge[[1]][[3]]#
for (n in 1:length(Dare$edge)){#
  Dare$edge[[n]][3] = (Dare$edge[[n]][[3]] - mintime) / 3600#
}#
Dare$edge = lapply(Dare$edge, function(x){x[1:3]})
load("/Users/bomin8319/Desktop/IPTM/paper/code/Daretest.RData")
Daretest1 = output#
Daretest1$C
TableWord = function(Zchain, K, textlist, vocabulary) {#
  # Generate a table of token-topic assignments with high probabilities for each IP#
  ##
  # Args #
  #  Zchain summary of Z obtained using MCMC function#
  #  K total number of topics specified by the user#
  #  textlist list of text containing the words in each document#
  #  vocabulary all vocabularies used over the corpus#
  ##
  # Returns#
  #  List of table that summarize token-topic assignments for each IP#
  W = length(vocabulary)#
    Zsummary = list()#
    topic.word = matrix(0, nrow = K, ncol = W)#
    colnames(topic.word) = vocabulary#
    iter = 1#
    for (d in seq(along = textlist)) {#
      if (length(Zchain[[d]]) > 0){#
        Zsummary[[iter]] = Zchain[[d]]#
        names(Zsummary[[iter]])<- vocabulary[textlist[[d]]]#
        iter = iter+1#
      }#
    }#
    topic.dist = t(tabulate(unlist(Zsummary), K)/length(unlist(Zsummary)))#
    colnames(topic.dist) = c(1L:K)#
    top.topic = topic.dist[, order(topic.dist, decreasing = TRUE)]#
    all.word = unlist(Zsummary)#
    for (i in seq(along = all.word)){#
      matchWZ = which(colnames(topic.word) == names(all.word[i]))#
      topic.word[all.word[i], matchWZ] = topic.word[all.word[i], matchWZ] + 1#
    }#
    table.word = top.topic.words(topic.word, num.words = 15, by.score = TRUE)#
    colnames(table.word) = names(top.topic)#
  return(table.word)#
}#
#
which(Sandy$date %in% unique(Sandy$date)[20:27])
TableWord(Daretest1$Z, 20, Dare$text, Dare$vocab)
#IPTM model results#
load('/Users/bomin8319/Desktop/IPTM/paper/code/Darenew.RData')#
# 762 - #
attach(Dare)#
Dare$node = 1:nrow(Dare$node)#
Dare$text = Dare$text[762:length(Dare$edge)]#
Dare$edge = Dare$edge[762:length(Dare$edge)]#
Dare$edge = Dare$edge[-which(sapply(Dare$text, function(d){length(d)})==0)]#
Dare$text = Dare$text[-which(sapply(Dare$text, function(d){length(d)})==0)]#
mintime = Dare$edge[[1]][[3]]#
for (n in 1:length(Dare$edge)){#
  Dare$edge[[n]][3] = (Dare$edge[[n]][[3]] - mintime) / 3600#
}#
Dare$edge = lapply(Dare$edge, function(x){x[1:3]})#
#
load("/Users/bomin8319/Desktop/IPTM/paper/code/Daretest.RData")#
Daretest1 = output#
Daretest1$C#
#
TableWord = function(Zchain, K, textlist, vocabulary) {#
  # Generate a table of token-topic assignments with high probabilities for each IP#
  ##
  # Args #
  #  Zchain summary of Z obtained using MCMC function#
  #  K total number of topics specified by the user#
  #  textlist list of text containing the words in each document#
  #  vocabulary all vocabularies used over the corpus#
  ##
  # Returns#
  #  List of table that summarize token-topic assignments for each IP#
  W = length(vocabulary)#
    Zsummary = list()#
    topic.word = matrix(0, nrow = K, ncol = W)#
    colnames(topic.word) = vocabulary#
    iter = 1#
    for (d in seq(along = textlist)) {#
      if (length(Zchain[[d]]) > 0){#
        Zsummary[[iter]] = Zchain[[d]]#
        names(Zsummary[[iter]])<- vocabulary[textlist[[d]]]#
        iter = iter+1#
      }#
    }#
    topic.dist = t(tabulate(unlist(Zsummary), K)/length(unlist(Zsummary)))#
    colnames(topic.dist) = c(1L:K)#
    top.topic = topic.dist[, order(topic.dist, decreasing = TRUE)]#
    all.word = unlist(Zsummary)#
    for (i in seq(along = all.word)){#
      matchWZ = which(colnames(topic.word) == names(all.word[i]))#
      topic.word[all.word[i], matchWZ] = topic.word[all.word[i], matchWZ] + 1#
    }#
    table.word = top.topic.words(topic.word, num.words = 15, by.score = TRUE)#
    colnames(table.word) = names(top.topic)#
  return(table.word)#
}#
TableWord(Daretest1$Z, 20, Dare$text, Dare$vocab)
rm(list=ls())
library(anytime)#
library(ggplot2)#
library(MCMCpack)#
library(reshape2)#
library(gridExtra)#
library(ggrepel)#
library(RColorBrewer)#
library(lda)#
#
load('/Users/bomin8319/Desktop/IPTM/paper/Darenew.RData')#
# 762 - #
attach(Dare)#
Dare$text = Dare$text[762:length(Dare$edge)]#
Dare$edge = Dare$edge[762:length(Dare$edge)]#
Dare$edge = Dare$edge[-which(sapply(Dare$text, function(d){length(d)})==0)]#
Dare$text = Dare$text[-which(sapply(Dare$text, function(d){length(d)})==0)]#
#mintime = Dare$edge[[1]][[3]]#
#for (n in 1:length(Dare$edge)){#
#  Dare$edge[[n]][3] = (Dare$edge[[n]][[3]] - mintime) / 3600#
#}#
#
sender = sapply(Dare$edge, function(d){d[[1]]})#
sender = data.frame(sender=sender, dept = Dare$node[sender, 3], time = sapply(Dare$edge, function(d){d[[3]]}), date = anydate(sapply(Dare$edge, function(d){d[[3]]})))#
sender$date = format(sender$date, format = "%m/%d")#
Dept = data.frame(Date = c(sapply(unique(sender$date), function(d){rep(as.character(d), 22)})), #
				  Department = c(sapply(unique(sender$date), function(d){sort(unique(Dare$node[,3]))#
})), Send = NA)#
i = 1#
for (date in unique(sender$date)) {#
Dept[(22*(i-1)+1) : (22*i),3]= tabulate(sender[which(sender[,4] == date),2], 22)#
i = i + 1#
}#
ave = c()#
i = 1#
for (date in unique(Dept$Date)) {#
	ave[i] = mean(Dept[which(Dept$Date == date),3])#
	i = i + 1#
}#
colourCount = 22#
getPalette = colorRampPalette(brewer.pal(9, "Set1"))#
#
Dept2 = data.frame(Date = unique(Dept$Date), Send = ave)#
	f <- ggplot(Dept, aes(Date, Send, colour = Department), show.legend=FALSE)#
	f + geom_line(aes(group = Department)) + guides(col = guide_legend(nrow=22)) + scale_x_discrete(breaks = function(n) n[seq(0, length(n), by = length(n)/20)]) + theme(legend.text = element_text(size = 8)) +geom_vline(xintercept = 48, colour = "red", size = 1) +geom_vline(xintercept = 52, colour = "red", size = 1) + #
	geom_vline(xintercept = 45, colour = "red", size = 0.5, linetype = "dashed")+ annotate("text", x =50, y = 21, label = "Sandy", colour= "red", size = 3 ) +#
	annotate("segment", x = 40, xend = 44, y = 15, yend = 14, colour = "red", size = 0.1, arrow = arrow()) + annotate("text", x =40, y = 15.5, label = "First Sandy", colour= "red", size = 3)  +scale_colour_manual(values = getPalette(colourCount)) + theme_bw()#
#+ stat_summary(aes(group = 1), size = 1.2, fun.y=mean, geom="line", colour = "grey35")
load('/Users/bomin8319/Desktop/IPTM/paper/code/Darenew.RData')#
# 762 - #
attach(Dare)#
Dare$text = Dare$text[762:length(Dare$edge)]#
Dare$edge = Dare$edge[762:length(Dare$edge)]#
Dare$edge = Dare$edge[-which(sapply(Dare$text, function(d){length(d)})==0)]#
Dare$text = Dare$text[-which(sapply(Dare$text, function(d){length(d)})==0)]#
#mintime = Dare$edge[[1]][[3]]#
#for (n in 1:length(Dare$edge)){#
#  Dare$edge[[n]][3] = (Dare$edge[[n]][[3]] - mintime) / 3600#
#}#
#
sender = sapply(Dare$edge, function(d){d[[1]]})#
sender = data.frame(sender=sender, dept = Dare$node[sender, 3], time = sapply(Dare$edge, function(d){d[[3]]}), date = anydate(sapply(Dare$edge, function(d){d[[3]]})))#
sender$date = format(sender$date, format = "%m/%d")#
Dept = data.frame(Date = c(sapply(unique(sender$date), function(d){rep(as.character(d), 22)})), #
				  Department = c(sapply(unique(sender$date), function(d){sort(unique(Dare$node[,3]))#
})), Send = NA)#
i = 1#
for (date in unique(sender$date)) {#
Dept[(22*(i-1)+1) : (22*i),3]= tabulate(sender[which(sender[,4] == date),2], 22)#
i = i + 1#
}#
ave = c()#
i = 1#
for (date in unique(Dept$Date)) {#
	ave[i] = mean(Dept[which(Dept$Date == date),3])#
	i = i + 1#
}#
colourCount = 22#
getPalette = colorRampPalette(brewer.pal(9, "Set1"))#
#
Dept2 = data.frame(Date = unique(Dept$Date), Send = ave)#
	f <- ggplot(Dept, aes(Date, Send, colour = Department), show.legend=FALSE)#
	f + geom_line(aes(group = Department)) + guides(col = guide_legend(nrow=22)) + scale_x_discrete(breaks = function(n) n[seq(0, length(n), by = length(n)/20)]) + theme(legend.text = element_text(size = 8)) +geom_vline(xintercept = 48, colour = "red", size = 1) +geom_vline(xintercept = 52, colour = "red", size = 1) + #
	geom_vline(xintercept = 45, colour = "red", size = 0.5, linetype = "dashed")+ annotate("text", x =50, y = 21, label = "Sandy", colour= "red", size = 3 ) +#
	annotate("segment", x = 40, xend = 44, y = 15, yend = 14, colour = "red", size = 0.1, arrow = arrow()) + annotate("text", x =40, y = 15.5, label = "First Sandy", colour= "red", size = 3)  +scale_colour_manual(values = getPalette(colourCount)) + theme_bw()#
#+ stat_summary(aes(group = 1), size = 1.2, fun.y=mean, geom="line", colour = "grey35")
f <- ggplot(Dept, aes(Date, Send, colour = Department), show.legend=FALSE)#
	f + geom_line(aes(group = Department)) + guides(col = guide_legend(nrow=22)) + scale_x_discrete(breaks = function(n) n[seq(0, length(n), by = length(n)/10)]) + theme(legend.text = element_text(size = 10)) +geom_vline(xintercept = 23, colour = "red", size = 1) +geom_vline(xintercept = 27, colour = "red", size = 1) + #
	geom_vline(xintercept = 20, colour = "red", size = 0.5, linetype = "dashed")+ annotate("text", x =25, y = 21, label = "Sandy", colour= "red" ) + #
	annotate("segment", x = 18, xend = 20, y = 15, yend = 14, colour = "red", size = 0.1, arrow = arrow()) + annotate("text", x =18, y = 15.5, label = "First Sandy", colour= "red", size = 3)  +scale_colour_manual(values = getPalette(colourCount))#
#+ stat_summary(aes(group = 1), size = 1.2, fun.y=mean, geom="line", colour = "grey35")
receiver = unlist(sapply(Dare$edge, function(d){d[[2]]}))#
time = unlist(sapply(Dare$edge, function(d){rep(d[[3]], length(d[[2]]))}))#
date = anydate(unlist(sapply(Dare$edge, function(d){rep(d[[3]], length(d[[2]]))})))#
receiver  = data.frame(receiver =receiver, dept = Dare$node[receiver , 3], time = time, date = date)#
receiver$date = format(receiver$date, format = "%m/%d")#
Dept = data.frame(Date = c(sapply(unique(receiver$date), function(d){rep(as.character(d), 22)})), #
				  Department = c(sapply(unique(receiver$date), function(d){sort(unique(Dare$node[,3]))#
})), Receive = NA)#
i = 1#
for (date in unique(receiver$date)) {#
Dept[(22*(i-1)+1) : (22*i),3]= tabulate(receiver[which(receiver[,4] == date),2], 22)#
i = i + 1#
}#
#
	f <- ggplot(Dept, aes(Date, Receive, colour = Department))#
	f + geom_line(aes(group = Department)) + guides(col = guide_legend(nrow=22))+ scale_x_discrete(breaks = function(n) n[seq(0, length(n), by = length(n)/10)]) + theme(legend.text = element_text(size = 5)) +geom_vline(xintercept = 23, colour = "red", size = 1) +geom_vline(xintercept = 27, colour = "red", size = 1) + #
	geom_vline(xintercept = 20, colour = "red", size = 0.5, linetype = "dashed")+ annotate("text", x =25, y = 27, label = "Sandy", colour= "red" ) + #
	annotate("segment", x = 18, xend = 20, y = 20, yend = 19, colour = "red", size = 0.1, arrow = arrow()) + annotate("text", x =18, y = 20.5, label = "First Sandy", colour= "red", size = 3)  +scale_colour_manual(values = getPalette(colourCount))
f <- ggplot(Dept, aes(Date, Receive, colour = Department))#
	f + geom_line(aes(group = Department)) + guides(col = guide_legend(nrow=22))+ scale_x_discrete(breaks = function(n) n[seq(0, length(n), by = length(n)/10)]) + theme(legend.text = element_text(size = 10)) +geom_vline(xintercept = 23, colour = "red", size = 1) +geom_vline(xintercept = 27, colour = "red", size = 1) + #
	geom_vline(xintercept = 20, colour = "red", size = 0.5, linetype = "dashed")+ annotate("text", x =25, y = 27, label = "Sandy", colour= "red" ) + #
	annotate("segment", x = 18, xend = 20, y = 20, yend = 19, colour = "red", size = 0.1, arrow = arrow()) + annotate("text", x =18, y = 20.5, label = "First Sandy", colour= "red", size = 3)  +scale_colour_manual(values = getPalette(colourCount))#
	#+ stat_summary(aes(group = 1), size = 1.2, fun.y=mean, geom="line", colour = "grey35")+
Sandy = sapply(Dare$text, function(d){sum(49 == d)})#
Sandy = data.frame(Sandy=Sandy, time = sapply(Dare$edge, function(d){d[[3]]}), date = anydate(sapply(Dare$edge, function(d){d[[3]]})))#
Sandy$date = format(Sandy$date, format = "%m/%d")#
Hurr = sapply(Dare$text, function(d){sum(81 == d)})#
Hurr = data.frame(Hurr =Hurr , time = sapply(Dare$edge, function(d){d[[3]]}), date = anydate(sapply(Dare$edge, function(d){d[[3]]})))#
Hurr $date = format(Hurr$date, format = "%m/%d")#
#
Dept = data.frame(Date = c(sapply(unique(Sandy$date), function(d){rep(as.character(d), 2)})), Word = c(sapply(unique(Sandy$date), function(d){c("Hurricane", "Sandy")})), Count = NA)#
i = 1#
for (date in unique(Sandy$date)) {#
Dept[(2*(i-1)+1) : (2*i),3]= c(sum(Hurr[which(Hurr[,3] == date),1]), sum(Sandy[which(Sandy[,3] == date),1]))#
i = i + 1#
}#
	f <- ggplot(Dept, aes(Date, Count, colour = Word))#
	f + geom_line(aes(group = Word))+ guides(col = guide_legend(nrow=2))+ scale_x_discrete(breaks = function(n) n[seq(0, length(n), by = length(n)/20)])  +geom_vline(xintercept = 48, colour = "red", size = 1) +geom_vline(xintercept = 52, colour = "red", size = 1) + #
	geom_vline(xintercept = 45, colour = "red", size = 0.5, linetype = "dashed")+ annotate("text", x =50, y = 27, label = "Sandy", colour= "red", size = 3) + theme_bw() +#
	annotate("segment", x = 40, xend = 44, y = 15, yend = 14, colour = "red", size = 0.1, arrow = arrow()) + annotate("text", x =40, y = 15.5, label = "First Sandy", colour= "red", size = 3)
f <- ggplot(Dept, aes(Date, Count, colour = Word))#
	f + geom_line(aes(group = Word))+ guides(col = guide_legend(nrow=2))+ scale_x_discrete(breaks = function(n) n[seq(0, length(n), by = length(n)/10)]) +geom_vline(xintercept = 23, colour = "red", size = 1) +geom_vline(xintercept = 27, colour = "red", size = 1) + #
	geom_vline(xintercept = 20, colour = "red", size = 0.5, linetype = "dashed")+ annotate("text", x =25, y = 27, label = "Sandy", colour= "red" ) + #
	annotate("segment", x = 18, xend = 20, y = 20, yend = 19, colour = "red", size = 0.1, arrow = arrow()) + annotate("text", x =18, y = 20.5, label = "First Sandy", colour= "red", size = 3)
library(GGally)#
library(network)#
library(sna)#
#
Network = list()#
Network[[1]]= Dare$edge[which(Sandy$date %in% unique(Sandy$date)[1:18])] #pre-sandy#
Network[[2]]= Dare$edge[which(Sandy$date %in% unique(Sandy$date)[19:39])] #sandy #
Network[[3]]= Dare$edge[which(Sandy$date %in% unique(Sandy$date)[40:55])] #post-sandy#
ggplotColours <- function(n = 6, h = c(0, 360) + 15){#
  if ((diff(h) %% 360) < 1) h[2] <- h[2] - 360/n#
  hcl(h = (seq(h[1], h[2], length = n)), c = 100, l = 65)#
}#
titles = c("Pre-Sandy", "Sandy", "Post-Sandy")#
colors = getPalette(colourCount)#
names(colors) = (unique(sort(Dare$node[,3])))#
g= list()#
for (i in 1:3) {#
	edge= matrix(unlist(sapply(Network[[i]], function(d){cbind(rep(d[[1]], length(d[[2]])), d[[2]])})), ncol = 2)#
	net = as.network(edge, matrix.type = "edgelist", directed = TRUE)#
	network.vertex.names(net) = 1:27#
	net %v% "Dept" <- as.character(Dare$node[,3])#
	g[[i]] = ggnet2(net, size = "degree", arrow.size =8, color = colors[net %v% "Dept"],  color.legend = "Dept",label.color = colors[net %v% "Dept"], mode = "kamadakawai",legend.position = "none", size.min=1) + ggtitle(titles[i]) +  theme(panel.border = element_rect(color = "grey50", fill = NA),#
          aspect.ratio = 1)#
}#
grid.arrange <- getFromNamespace("grid.arrange", asNamespace("gridExtra"))#
grid.arrange(grobs = g, nrow = 1)
#Vance EDA#
load('/Users/bomin8319/Desktop/IPTM/paper/code/Vancenew.RData')#
# 762 - #
attach(Vance)#
Vance$text = Vance$text[1:length(Vance$edge)]#
Vance$edge = Vance$edge[1:length(Vance$edge)]#
Vance$edge = Vance$edge[-which(sapply(Vance$text, function(d){length(d)})==0)]#
Vance$text = Vance$text[-which(sapply(Vance$text, function(d){length(d)})==0)]#
#mintime = Dare$edge[[1]][[3]]#
#for (n in 1:length(Dare$edge)){#
#  Dare$edge[[n]][3] = (Dare$edge[[n]][[3]] - mintime) / 3600#
#}#
#
sender = sapply(Vance$edge, function(d){d[[1]]})#
sender = data.frame(sender=sender, dept =Vance$node[sender, 3], time = sapply(Vance$edge, function(d){d[[3]]}), date = anydate(sapply(Vance$edge, function(d){d[[3]]})))#
sender$date = format(sender$date, format = "%m/%d")#
Dept = data.frame(Date = c(sapply(unique(sender$date), function(d){rep(as.character(d), 17)})), #
				  Department = c(sapply(unique(sender$date), function(d){sort(unique(Vance$node[,3]))#
})), Send = NA)#
i = 1#
for (date in unique(sender$date)) {#
Dept[(17*(i-1)+1) : (17*i),3]= tabulate(sender[which(sender[,4] == date),2], 17)#
i = i + 1#
}#
ave = c()#
i = 1#
for (date in unique(Dept$Date)) {#
	ave[i] = mean(Dept[which(Dept$Date == date),3])#
	i = i + 1#
}#
colourCount = 17#
getPalette = colorRampPalette(brewer.pal(9, "Set1"))#
#
Dept2 = data.frame(Date = unique(Dept$Date), Send = ave)#
	f <- ggplot(Dept, aes(Date, Send, colour = Department), show.legend=FALSE)#
	f + geom_line(aes(group = Department)) + guides(col = guide_legend(nrow=17)) + scale_x_discrete(breaks = function(n) n[seq(0, length(n), by = length(n)/10)]) + theme(legend.text = element_text(size = 8)) +geom_vline(xintercept = 33, colour = "red", size = 1) +geom_vline(xintercept = 31, colour = "red", size = 1) + theme_bw()+#
	geom_vline(xintercept = 30, colour = "red", size = 0.5, linetype = "dashed")+ annotate("text", x =32, y = 5, label = "Sandy", colour= "red", size = 3 ) +#
	annotate("segment", x = 29, xend = 30, y = 4.2, yend = 4, colour = "red", size = 0.1, arrow = arrow()) + annotate("text", x =28, y = 4.3, label = "First Sandy", colour= "red", size = 3)  +scale_colour_manual(values = getPalette(colourCount))#
#+ stat_summary(aes(group = 1), size = 1.2, fun.y=mean, geom="line", colour = "grey35")
sender = sapply(Vance$edge, function(d){d[[1]]})#
sender = data.frame(sender=sender, dept =Vance$node[sender, 3], time = sapply(Vance$edge, function(d){d[[3]]}), date = anydate(sapply(Vance$edge, function(d){d[[3]]})))#
sender$date = format(sender$date, format = "%m/%d")#
Dept = data.frame(Date = c(sapply(unique(sender$date), function(d){rep(as.character(d), 17)})), #
				  Department = c(sapply(unique(sender$date), function(d){sort(unique(Vance$node[,3]))#
})), Send = NA)#
i = 1#
for (date in unique(sender$date)) {#
Dept[(17*(i-1)+1) : (17*i),3]= tabulate(sender[which(sender[,4] == date),2], 17)#
i = i + 1#
}#
ave = c()#
i = 1#
for (date in unique(Dept$Date)) {#
	ave[i] = mean(Dept[which(Dept$Date == date),3])#
	i = i + 1#
}#
colourCount = 17#
getPalette = colorRampPalette(brewer.pal(9, "Set1"))#
#
Dept2 = data.frame(Date = unique(Dept$Date), Send = ave)#
	f <- ggplot(Dept, aes(Date, Send, colour = Department), show.legend=FALSE)#
	f + geom_line(aes(group = Department)) + guides(col = guide_legend(nrow=17)) + scale_x_discrete(breaks = function(n) n[seq(0, length(n), by = length(n)/10)]) + theme(legend.text = element_text(size = 8)) +geom_vline(xintercept = 33, colour = "red", size = 1) +geom_vline(xintercept = 31, colour = "red", size = 1) + 	 annotate("text", x =32, y = 5, label = "Sandy", colour= "red", size = 3 ) +
annotate("segment", x = 29, xend = 30, y = 4.2, yend = 4, colour = "red", size = 0.1, arrow = arrow())   +scale_colour_manual(values = getPalette(colourCount))
Dept2 = data.frame(Date = unique(Dept$Date), Send = ave)#
	f <- ggplot(Dept, aes(Date, Send, colour = Department), show.legend=FALSE)#
	f + geom_line(aes(group = Department)) + guides(col = guide_legend(nrow=17)) + scale_x_discrete(breaks = function(n) n[seq(0, length(n), by = length(n)/10)]) + theme(legend.text = element_text(size = 8)) +geom_vline(xintercept = 33, colour = "red", size = 1) +geom_vline(xintercept = 31, colour = "red", size = 1) + 	 annotate("text", x =32, y = 5, label = "Sandy", colour= "red", size = 3 )  +scale_colour_manual(values = getPalette(colourCount))
Dept2 = data.frame(Date = unique(Dept$Date), Send = ave)#
	f <- ggplot(Dept, aes(Date, Send, colour = Department), show.legend=FALSE)#
	f + geom_line(aes(group = Department)) + guides(col = guide_legend(nrow=17)) + scale_x_discrete(breaks = function(n) n[seq(0, length(n), by = length(n)/10)]) + theme(legend.text = element_text(size = 10)) +geom_vline(xintercept = 33, colour = "red", size = 1) +geom_vline(xintercept = 31, colour = "red", size = 1) + 	 annotate("text", x =32, y = 5, label = "Sandy", colour= "red", size = 3 )  +scale_colour_manual(values = getPalette(colourCount))#
#+ stat_summary(aes(group = 1), size = 1.2, fun.y=mean, geom="line", colour = "grey35") #
#geom_vline(xintercept = 30, colour = "red", size = 0.5, linetype = "dashed")+#
#+ annotate("text", x =28, y = 4.3, label = "First Sandy", colour= "red", size = 3)#
#	annotate("segment", x = 29, xend = 30, y = 4.2, yend = 4, colour = "red", size = 0.1, arrow = arrow())
receiver = unlist(sapply(Vance$edge, function(d){d[[2]]}))#
time = unlist(sapply(Vance$edge, function(d){rep(d[[3]], length(d[[2]]))}))#
date = anydate(unlist(sapply(Vance$edge, function(d){rep(d[[3]], length(d[[2]]))})))#
receiver  = data.frame(receiver =receiver, dept = Vance$node[receiver , 3], time = time, date = date)#
receiver$date = format(receiver$date, format = "%m/%d")#
Dept = data.frame(Date = c(sapply(unique(receiver$date), function(d){rep(as.character(d), 17)})), #
				  Department = c(sapply(unique(receiver$date), function(d){sort(unique(Vance$node[,3]))#
})), Receive = NA)#
i = 1#
for (date in unique(receiver$date)) {#
Dept[(17*(i-1)+1) : (17*i),3]= tabulate(receiver[which(receiver[,4] == date),2], 17)#
i = i + 1#
}#
#
	f <- ggplot(Dept, aes(Date, Receive, colour = Department))#
	f + geom_line(aes(group = Department)) + guides(col = guide_legend(nrow=17)) +scale_x_discrete(breaks = function(n) n[seq(0, length(n), by = length(n)/10)]) + theme(legend.text = element_text(size = 8)) +geom_vline(xintercept = 33, colour = "red", size = 1) +geom_vline(xintercept = 31, colour = "red", size = 1) +annotate("text", x =32, y = 6, label = "Sandy", colour= "red", size = 3 ) + annotate("text", x =28, y = 5.3, label = "First Sandy", colour= "red", size = 3)  +scale_colour_manual(values = getPalette(colourCount))
f <- ggplot(Dept, aes(Date, Receive, colour = Department))#
	f + geom_line(aes(group = Department)) + guides(col = guide_legend(nrow=17)) +scale_x_discrete(breaks = function(n) n[seq(0, length(n), by = length(n)/10)]) + theme(legend.text = element_text(size = 8)) +geom_vline(xintercept = 33, colour = "red", size = 1) +geom_vline(xintercept = 31, colour = "red", size = 1) +annotate("text", x =32, y = 6, label = "Sandy", colour= "red", size = 3 )  +scale_colour_manual(values = getPalette(colourCount))#
#
#+ stat_summary(aes(group = 1), size = 1.2, fun.y=mean, geom="line", colour = "grey35")+
f <- ggplot(Dept, aes(Date, Receive, colour = Department))#
	f + geom_line(aes(group = Department)) + guides(col = guide_legend(nrow=17)) +scale_x_discrete(breaks = function(n) n[seq(0, length(n), by = length(n)/10)]) + theme(legend.text = element_text(size = 10)) +geom_vline(xintercept = 33, colour = "red", size = 1) +geom_vline(xintercept = 31, colour = "red", size = 1) +annotate("text", x =32, y = 6, label = "Sandy", colour= "red", size = 3 )  +scale_colour_manual(values = getPalette(colourCount))
Sandy = sapply(Vance$text, function(d){sum(244 == d)})#
Sandy = data.frame(Sandy=Sandy, time = sapply(Vance$edge, function(d){d[[3]]}), date = anydate(sapply(Vance$edge, function(d){d[[3]]})))#
Sandy$date = format(Sandy$date, format = "%m/%d")#
Hurr = sapply(Vance$text, function(d){sum(243 == d)})#
Hurr = data.frame(Hurr =Hurr , time = sapply(Vance$edge, function(d){d[[3]]}), date = anydate(sapply(Vance$edge, function(d){d[[3]]})))#
Hurr $date = format(Hurr$date, format = "%m/%d")#
#
Dept = data.frame(Date = c(sapply(unique(Sandy$date), function(d){rep(as.character(d), 2)})), Word = c(sapply(unique(Sandy$date), function(d){c("Hurricane", "Sandy")})), Count = NA)#
i = 1#
for (date in unique(Sandy$date)) {#
Dept[(2*(i-1)+1) : (2*i),3]= c(sum(Hurr[which(Hurr[,3] == date),1]), sum(Sandy[which(Sandy[,3] == date),1]))#
i = i + 1#
}#
	f <- ggplot(Dept, aes(Date, Count, colour = Word))#
	f + geom_line(aes(group = Word))+ guides(col = guide_legend(nrow=2))+scale_x_discrete(breaks = function(n) n[seq(0, length(n), by = length(n)/10)]) + theme(legend.text = element_text(size = 8)) +geom_vline(xintercept = 33, colour = "red", size = 1) +geom_vline(xintercept = 31, colour = "red", size = 1) + theme_bw()+#
	geom_vline(xintercept = 30, colour = "red", size = 0.5, linetype = "dashed")+ annotate("text", x =32, y = 4.5, label = "Sandy", colour= "red", size = 3 ) +#
	annotate("segment", x = 29, xend = 30, y = 4.2, yend = 4, colour = "red", size = 0.1, arrow = arrow()) + annotate("text", x =28, y = 4.3, label = "First Sandy", colour= "red", size = 3)
f <- ggplot(Dept, aes(Date, Count, colour = Word))#
	f + geom_line(aes(group = Word))+ guides(col = guide_legend(nrow=2))+scale_x_discrete(breaks = function(n) n[seq(0, length(n), by = length(n)/10)]) + theme(legend.text = element_text(size = 8)) +geom_vline(xintercept = 33, colour = "red", size = 1) +geom_vline(xintercept = 31, colour = "red", size = 1) + annotate("text", x =32, y = 4.5, label = "Sandy", colour= "red", size = 3 ) +#
	annotate("segment", x = 29, xend = 30, y = 4.2, yend = 4, colour = "red", size = 0.1, arrow = arrow())
f <- ggplot(Dept, aes(Date, Count, colour = Word))#
	f + geom_line(aes(group = Word))+ guides(col = guide_legend(nrow=2))+scale_x_discrete(breaks = function(n) n[seq(0, length(n), by = length(n)/10)]) + theme(legend.text = element_text(size = 8)) +geom_vline(xintercept = 33, colour = "red", size = 1) +geom_vline(xintercept = 31, colour = "red", size = 1) + annotate("text", x =32, y = 4.5, label = "Sandy", colour= "red", size = 3 )
library(GGally)#
library(network)#
library(sna)#
#
Network = list()#
Network[[1]]= Vance$edge[which(Sandy$date %in% unique(Sandy$date)[1:28])] #pre-sandy#
Network[[2]]= Vance$edge[which(Sandy$date %in% unique(Sandy$date)[29:35])] #sandy #
Network[[3]]= Vance$edge[which(Sandy$date %in% unique(Sandy$date)[36:48])] #post-sandy#
ggplotColours <- function(n = 6, h = c(0, 360) + 15){#
  if ((diff(h) %% 360) < 1) h[2] <- h[2] - 360/n#
  hcl(h = (seq(h[1], h[2], length = n)), c = 100, l = 65)#
}#
titles = c("Pre-Sandy", "Sandy", "Post-Sandy")#
colors = getPalette(colourCount)#
names(colors) = (unique(sort(Vance$node[,3])))#
g= list()#
for (i in 1:3) {#
	edge= matrix(unlist(sapply(Network[[i]], function(d){cbind(rep(d[[1]], length(d[[2]])), d[[2]])})), ncol = 2)#
	net = as.network(edge, matrix.type = "edgelist", directed = TRUE)#
	network.vertex.names(net) = 1:27#
	net %v% "Dept" <- as.character(Vance$node[,3])#
	g[[i]] = ggnet2(net, size = "degree", arrow.size =8, color = colors[net %v% "Dept"],  color.legend = "Dept",label.color = colors[net %v% "Dept"], mode = "kamadakawai",legend.position = "none", size.min=1) + ggtitle(titles[i]) +  theme(panel.border = element_rect(color = "grey50", fill = NA),#
          aspect.ratio = 1)#
}#
grid.arrange <- getFromNamespace("grid.arrange", asNamespace("gridExtra"))#
grid.arrange(grobs = g, nrow = 1)
load('~/Desktop/output55.RData')
plot(output$D, type = 'l')
plot(output$B[[1]][1,], type = 'l')
plot(output$B[[2]][1,], type = 'l')
plot(output$B[[2]][2,], type = 'l')
plot(output$B[[1]][2,], type = 'l')
plot(output$B[[1]][3,], type = 'l')
plot(output$B[[2]][3,], type = 'l')
plot(output$B[[3]][3,], type = 'l')
plot(output$B[[2]][4,], type = 'l')
plot(output$B[[2]][5,], type = 'l')
plot(output$B[[2]][6,], type = 'l')
plot(output$B[[2]][7,], type = 'l')
plot(output$B[[2]][8,], type = 'l')
plot(output$B[[2]][9,], type = 'l')
plot(output$B[[2]][10,], type = 'l')
plot(output$B[[2]][11,], type = 'l')
plot(output$B[[2]][12,], type = 'l')
plot(output$B[[2]][13,], type = 'l')
plot(output$B[[2]][14,], type = 'l')
plot(output$B[[2]][15,], type = 'l')
plot(output$B[[2]][16,], type = 'l')
plot(output$B[[2]][17,], type = 'l')
plot(output$B[[2]][18,], type = 'l')
plot(output$B[[2]][19,], type = 'l')
plot(output$B[[2]][20,], type = 'l')
plot(output$B[[2]][21,], type = 'l')
plot(output$B[[2]][22,], type = 'l')
plot(output$B[[2]][23,], type = 'l')
plot(output$B[[2]][24,], type = 'l')
plot(output$B[[2]][25,], type = 'l')
plot(output$B[[1]][25,], type = 'l')
plot(output$B[[1]][24,], type = 'l')
plot(output$B[[1]][23,], type = 'l')
plot(output$B[[1]][22,], type = 'l')
plot(output$B[[1]][21,], type = 'l')
plot(output$B[[1]][20,], type = 'l')
plot(output$B[[1]][19,], type = 'l')
plot(output$B[[1]][18,], type = 'l')
plot(output$B[[1]][17,], type = 'l')
plot(output$B[[1]][16,], type = 'l')
plot(output$B[[1]][15,], type = 'l')
plot(output$B[[1]][14,], type = 'l')
plot(output$B[[1]][13,], type = 'l')
plot(output$B[[1]][12,], type = 'l')
plot(output$B[[1]][11,], type = 'l')
plot(output$B[[1]][10,], type = 'l')
plot(output$B[[1]][9,], type = 'l')
plot(output$B[[1]][8,], type = 'l')
plot(output$B[[1]][7,], type = 'l')
plot(output$B[[1]][6,], type = 'l')
plot(output$B[[1]][5,], type = 'l')
plot(output$B[[1]][4,], type = 'l')
plot(output$B[[1]][3,], type = 'l')
plot(output$B[[1]][2,], type = 'l')
plot(output$B[[1]][1,], type = 'l')
plot(output$B[[2]][1,], type = 'l')
#Dare EDA#
library(anytime)#
library(ggplot2)#
library(MCMCpack)#
library(reshape2)#
library(gridExtra)#
library(ggrepel)#
library(RColorBrewer)#
library(lda)
load('/Users/bomin8319/Desktop/IPTM/paper/code/Darenew.RData')#
# 762 - #
attach(Dare)#
Dare$node = 1:nrow(Dare$node)#
Dare$text = Dare$text[762:length(Dare$edge)]#
Dare$edge = Dare$edge[762:length(Dare$edge)]#
Dare$edge = Dare$edge[-which(sapply(Dare$text, function(d){length(d)})==0)]#
Dare$text = Dare$text[-which(sapply(Dare$text, function(d){length(d)})==0)]#
mintime = Dare$edge[[1]][[3]]#
for (n in 1:length(Dare$edge)){#
  Dare$edge[[n]][3] = (Dare$edge[[n]][[3]] - mintime) / 3600#
}#
Dare$edge = lapply(Dare$edge, function(x){x[1:3]})
Daretest1 = output#
Daretest1$C
TableWord = function(Zchain, K, textlist, vocabulary) {#
  # Generate a table of token-topic assignments with high probabilities for each IP#
  ##
  # Args #
  #  Zchain summary of Z obtained using MCMC function#
  #  K total number of topics specified by the user#
  #  textlist list of text containing the words in each document#
  #  vocabulary all vocabularies used over the corpus#
  ##
  # Returns#
  #  List of table that summarize token-topic assignments for each IP#
  W = length(vocabulary)#
    Zsummary = list()#
    topic.word = matrix(0, nrow = K, ncol = W)#
    colnames(topic.word) = vocabulary#
    iter = 1#
    for (d in seq(along = textlist)) {#
      if (length(Zchain[[d]]) > 0){#
        Zsummary[[iter]] = Zchain[[d]]#
        names(Zsummary[[iter]])<- vocabulary[textlist[[d]]]#
        iter = iter+1#
      }#
    }#
    topic.dist = t(tabulate(unlist(Zsummary), K)/length(unlist(Zsummary)))#
    colnames(topic.dist) = c(1L:K)#
    top.topic = topic.dist[, order(topic.dist, decreasing = TRUE)]#
    all.word = unlist(Zsummary)#
    for (i in seq(along = all.word)){#
      matchWZ = which(colnames(topic.word) == names(all.word[i]))#
      topic.word[all.word[i], matchWZ] = topic.word[all.word[i], matchWZ] + 1#
    }#
    table.word = top.topic.words(topic.word, num.words = 15, by.score = TRUE)#
    colnames(table.word) = names(top.topic)#
  return(table.word)#
}#
TableWord(Daretest1$Z, 20, Dare$text, Dare$vocab)
table(unlist(Daretest1$Z)) / sum(table(unlist(Daretest1$Z)))
dim(Daretest1$B[[1]])
DareB = matrix(NA, 500, 25)#
DareB[,1:25] = t(Daretest1$B[[1]])#
colnames(DareB)= c( "intercept",#
"outdegree1", "outdegree2", "outdegree3", "indegree1", "indegree2", "indegree3",#
"send1", "send2", "send3" ,"receive1", "receive2", "receive3",#
"2-send1", "2-send2", "2-send3", "2-receive1", "2-receive2" ,"2-receive3",#
"sibling1", "sibling2" ,"sibling3", "cosibling1", "cosibling2", "cosibling3")#
DareB = melt(DareB)#
#
DareB2 = matrix(NA, 500, 25)#
DareB2[,1:25] = t(Daretest1$B[[2]])#
colnames(DareB2)= c( "intercept",#
"outdegree1", "outdegree2", "outdegree3", "indegree1", "indegree2", "indegree3",#
"send1", "send2", "send3" ,"receive1", "receive2", "receive3",#
"2-send1", "2-send2", "2-send3", "2-receive1", "2-receive2" ,"2-receive3",#
"sibling1", "sibling2" ,"sibling3", "cosibling1", "cosibling2", "cosibling3")#
DareB2 = melt(DareB2)#
#
DareB$IP = 1#
DareB2$IP = 2#
DareBnew = rbind(DareB, DareB2)[,-1]#
DareBnew$IP = as.factor(DareBnew$IP)#
colnames(DareBnew) = c("Netstat", "Estimate", "IP")#
DareBnew$Netstat = factor(DareBnew$Netstat, levels =  c( "intercept",#
"outdegree1", "outdegree2", "outdegree3", "indegree1", "indegree2", "indegree3",#
"send1", "send2", "send3" ,"receive1", "receive2", "receive3",#
"2-send1", "2-send2", "2-send3", "2-receive1", "2-receive2" ,"2-receive3",#
"sibling1", "sibling2" ,"sibling3", "cosibling1", "cosibling2", "cosibling3"))#
colnames(DareBnew) = c("Netstat", "Estimate", "IP")#
p <- ggplot(DareBnew, aes(Netstat, Estimate, colour = IP)) + geom_boxplot(aes(colour = factor(IP)), position = position_dodge()) + coord_flip() + geom_hline(yintercept = 0.0, colour = "black", size = 0.5)
p
sum(table(unlist(Daretest1$Z)) / sum(table(unlist(Daretest1$Z)))[c(1,3,5,7,9,11,13,15,17,19)]
)
sum((table(unlist(Daretest1$Z)) / sum(table(unlist(Daretest1$Z))))[c(1,3,5,7,9,11,13,15,17,19)]
)
sum((table(unlist(Daretest1$Z)) / sum(table(unlist(Daretest1$Z))))[c(2,4,6,8,10,12,14,16,18,20)])
TableWord = function(Zchain, K, textlist, vocabulary) {#
  # Generate a table of token-topic assignments with high probabilities for each IP#
  ##
  # Args #
  #  Zchain summary of Z obtained using MCMC function#
  #  K total number of topics specified by the user#
  #  textlist list of text containing the words in each document#
  #  vocabulary all vocabularies used over the corpus#
  ##
  # Returns#
  #  List of table that summarize token-topic assignments for each IP#
  W = length(vocabulary)#
    Zsummary = list()#
    topic.word = matrix(0, nrow = K, ncol = W)#
    colnames(topic.word) = vocabulary#
    iter = 1#
    for (d in seq(along = textlist)) {#
      if (length(Zchain[[d]]) > 0){#
        Zsummary[[iter]] = Zchain[[d]]#
        names(Zsummary[[iter]])<- vocabulary[textlist[[d]]]#
        iter = iter+1#
      }#
    }#
    topic.dist = t(tabulate(unlist(Zsummary), K)/length(unlist(Zsummary)))#
    colnames(topic.dist) = c(1L:K)#
    top.topic = topic.dist[, order(topic.dist, decreasing = TRUE)]#
    all.word = unlist(Zsummary)#
    for (i in seq(along = all.word)){#
      matchWZ = which(colnames(topic.word) == names(all.word[i]))#
      topic.word[all.word[i], matchWZ] = topic.word[all.word[i], matchWZ] + 1#
    }#
    table.word = top.topic.words(topic.word, num.words = 15, by.score = TRUE)#
    colnames(table.word) = names(top.topic)#
  return(table.word)#
}#
TableWord(Daretest1$Z, 20, Dare$text, Dare$vocab)
library(mcmcse)
mcmcse
mcse
###############
####Set Up#####
###############
#
#clear memory#
rm( list=ls() )#
#
#load necessary libraries 						                                 #
library(foreign)#
library(Zelig)#
library(car)#
library(MASS)#
library(VGAM)#
library(plotrix)#
library(pscl)#
library(survival)#
library(msm)#
library(verification)#
library(corpcor)#
library(Design)#
library(coda)#
library(mcmcse)#
#library(devtools)#
#install_github('bomin8319/BayesOFsurv/pkg')#
library(BayesOFsurv)#
#set working directory#
setwd("/Users/bomin8319/Desktop/BayesOFsurv/coding material/Monte Carlos/Mixture DGP/")#
#
###########################################################################
###########################################################################
############################Monte Carlo####################################
###########################################################################
#
#set seed#
set.seed(3)   #
#
#set the number of observations#
n<-1000#
#
#set the number of simulations, and create matrices to store the results#
nsims<-1000
i = 1
#history matrix for true estimates#
tru.est<-matrix(NA,nrow=nsims,ncol=8)#
#history matrix for cox estimates#
cox.est<-matrix(NA,nrow=nsims,ncol=2)#
#history matrix for exp estimates#
exp.est<-matrix(NA,nrow=nsims,ncol=24)#
#history matrix for weibull estimates#
weib.est<-matrix(NA,nrow=nsims,ncol=30)#
#history matrix for cox RMSE#
cox.rmse<-matrix(NA,nrow=nsims,ncol=1)#
#history matrix for exp RMSE#
exp.rmse<-matrix(NA,nrow=nsims,ncol=12)#
#history matrix for exp RMSE#
weib.rmse<-matrix(NA,nrow=nsims,ncol=15)#
#history matrix for cox CP#
cox.cp<-matrix(NA,nrow=nsims,ncol=1)#
#history matrix for exp CP#
exp.cp<-matrix(NA,nrow=nsims,ncol=12)#
#history matrix for exp CP#
weib.cp<-matrix(NA,nrow=nsims,ncol=15)#
main.data<-cbind(tru.est,cox.est,exp.est,weib.est,cox.rmse,exp.rmse,weib.rmse,cox.cp,exp.cp,weib.cp)#
colnames(main.data)<-c("true.x0","true.x1","true.z0","true.z1","true.z2","true.p","cen.lat","cen.obs",#
                       "cox.x1","cox.x1.se",#
                       "exp.x0","exp.x0.se","exp.x1","exp.x1.se",#
                       "zexp.z0","zexp.z0.se","zexp.z1","zexp.z1.se","zexp.z2","zexp.z2.se","zexp.x0","zexp.x0.se","zexp.x1","zexp.x1.se",#
                       "bzexp.x0","zexp.x0.se","bzexp.x1","bzexp.x1.se","bzexp.z0","bzexp.z0.se","bzexp.z1","bzexp.z1.se","bzexp.z2","bzexp.z2.se",#
                       "wei.x0","wei.x0.se","wei.x1","wei.x1.se","wei.p","wei.p.se",#
                       "zwei.z0","zwei.z0.se","zwei.z1","zwei.z1.se","zwei.z2","zwei.z2.se","zwei.x0","zwei.x0.se","zwei.x1","zwei.x1.se","zwei.p","zwei.p.se",#
                       "bzwei.x0","bzwei.x0.se","bzwei.x1","bzwei.x1.se","bzwei.z0","bzwei.z0.se","bzwei.z1","bzwei.z1.se","bzwei.z2","bzwei.z2.se","bzwei.p","bzwei.p.se",#
                       "cox.x1.rmse",#
                       "exp.x0.rmse","exp.x1.rmse","zexp.z0.rmse","zexp.z1.rmse","zexp.z2.rmse","zexp.x0.rmse","zexp.x1.rmse","bzexp.x0.rmse","bzexp.x1.rmse","bzexp.z0.rmse","bzexp.z1.rmse","bzexp.z2.rmse",#
                       "wei.x0.rmse","wei.x1.rmse","wei.p.rmse","zwei.z0.rmse","zwei.z1.rmse","zwei.z2.rmse",#
                       "zwei.x0.rmse","zwei.x1.rmse","zwei.p.rmse", "bzwei.x0.rmse","bzwei.x1.rmse","bzwei.z0.rmse","bzwei.z1.rmse","bzwei.z2.rmse","bzwei.p.rmse",#
                       "cox.x1.cp","exp.x0.cp","exp.x1.cp","zexp.z0.cp","zexp.z1.cp","zexp.z2.cp","zexp.x0.cp","zexp.x1.cp","bzexp.x0.cp","bzexp.x1.cp","bzexp.z0.cp","bzexp.z1.cp","bzexp.z2.cp",#
                       "wei.x0.cp","wei.x1.cp","wei.p.cp",#
                       "zwei.z0.cp","zwei.z1.cp","zwei.z2.cp","zwei.x0.cp","zwei.x1.cp","zwei.p.cp", "bzwei.x0.cp","bzwei.x1.cp","bzwei.z0.cp","bzwei.z1.cp","bzwei.z2.cp","bzwei.p.cp")#
#
#create covariates#
x<-runif(n, min=-2.5, max=12)#
z<-log(runif(n, min=1, max=100))
#Assign parameter values#
tru.est[i,1]<-1#
tru.est[i,2]<-3.5#
tru.est[i,3]<--2#
tru.est[i,4]<-2#
tru.est[i,5]<-3#
tru.est[i,6]<-1#
#
myrates <- exp(tru.est[i,1]+(tru.est[i,2]*x)) #
y <- rexp(n, rate = myrates) # generates the r.v.#
cen <- rexp(n, rate = 1 )#
ycen <- pmin(y, cen)#
di <- as.numeric(y <= cen)#
tru.est[i,7]<-table(di)[1]#
#
#create parameters for ZG#
phi<-1/(1+exp(-(tru.est[i,3]+tru.est[i,4]*z+tru.est[i,5]*x)))#
print(mean(phi))#
yzero<-matrix(1,n,1)#
error<--1*rlogis(n)#
flag<-error<qlogis(phi)#
yzero[flag]<-error[flag]#
flag<-yzero==1#
di[flag]<-ifelse(di[flag]==0,yzero[flag],di[flag])#
tru.est[i,8]<-table(di)[1]#
#
data<-cbind(ycen,di,x,z)
######################################################################################
###################################COX Model##########################################
######################################################################################
#
#store estimate and se#
cox.est[i,1]<-summary(coxph(Surv(ycen, di)~x,coxph.control(iter.max = 10000)))$coef[1]#
cox.est[i,2]<-summary(coxph(Surv(ycen, di)~x,coxph.control(iter.max = 10000)))$coef[3]#
#
#store rmse#
cox.rmse[i,1]<-sqrt((tru.est[i,2]-cox.est[i,1])^2)#
#
#calculate upper and lower 95% CI's#
b1.lower<-cox.est[i,1]-(1.959964*cox.est[i,2])#
b1.upper<-cox.est[i,1]+(1.959964*cox.est[i,2])#
#
#store coverage parameters#
cox.cp[i,1]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
##############################################################################
########################Simple Exponential Model##############################
##############################################################################
Exponential<- function(est,Y,C,X,data) {					      #
	n=nrow(data)							      					  #
	llik <- matrix(0, nrow=n, ncol = 1)#
	beta<-est[1:length(est)]#
	XB<-X%*%beta#
	llik<-C*(XB-exp(XB)*Y)+(1-C)*(-exp(XB)*Y)#
	llik<--1*sum(llik)#
	return(llik)#
	}#
#set starting parameters#
est<-rbind(.01,.01)#
#
#set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
#optimize#
output.Exponential<-try(optim(f=Exponential,  p=est, X=X,Y=Y,C=C, method="BFGS", control=list(maxit=10000),  data=data, hessian=TRUE), TRUE)#
#
if(class(output.Exponential)=="list"){#
	ifelse(is.positive.definite(output.Exponential$hessian)==TRUE,vcv<-solve(output.Exponential$hessian),vcv<-matrix(data=NA,nrow=2,ncol=2))#
#
#store betas and ses#
exp.est[i,1]<-output.Exponential$par[1]#
exp.est[i,2]<-sqrt(vcv[1,1])#
exp.est[i,3]<-output.Exponential$par[2]#
exp.est[i,4]<-sqrt(vcv[2,2])#
#
#store rmse#
exp.rmse[i,1]<-sqrt((tru.est[i,1]-exp.est[i,1])^2)#
exp.rmse[i,2]<-sqrt((tru.est[i,2]-exp.est[i,3])^2)#
#
#calculate upper and lower 95% CI's#
b0.lower<-exp.est[i,1]-(1.959964*exp.est[i,2])#
b0.upper<-exp.est[i,1]+(1.959964*exp.est[i,2])#
b1.lower<-exp.est[i,3]-(1.959964*exp.est[i,4])#
b1.upper<-exp.est[i,3]+(1.959964*exp.est[i,4])#
#store coverage parameters#
exp.cp[i,1]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
exp.cp[i,2]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
#
}#
#
#################################################################################
#########################Simple Weibull Model ###################################
#################################################################################
#
#Note this estiamtes the model via hazard rates, a la Stata#
#
test<-survreg(Surv(ycen, di)~x, dist="weibull")#
summary(test)#
Weibull<- function(est,Y,C,X,data) {					      #
	n=nrow(data)							      					  #
	llik <- matrix(0, nrow=n, ncol = 1)#
	beta<-est[1:length(est)-1]#
	p<-est[length(est)]#
	p<-exp(p)#
	XB<-X%*%beta#
	llik<-C*(log(exp(XB+1/p)*p*((exp(XB+1/p)*Y)^(p-1))*exp(-(exp(XB+1/p)*Y)^p)))+(1-C)*log(exp(-(exp(XB+1/p)*Y)^p))#
	llik<--1*sum(llik)#
	return(llik)#
	}#
#set starting parameters#
est<-rbind(exp.est[i,1],exp.est[i,3],.01)#
#
#set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
#optimize#
output.Weibull<-try(optim(f=Weibull,  p=est, X=X,Y=Y,C=C, method="BFGS", control=list(maxit=10000),  data=data, hessian=TRUE), TRUE)#
#
if(class(output.Weibull)=="list"){#
	ifelse(is.positive.definite(output.Weibull$hessian)==TRUE,vcv<-solve(output.Weibull$hessian),vcv<-matrix(data=NA,nrow=3,ncol=3))#
#
#store betas and ses#
weib.est[i,1]<-output.Weibull$par[1]+1/exp(output.Weibull$par[3])#
coeff<-c(weib.est[i,1],output.Weibull$par[3])#
varcov<-matrix(NA,2,2)#
varcov[1,1]<-vcv[1,1]#
varcov[1,2]<-vcv[1,3]#
varcov[2,1]<-vcv[3,1]#
varcov[2,2]<-vcv[3,3]#
weib.est[i,2]<-deltamethod(~(x1+1/exp(x2)), coeff, varcov, ses=TRUE)#
weib.est[i,3]<-output.Weibull$par[2]#
weib.est[i,4]<-sqrt(vcv[2,2])#
weib.est[i,5]<-exp(output.Weibull$par[3])#
coeff<-c(weib.est[i,5])#
varcov<-matrix(NA,1,1)#
varcov[1,1]<-vcv[3,3]#
weib.est[i,6]<-deltamethod(~(exp(x1)), coeff, varcov, ses=TRUE)#
#store rmse#
weib.rmse[i,1]<-sqrt((tru.est[i,1]-weib.est[i,1])^2)#
weib.rmse[i,2]<-sqrt((tru.est[i,2]-weib.est[i,3])^2)#
weib.rmse[i,3]<-sqrt((tru.est[i,6]-weib.est[i,5])^2)#
#
#calculate upper and lower 95% CI's#
b0.lower<-weib.est[i,1]-(1.959964*weib.est[i,2])#
b0.upper<-weib.est[i,1]+(1.959964*weib.est[i,2])#
b1.lower<-weib.est[i,3]-(1.959964*weib.est[i,4])#
b1.upper<-weib.est[i,3]+(1.959964*weib.est[i,4])#
p.lower<-weib.est[i,5]-(1.959964*weib.est[i,6])#
p.upper<-weib.est[i,5]+(1.959964*weib.est[i,6])#
#
#store coverage parameters#
weib.cp[i,1]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
weib.cp[i,2]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
weib.cp[i,3]<-ifelse(tru.est[i,6]>p.lower & tru.est[i,6]<p.upper, 1,0)#
#
}#
#
###logit estimates####
dataset<-as.data.frame(data)#
logitcoef1<-glm(di~ z+x, data = dataset, family = "binomial")$coef[1]#
logitcoef2<-glm(di~ z+x, data = dataset, family = "binomial")$coef[2]#
logitcoef3<-glm(di~ z+x, data = dataset, family = "binomial")$coef[3]#
#
################################################################################
##########################Zombie Exponential Model##############################
################################################################################
#This program estimates the Exponential loglikelihood function returning hazard rate form coefficients#
#
ZExponential<- function(est,Y,C,X,Z,data) {					      #
	n=nrow(data)							      					  #
	llik <- matrix(0, nrow=n, ncol = 1)#
	gamma<-est[1:ncol(Z)]#
	beta<-est[(ncol(Z)+1):length(est)]#
	XB<-X%*%beta#
	ZG<-Z%*%gamma#
	phi<-1/(1+exp(-ZG))#
	llik<-C*(log((1-phi)+phi*exp(XB)*exp(-exp(XB)*Y)))+(1-C)*(log(phi)+-exp(XB)*Y)#
	llik<--1*sum(llik)#
	return(llik)#
	}#
#set starting parameters#
est<-rbind(.01,.01,.01,exp.est[i,1],exp.est[i,3])#
#
#set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
Z<-cbind(1,z,x)#
#optimize#
output.ZExponential<-try(optim(f=ZExponential,  p=est, X=X,Y=Y,C=C,Z=Z, method="BFGS", control=list(maxit=10000),  data=data, hessian=TRUE), TRUE)#
#
if(class(output.ZExponential)=="list"){#
	ifelse(is.positive.definite(output.ZExponential$hessian)==TRUE,vcv<-solve(output.ZExponential$hessian),vcv<-matrix(data=NA,nrow=5,ncol=5))#
#
#store betas and ses#
exp.est[i,5]<-output.ZExponential$par[1]#
exp.est[i,6]<-sqrt(vcv[1,1])#
exp.est[i,7]<-output.ZExponential$par[2]#
exp.est[i,8]<-sqrt(vcv[2,2])#
exp.est[i,9]<-output.ZExponential$par[3]#
exp.est[i,10]<-sqrt(vcv[3,3])#
exp.est[i,11]<-output.ZExponential$par[4]#
exp.est[i,12]<-sqrt(vcv[4,4])#
exp.est[i,13]<-output.ZExponential$par[5]#
exp.est[i,14]<-sqrt(vcv[5,5])#
#
#store rmse#
exp.rmse[i,3]<-sqrt((tru.est[i,3]-exp.est[i,5])^2)#
exp.rmse[i,4]<-sqrt((tru.est[i,4]-exp.est[i,7])^2)#
exp.rmse[i,5]<-sqrt((tru.est[i,5]-exp.est[i,9])^2)#
exp.rmse[i,6]<-sqrt((tru.est[i,1]-exp.est[i,11])^2)#
exp.rmse[i,7]<-sqrt((tru.est[i,2]-exp.est[i,13])^2)#
#
#calculate upper and lower 95% CI's#
g0.lower<-exp.est[i,5]-(1.959964*exp.est[i,6])#
g0.upper<-exp.est[i,5]+(1.959964*exp.est[i,6])#
g1.lower<-exp.est[i,7]-(1.959964*exp.est[i,8])#
g1.upper<-exp.est[i,7]+(1.959964*exp.est[i,8])#
g2.lower<-exp.est[i,9]-(1.959964*exp.est[i,10])#
g2.upper<-exp.est[i,9]+(1.959964*exp.est[i,10])#
b0.lower<-exp.est[i,11]-(1.959964*exp.est[i,12])#
b0.upper<-exp.est[i,11]+(1.959964*exp.est[i,12])#
b1.lower<-exp.est[i,13]-(1.959964*exp.est[i,14])#
b1.upper<-exp.est[i,13]+(1.959964*exp.est[i,14])#
#store coverage parameters#
exp.cp[i,3]<-ifelse(tru.est[i,3]>g0.lower & tru.est[i,3]<g0.upper, 1,0)#
exp.cp[i,4]<-ifelse(tru.est[i,4]>g1.lower & tru.est[i,4]<g1.upper, 1,0)#
exp.cp[i,5]<-ifelse(tru.est[i,5]>g2.lower & tru.est[i,5]<g2.upper, 1,0)#
exp.cp[i,6]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
exp.cp[i,7]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
#
}#
#
######################################################################################
##########################Zombie Weibull Model #######################################
######################################################################################
#This program estimates the Exponential loglikelihood function returning hazard rate form coefficients#
#
ZWeibull<- function(est,Y,C,X,Z,data) {					      #
	n=nrow(data)							      					  #
	llik <- matrix(0, nrow=n, ncol = 1)#
	gamma<-est[1:ncol(Z)]#
	beta<-est[(ncol(Z)+1):(length(est)-1)]#
	p<-est[length(est)]#
	p<-exp(p)#
	XB<-X%*%beta#
	ZG<-Z%*%gamma#
	phi<-1/(1+exp(-(ZG+1/p)))#
	llik<-C*(log((1-phi)+phi*exp(XB+1/p)*p*((exp(XB+1/p)*Y)^(p-1))*exp(-(exp(XB+1/p)*Y)^p)))+(1-C)*(log(phi)+-(exp(XB+1/p)*Y)^p)#
	llik<--1*sum(llik)#
	return(llik)#
	}#
#set starting parameters#
est<-rbind(.01,.01,.01,output.Weibull$par[1],output.Weibull$par[2],output.Weibull$par[3])#
#
#set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
Z<-cbind(1,z,x)#
#optimize#
output.ZWeibull<-try(optim(f=ZWeibull,  p=est, X=X,Y=Y,C=C,Z=Z, method="BFGS", control=list(maxit=10000),  data=data, hessian=TRUE), TRUE)#
#
if(class(output.ZWeibull)=="list"){#
	ifelse(is.positive.definite(output.ZWeibull$hessian)==TRUE,vcv<-solve(output.ZWeibull$hessian),vcv<-matrix(data=NA,nrow=6,ncol=6))#
#
#store betas and ses#
weib.est[i,7]<-output.ZWeibull$par[1]+1/exp(output.ZWeibull$par[6])#
coeff<-c(weib.est[i,7],output.ZWeibull$par[6])#
varcov<-matrix(NA,2,2)#
varcov[1,1]<-vcv[1,1]#
varcov[1,2]<-vcv[1,6]#
varcov[2,1]<-vcv[6,1]#
varcov[2,2]<-vcv[6,6]#
weib.est[i,8]<-deltamethod(~(x1+1/exp(x2)), coeff, varcov, ses=TRUE)#
weib.est[i,9]<-output.ZWeibull$par[2]#
weib.est[i,10]<-sqrt(vcv[2,2])#
weib.est[i,11]<-output.ZWeibull$par[3]#
weib.est[i,12]<-sqrt(vcv[3,3])#
weib.est[i,13]<-output.ZWeibull$par[4]+1/exp(output.ZWeibull$par[6])#
coeff<-c(weib.est[i,13],output.ZWeibull$par[6])#
varcov<-matrix(NA,2,2)#
varcov[1,1]<-vcv[4,4]#
varcov[1,2]<-vcv[4,6]#
varcov[2,1]<-vcv[6,4]#
varcov[2,2]<-vcv[6,6]#
weib.est[i,14]<-deltamethod(~(x1+1/exp(x2)), coeff, varcov, ses=TRUE)#
weib.est[i,15]<-output.ZWeibull$par[5]#
weib.est[i,16]<-sqrt(vcv[5,5])#
weib.est[i,17]<-exp(output.ZWeibull$par[6])#
coeff<-c(weib.est[i,17])#
varcov<-matrix(NA,1,1)#
varcov[1,1]<-vcv[6,6]#
weib.est[i,18]<-deltamethod(~(exp(x1)), coeff, varcov, ses=TRUE)#
#store rmse#
weib.rmse[i,4]<-sqrt((tru.est[i,3]-weib.est[i,7])^2)#
weib.rmse[i,5]<-sqrt((tru.est[i,4]-weib.est[i,9])^2)#
weib.rmse[i,6]<-sqrt((tru.est[i,5]-weib.est[i,11])^2)#
weib.rmse[i,7]<-sqrt((tru.est[i,1]-weib.est[i,13])^2)#
weib.rmse[i,8]<-sqrt((tru.est[i,2]-weib.est[i,15])^2)#
weib.rmse[i,9]<-sqrt((tru.est[i,6]-weib.est[i,17])^2)#
#
#calculate upper and lower 95% CI's#
g0.lower<-weib.est[i,7]-(1.959964*weib.est[i,8])#
g0.upper<-weib.est[i,7]+(1.959964*weib.est[i,8])#
g1.lower<-weib.est[i,9]-(1.959964*weib.est[i,10])#
g1.upper<-weib.est[i,9]+(1.959964*weib.est[i,10])#
g2.lower<-weib.est[i,11]-(1.959964*weib.est[i,12])#
g2.upper<-weib.est[i,11]+(1.959964*weib.est[i,12])#
b0.lower<-weib.est[i,13]-(1.959964*weib.est[i,14])#
b0.upper<-weib.est[i,13]+(1.959964*weib.est[i,14])#
b1.lower<-weib.est[i,15]-(1.959964*weib.est[i,16])#
b1.upper<-weib.est[i,15]+(1.959964*weib.est[i,16])#
p.lower<-weib.est[i,17]-(1.959964*weib.est[i,18])#
p.upper<-weib.est[i,17]+(1.959964*weib.est[i,18])#
#
#store coverage parameters#
weib.cp[i,4]<-ifelse(tru.est[i,3]>g0.lower & tru.est[i,3]<g0.upper, 1,0)#
weib.cp[i,5]<-ifelse(tru.est[i,4]>g1.lower & tru.est[i,4]<g1.upper, 1,0)#
weib.cp[i,6]<-ifelse(tru.est[i,5]>g2.lower & tru.est[i,5]<g2.upper, 1,0)#
weib.cp[i,7]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
weib.cp[i,8]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
weib.cp[i,9]<-ifelse(tru.est[i,6]>p.lower & tru.est[i,6]<p.upper, 1,0)#
}
Y<-ycen#
C<-di#
X<-cbind(1,x)#
Z<-cbind(1,z,x)#
BayesZExponential = mcmcOF2(Y, C, X, Z, N = 400, burn = 100, thin = 1,  w = c(1, 1, 1), m = 10, form = "Exponential")#
output.BayesZExponential = list(par = c(summary(mcmc(BayesZExponential$beta))[[1]][,1], summary(mcmc(BayesZExponential$gamma))[[1]][,1]), #
                                se = c(summary(mcmc(BayesZExponential$beta))[[1]][,2], summary(mcmc(BayesZExponential$gamma))[[1]][,2]),#
                                CI = rbind(summary(mcmc(BayesZExponential$beta))[[2]], summary(mcmc(BayesZExponential$gamma))[[2]]))
output.BayesZExponential
summary(mcmc(BayesZExponential$beta))
BayesZExponential$beta
mcmcse(BayesZExponential)
mcse(BayesZExponential$beta)
mcse.multi(BayesZExponential$beta)
summary(mcmc(BayesZExponential$beta))
?mcse.multi
output.BayesZExponentia
output.BayesZExponential
?summary
?mcmc::summary
??summary
colMeans(Results)
colMeans(Result)
b0.lower<-output.BayesZExponential$CI[1,1]#
b0.upper<-output.BayesZExponential$CI[1,5]#
b1.lower<-output.BayesZExponential$CI[2,1]#
b1.upper<-output.BayesZExponential$CI[2,5]#
g0.lower<-output.BayesZExponential$CI[3,1]#
g0.upper<-output.BayesZExponential$CI[3,5]#
g1.lower<-output.BayesZExponential$CI[4,1]#
g1.upper<-output.BayesZExponential$CI[4,5]#
g2.lower<-output.BayesZExponential$CI[5,1]#
g2.upper<-output.BayesZExponential$CI[5,5]
b0.lower
#store rmse#
weib.rmse[i,10]<-sqrt((tru.est[i,3]-weib.est[i,19])^2)#
weib.rmse[i,11]<-sqrt((tru.est[i,4]-weib.est[i,21])^2)#
weib.rmse[i,12]<-sqrt((tru.est[i,5]-weib.est[i,23])^2)#
weib.rmse[i,13]<-sqrt((tru.est[i,1]-weib.est[i,25])^2)#
weib.rmse[i,14]<-sqrt((tru.est[i,2]-weib.est[i,27])^2)#
weib.rmse[i,15]<-sqrt((tru.est[i,6]-weib.est[i,29])^2)
weib.rmse
summary(mcmc(BayesZWeibull$beta))
summary(mcmc(BayesZExponential$beta))
summary(mcmc(BayesZExponential$beta))[[1]]
summary(mcmc(BayesZExponential$beta))[[1]][,4]
####
###1. Store True values for X0, X1, Z0, Z1, Z2, P#
###2. Store proportion censored pre & post#
###3. Simmulate n of 1000, do this 1000 times#
###4. Estimate cox, weibull, store all relevant coefficient estimates (exponentiate p's where applicable)#
###5. For each value in 4, calculate CPs and RMSEs, store.#
###6. Estimate zombie exp and zombie weibull-->store all relevant coefficient estimates (exponentiate p's where applicable).#
###7. For each value in 6, calculate CPs and RMSEs, store.#
#
###############
####Set Up#####
###############
#
#clear memory#
rm( list=ls() )#
#
#load necessary libraries 						                                 #
library(foreign)#
library(Zelig)#
library(car)#
library(MASS)#
library(VGAM)#
library(plotrix)#
library(pscl)#
library(survival)#
library(msm)#
library(verification)#
library(corpcor)#
library(Design)#
library(coda)#
library(mcmcse)#
#library(devtools)#
#install_github('bomin8319/BayesOFsurv/pkg')#
library(BayesOFsurv)#
#set working directory#
setwd("/Users/bomin8319/Desktop/BayesOFsurv/coding material/Monte Carlos/Mixture DGP/")#
#
###########################################################################
###########################################################################
############################Monte Carlo####################################
###########################################################################
#
#set seed#
set.seed(3)   #
#
#set the number of observations#
n<-1000#
#
#set the number of simulations, and create matrices to store the results#
nsims<-1000#
#history matrix for true estimates#
tru.est<-matrix(NA,nrow=nsims,ncol=8)#
#history matrix for cox estimates#
cox.est<-matrix(NA,nrow=nsims,ncol=2)#
#history matrix for exp estimates#
exp.est<-matrix(NA,nrow=nsims,ncol=24)#
#history matrix for weibull estimates#
weib.est<-matrix(NA,nrow=nsims,ncol=30)#
#history matrix for cox RMSE#
cox.rmse<-matrix(NA,nrow=nsims,ncol=1)#
#history matrix for exp RMSE#
exp.rmse<-matrix(NA,nrow=nsims,ncol=12)#
#history matrix for exp RMSE#
weib.rmse<-matrix(NA,nrow=nsims,ncol=15)#
#history matrix for cox CP#
cox.cp<-matrix(NA,nrow=nsims,ncol=1)#
#history matrix for exp CP#
exp.cp<-matrix(NA,nrow=nsims,ncol=12)#
#history matrix for exp CP#
weib.cp<-matrix(NA,nrow=nsims,ncol=15)#
main.data<-cbind(tru.est,cox.est,exp.est,weib.est,cox.rmse,exp.rmse,weib.rmse,cox.cp,exp.cp,weib.cp)#
colnames(main.data)<-c("true.x0","true.x1","true.z0","true.z1","true.z2","true.p","cen.lat","cen.obs",#
                       "cox.x1","cox.x1.se",#
                       "exp.x0","exp.x0.se","exp.x1","exp.x1.se",#
                       "zexp.z0","zexp.z0.se","zexp.z1","zexp.z1.se","zexp.z2","zexp.z2.se","zexp.x0","zexp.x0.se","zexp.x1","zexp.x1.se",#
                       "bzexp.x0","zexp.x0.se","bzexp.x1","bzexp.x1.se","bzexp.z0","bzexp.z0.se","bzexp.z1","bzexp.z1.se","bzexp.z2","bzexp.z2.se",#
                       "wei.x0","wei.x0.se","wei.x1","wei.x1.se","wei.p","wei.p.se",#
                       "zwei.z0","zwei.z0.se","zwei.z1","zwei.z1.se","zwei.z2","zwei.z2.se","zwei.x0","zwei.x0.se","zwei.x1","zwei.x1.se","zwei.p","zwei.p.se",#
                       "bzwei.x0","bzwei.x0.se","bzwei.x1","bzwei.x1.se","bzwei.z0","bzwei.z0.se","bzwei.z1","bzwei.z1.se","bzwei.z2","bzwei.z2.se","bzwei.p","bzwei.p.se",#
                       "cox.x1.rmse",#
                       "exp.x0.rmse","exp.x1.rmse","zexp.z0.rmse","zexp.z1.rmse","zexp.z2.rmse","zexp.x0.rmse","zexp.x1.rmse","bzexp.x0.rmse","bzexp.x1.rmse","bzexp.z0.rmse","bzexp.z1.rmse","bzexp.z2.rmse",#
                       "wei.x0.rmse","wei.x1.rmse","wei.p.rmse","zwei.z0.rmse","zwei.z1.rmse","zwei.z2.rmse",#
                       "zwei.x0.rmse","zwei.x1.rmse","zwei.p.rmse", "bzwei.x0.rmse","bzwei.x1.rmse","bzwei.z0.rmse","bzwei.z1.rmse","bzwei.z2.rmse","bzwei.p.rmse",#
                       "cox.x1.cp","exp.x0.cp","exp.x1.cp","zexp.z0.cp","zexp.z1.cp","zexp.z2.cp","zexp.x0.cp","zexp.x1.cp","bzexp.x0.cp","bzexp.x1.cp","bzexp.z0.cp","bzexp.z1.cp","bzexp.z2.cp",#
                       "wei.x0.cp","wei.x1.cp","wei.p.cp",#
                       "zwei.z0.cp","zwei.z1.cp","zwei.z2.cp","zwei.x0.cp","zwei.x1.cp","zwei.p.cp", "bzwei.x0.cp","bzwei.x1.cp","bzwei.z0.cp","bzwei.z1.cp","bzwei.z2.cp","bzwei.p.cp")#
#
#create covariates#
x<-runif(n, min=-2.5, max=12)#
z<-log(runif(n, min=1, max=100))#
#create a dependent variable, begin the simmulations#
for(i in 1:nsims){#
  print(i)#
#Assign parameter values#
tru.est[i,1]<-1#
tru.est[i,2]<-3.5#
tru.est[i,3]<--2#
tru.est[i,4]<-2#
tru.est[i,5]<-3#
tru.est[i,6]<-1#
#
myrates <- exp(tru.est[i,1]+(tru.est[i,2]*x)) #
y <- rexp(n, rate = myrates) # generates the r.v.#
cen <- rexp(n, rate = 1 )#
ycen <- pmin(y, cen)#
di <- as.numeric(y <= cen)#
tru.est[i,7]<-table(di)[1]#
#
#create parameters for ZG#
phi<-1/(1+exp(-(tru.est[i,3]+tru.est[i,4]*z+tru.est[i,5]*x)))#
print(mean(phi))#
yzero<-matrix(1,n,1)#
error<--1*rlogis(n)#
flag<-error<qlogis(phi)#
yzero[flag]<-error[flag]#
flag<-yzero==1#
di[flag]<-ifelse(di[flag]==0,yzero[flag],di[flag])#
tru.est[i,8]<-table(di)[1]#
#
data<-cbind(ycen,di,x,z)#
######################################################################################
###################################COX Model##########################################
######################################################################################
#
#store estimate and se#
cox.est[i,1]<-summary(coxph(Surv(ycen, di)~x,coxph.control(iter.max = 10000)))$coef[1]#
cox.est[i,2]<-summary(coxph(Surv(ycen, di)~x,coxph.control(iter.max = 10000)))$coef[3]#
#
#store rmse#
cox.rmse[i,1]<-sqrt((tru.est[i,2]-cox.est[i,1])^2)#
#
#calculate upper and lower 95% CI's#
b1.lower<-cox.est[i,1]-(1.959964*cox.est[i,2])#
b1.upper<-cox.est[i,1]+(1.959964*cox.est[i,2])#
#
#store coverage parameters#
cox.cp[i,1]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
##############################################################################
########################Simple Exponential Model##############################
##############################################################################
Exponential<- function(est,Y,C,X,data) {					      #
	n=nrow(data)							      					  #
	llik <- matrix(0, nrow=n, ncol = 1)#
	beta<-est[1:length(est)]#
	XB<-X%*%beta#
	llik<-C*(XB-exp(XB)*Y)+(1-C)*(-exp(XB)*Y)#
	llik<--1*sum(llik)#
	return(llik)#
	}#
#set starting parameters#
est<-rbind(.01,.01)#
#
#set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
#optimize#
output.Exponential<-try(optim(f=Exponential,  p=est, X=X,Y=Y,C=C, method="BFGS", control=list(maxit=10000),  data=data, hessian=TRUE), TRUE)#
#
if(class(output.Exponential)=="list"){#
	ifelse(is.positive.definite(output.Exponential$hessian)==TRUE,vcv<-solve(output.Exponential$hessian),vcv<-matrix(data=NA,nrow=2,ncol=2))#
#
#store betas and ses#
exp.est[i,1]<-output.Exponential$par[1]#
exp.est[i,2]<-sqrt(vcv[1,1])#
exp.est[i,3]<-output.Exponential$par[2]#
exp.est[i,4]<-sqrt(vcv[2,2])#
#
#store rmse#
exp.rmse[i,1]<-sqrt((tru.est[i,1]-exp.est[i,1])^2)#
exp.rmse[i,2]<-sqrt((tru.est[i,2]-exp.est[i,3])^2)#
#
#calculate upper and lower 95% CI's#
b0.lower<-exp.est[i,1]-(1.959964*exp.est[i,2])#
b0.upper<-exp.est[i,1]+(1.959964*exp.est[i,2])#
b1.lower<-exp.est[i,3]-(1.959964*exp.est[i,4])#
b1.upper<-exp.est[i,3]+(1.959964*exp.est[i,4])#
#store coverage parameters#
exp.cp[i,1]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
exp.cp[i,2]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
#
}#
#
#################################################################################
#########################Simple Weibull Model ###################################
#################################################################################
#
#Note this estiamtes the model via hazard rates, a la Stata#
#
test<-survreg(Surv(ycen, di)~x, dist="weibull")#
summary(test)#
Weibull<- function(est,Y,C,X,data) {					      #
	n=nrow(data)							      					  #
	llik <- matrix(0, nrow=n, ncol = 1)#
	beta<-est[1:length(est)-1]#
	p<-est[length(est)]#
	p<-exp(p)#
	XB<-X%*%beta#
	llik<-C*(log(exp(XB+1/p)*p*((exp(XB+1/p)*Y)^(p-1))*exp(-(exp(XB+1/p)*Y)^p)))+(1-C)*log(exp(-(exp(XB+1/p)*Y)^p))#
	llik<--1*sum(llik)#
	return(llik)#
	}#
#set starting parameters#
est<-rbind(exp.est[i,1],exp.est[i,3],.01)#
#
#set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
#optimize#
output.Weibull<-try(optim(f=Weibull,  p=est, X=X,Y=Y,C=C, method="BFGS", control=list(maxit=10000),  data=data, hessian=TRUE), TRUE)#
#
if(class(output.Weibull)=="list"){#
	ifelse(is.positive.definite(output.Weibull$hessian)==TRUE,vcv<-solve(output.Weibull$hessian),vcv<-matrix(data=NA,nrow=3,ncol=3))#
#
#store betas and ses#
weib.est[i,1]<-output.Weibull$par[1]+1/exp(output.Weibull$par[3])#
coeff<-c(weib.est[i,1],output.Weibull$par[3])#
varcov<-matrix(NA,2,2)#
varcov[1,1]<-vcv[1,1]#
varcov[1,2]<-vcv[1,3]#
varcov[2,1]<-vcv[3,1]#
varcov[2,2]<-vcv[3,3]#
weib.est[i,2]<-deltamethod(~(x1+1/exp(x2)), coeff, varcov, ses=TRUE)#
weib.est[i,3]<-output.Weibull$par[2]#
weib.est[i,4]<-sqrt(vcv[2,2])#
weib.est[i,5]<-exp(output.Weibull$par[3])#
coeff<-c(weib.est[i,5])#
varcov<-matrix(NA,1,1)#
varcov[1,1]<-vcv[3,3]#
weib.est[i,6]<-deltamethod(~(exp(x1)), coeff, varcov, ses=TRUE)#
#store rmse#
weib.rmse[i,1]<-sqrt((tru.est[i,1]-weib.est[i,1])^2)#
weib.rmse[i,2]<-sqrt((tru.est[i,2]-weib.est[i,3])^2)#
weib.rmse[i,3]<-sqrt((tru.est[i,6]-weib.est[i,5])^2)#
#
#calculate upper and lower 95% CI's#
b0.lower<-weib.est[i,1]-(1.959964*weib.est[i,2])#
b0.upper<-weib.est[i,1]+(1.959964*weib.est[i,2])#
b1.lower<-weib.est[i,3]-(1.959964*weib.est[i,4])#
b1.upper<-weib.est[i,3]+(1.959964*weib.est[i,4])#
p.lower<-weib.est[i,5]-(1.959964*weib.est[i,6])#
p.upper<-weib.est[i,5]+(1.959964*weib.est[i,6])#
#
#store coverage parameters#
weib.cp[i,1]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
weib.cp[i,2]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
weib.cp[i,3]<-ifelse(tru.est[i,6]>p.lower & tru.est[i,6]<p.upper, 1,0)#
#
}#
#
###logit estimates####
dataset<-as.data.frame(data)#
logitcoef1<-glm(di~ z+x, data = dataset, family = "binomial")$coef[1]#
logitcoef2<-glm(di~ z+x, data = dataset, family = "binomial")$coef[2]#
logitcoef3<-glm(di~ z+x, data = dataset, family = "binomial")$coef[3]#
#
################################################################################
##########################Zombie Exponential Model##############################
################################################################################
#This program estimates the Exponential loglikelihood function returning hazard rate form coefficients#
#
ZExponential<- function(est,Y,C,X,Z,data) {					      #
	n=nrow(data)							      					  #
	llik <- matrix(0, nrow=n, ncol = 1)#
	gamma<-est[1:ncol(Z)]#
	beta<-est[(ncol(Z)+1):length(est)]#
	XB<-X%*%beta#
	ZG<-Z%*%gamma#
	phi<-1/(1+exp(-ZG))#
	llik<-C*(log((1-phi)+phi*exp(XB)*exp(-exp(XB)*Y)))+(1-C)*(log(phi)+-exp(XB)*Y)#
	llik<--1*sum(llik)#
	return(llik)#
	}#
#set starting parameters#
est<-rbind(.01,.01,.01,exp.est[i,1],exp.est[i,3])#
#
#set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
Z<-cbind(1,z,x)#
#optimize#
output.ZExponential<-try(optim(f=ZExponential,  p=est, X=X,Y=Y,C=C,Z=Z, method="BFGS", control=list(maxit=10000),  data=data, hessian=TRUE), TRUE)#
#
if(class(output.ZExponential)=="list"){#
	ifelse(is.positive.definite(output.ZExponential$hessian)==TRUE,vcv<-solve(output.ZExponential$hessian),vcv<-matrix(data=NA,nrow=5,ncol=5))#
#
#store betas and ses#
exp.est[i,5]<-output.ZExponential$par[1]#
exp.est[i,6]<-sqrt(vcv[1,1])#
exp.est[i,7]<-output.ZExponential$par[2]#
exp.est[i,8]<-sqrt(vcv[2,2])#
exp.est[i,9]<-output.ZExponential$par[3]#
exp.est[i,10]<-sqrt(vcv[3,3])#
exp.est[i,11]<-output.ZExponential$par[4]#
exp.est[i,12]<-sqrt(vcv[4,4])#
exp.est[i,13]<-output.ZExponential$par[5]#
exp.est[i,14]<-sqrt(vcv[5,5])#
#
#store rmse#
exp.rmse[i,3]<-sqrt((tru.est[i,3]-exp.est[i,5])^2)#
exp.rmse[i,4]<-sqrt((tru.est[i,4]-exp.est[i,7])^2)#
exp.rmse[i,5]<-sqrt((tru.est[i,5]-exp.est[i,9])^2)#
exp.rmse[i,6]<-sqrt((tru.est[i,1]-exp.est[i,11])^2)#
exp.rmse[i,7]<-sqrt((tru.est[i,2]-exp.est[i,13])^2)#
#
#calculate upper and lower 95% CI's#
g0.lower<-exp.est[i,5]-(1.959964*exp.est[i,6])#
g0.upper<-exp.est[i,5]+(1.959964*exp.est[i,6])#
g1.lower<-exp.est[i,7]-(1.959964*exp.est[i,8])#
g1.upper<-exp.est[i,7]+(1.959964*exp.est[i,8])#
g2.lower<-exp.est[i,9]-(1.959964*exp.est[i,10])#
g2.upper<-exp.est[i,9]+(1.959964*exp.est[i,10])#
b0.lower<-exp.est[i,11]-(1.959964*exp.est[i,12])#
b0.upper<-exp.est[i,11]+(1.959964*exp.est[i,12])#
b1.lower<-exp.est[i,13]-(1.959964*exp.est[i,14])#
b1.upper<-exp.est[i,13]+(1.959964*exp.est[i,14])#
#store coverage parameters#
exp.cp[i,3]<-ifelse(tru.est[i,3]>g0.lower & tru.est[i,3]<g0.upper, 1,0)#
exp.cp[i,4]<-ifelse(tru.est[i,4]>g1.lower & tru.est[i,4]<g1.upper, 1,0)#
exp.cp[i,5]<-ifelse(tru.est[i,5]>g2.lower & tru.est[i,5]<g2.upper, 1,0)#
exp.cp[i,6]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
exp.cp[i,7]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
#
}#
#
######################################################################################
##########################Zombie Weibull Model #######################################
######################################################################################
#This program estimates the Exponential loglikelihood function returning hazard rate form coefficients#
#
ZWeibull<- function(est,Y,C,X,Z,data) {					      #
	n=nrow(data)							      					  #
	llik <- matrix(0, nrow=n, ncol = 1)#
	gamma<-est[1:ncol(Z)]#
	beta<-est[(ncol(Z)+1):(length(est)-1)]#
	p<-est[length(est)]#
	p<-exp(p)#
	XB<-X%*%beta#
	ZG<-Z%*%gamma#
	phi<-1/(1+exp(-(ZG+1/p)))#
	llik<-C*(log((1-phi)+phi*exp(XB+1/p)*p*((exp(XB+1/p)*Y)^(p-1))*exp(-(exp(XB+1/p)*Y)^p)))+(1-C)*(log(phi)+-(exp(XB+1/p)*Y)^p)#
	llik<--1*sum(llik)#
	return(llik)#
	}#
#set starting parameters#
est<-rbind(.01,.01,.01,output.Weibull$par[1],output.Weibull$par[2],output.Weibull$par[3])#
#
#set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
Z<-cbind(1,z,x)#
#optimize#
output.ZWeibull<-try(optim(f=ZWeibull,  p=est, X=X,Y=Y,C=C,Z=Z, method="BFGS", control=list(maxit=10000),  data=data, hessian=TRUE), TRUE)#
#
if(class(output.ZWeibull)=="list"){#
	ifelse(is.positive.definite(output.ZWeibull$hessian)==TRUE,vcv<-solve(output.ZWeibull$hessian),vcv<-matrix(data=NA,nrow=6,ncol=6))#
#
#store betas and ses#
weib.est[i,7]<-output.ZWeibull$par[1]+1/exp(output.ZWeibull$par[6])#
coeff<-c(weib.est[i,7],output.ZWeibull$par[6])#
varcov<-matrix(NA,2,2)#
varcov[1,1]<-vcv[1,1]#
varcov[1,2]<-vcv[1,6]#
varcov[2,1]<-vcv[6,1]#
varcov[2,2]<-vcv[6,6]#
weib.est[i,8]<-deltamethod(~(x1+1/exp(x2)), coeff, varcov, ses=TRUE)#
weib.est[i,9]<-output.ZWeibull$par[2]#
weib.est[i,10]<-sqrt(vcv[2,2])#
weib.est[i,11]<-output.ZWeibull$par[3]#
weib.est[i,12]<-sqrt(vcv[3,3])#
weib.est[i,13]<-output.ZWeibull$par[4]+1/exp(output.ZWeibull$par[6])#
coeff<-c(weib.est[i,13],output.ZWeibull$par[6])#
varcov<-matrix(NA,2,2)#
varcov[1,1]<-vcv[4,4]#
varcov[1,2]<-vcv[4,6]#
varcov[2,1]<-vcv[6,4]#
varcov[2,2]<-vcv[6,6]#
weib.est[i,14]<-deltamethod(~(x1+1/exp(x2)), coeff, varcov, ses=TRUE)#
weib.est[i,15]<-output.ZWeibull$par[5]#
weib.est[i,16]<-sqrt(vcv[5,5])#
weib.est[i,17]<-exp(output.ZWeibull$par[6])#
coeff<-c(weib.est[i,17])#
varcov<-matrix(NA,1,1)#
varcov[1,1]<-vcv[6,6]#
weib.est[i,18]<-deltamethod(~(exp(x1)), coeff, varcov, ses=TRUE)#
#store rmse#
weib.rmse[i,4]<-sqrt((tru.est[i,3]-weib.est[i,7])^2)#
weib.rmse[i,5]<-sqrt((tru.est[i,4]-weib.est[i,9])^2)#
weib.rmse[i,6]<-sqrt((tru.est[i,5]-weib.est[i,11])^2)#
weib.rmse[i,7]<-sqrt((tru.est[i,1]-weib.est[i,13])^2)#
weib.rmse[i,8]<-sqrt((tru.est[i,2]-weib.est[i,15])^2)#
weib.rmse[i,9]<-sqrt((tru.est[i,6]-weib.est[i,17])^2)#
#
#calculate upper and lower 95% CI's#
g0.lower<-weib.est[i,7]-(1.959964*weib.est[i,8])#
g0.upper<-weib.est[i,7]+(1.959964*weib.est[i,8])#
g1.lower<-weib.est[i,9]-(1.959964*weib.est[i,10])#
g1.upper<-weib.est[i,9]+(1.959964*weib.est[i,10])#
g2.lower<-weib.est[i,11]-(1.959964*weib.est[i,12])#
g2.upper<-weib.est[i,11]+(1.959964*weib.est[i,12])#
b0.lower<-weib.est[i,13]-(1.959964*weib.est[i,14])#
b0.upper<-weib.est[i,13]+(1.959964*weib.est[i,14])#
b1.lower<-weib.est[i,15]-(1.959964*weib.est[i,16])#
b1.upper<-weib.est[i,15]+(1.959964*weib.est[i,16])#
p.lower<-weib.est[i,17]-(1.959964*weib.est[i,18])#
p.upper<-weib.est[i,17]+(1.959964*weib.est[i,18])#
#
#store coverage parameters#
weib.cp[i,4]<-ifelse(tru.est[i,3]>g0.lower & tru.est[i,3]<g0.upper, 1,0)#
weib.cp[i,5]<-ifelse(tru.est[i,4]>g1.lower & tru.est[i,4]<g1.upper, 1,0)#
weib.cp[i,6]<-ifelse(tru.est[i,5]>g2.lower & tru.est[i,5]<g2.upper, 1,0)#
weib.cp[i,7]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
weib.cp[i,8]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
weib.cp[i,9]<-ifelse(tru.est[i,6]>p.lower & tru.est[i,6]<p.upper, 1,0)#
}#
################################################################################
######################Bayesian Zombie Exponential Model#########################
################################################################################
##set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
Z<-cbind(1,z,x)#
BayesZExponential = mcmcOF2(Y, C, X, Z, N = 400, burn = 100, thin = 1,  w = c(1, 1, 1), m = 10, form = "Exponential")#
output.BayesZExponential = list(par = c(summary(mcmc(BayesZExponential$beta))[[1]][,1], summary(mcmc(BayesZExponential$gamma))[[1]][,1]), #
                                se = c(summary(mcmc(BayesZExponential$beta))[[1]][,4], summary(mcmc(BayesZExponential$gamma))[[1]][,4]),#
                                CI = rbind(summary(mcmc(BayesZExponential$beta))[[2]], summary(mcmc(BayesZExponential$gamma))[[2]]))#
exp.est[i,15]<-output.BayesZExponential$par[1]#
exp.est[i,16]<-output.BayesZExponential$se[1]#
exp.est[i,17]<-output.BayesZExponential$par[2]#
exp.est[i,18]<-output.BayesZExponential$se[2]#
exp.est[i,19]<-output.BayesZExponential$par[3]#
exp.est[i,20]<-output.BayesZExponential$se[3]#
exp.est[i,21]<-output.BayesZExponential$par[4]#
exp.est[i,22]<-output.BayesZExponential$se[4]#
exp.est[i,23]<-output.BayesZExponential$par[5]#
exp.est[i,24]<-output.BayesZExponential$se[5]#
#
##store rmse#
exp.rmse[i,8]<-sqrt((tru.est[i,3]-exp.est[i,15])^2)#
exp.rmse[i,9]<-sqrt((tru.est[i,4]-exp.est[i,17])^2)#
exp.rmse[i,10]<-sqrt((tru.est[i,5]-exp.est[i,19])^2)#
exp.rmse[i,11]<-sqrt((tru.est[i,1]-exp.est[i,21])^2)#
exp.rmse[i,12]<-sqrt((tru.est[i,2]-exp.est[i,23])^2)#
##calculate upper and lower 95% CI's#
b0.lower<-output.BayesZExponential$CI[1,1]#
b0.upper<-output.BayesZExponential$CI[1,5]#
b1.lower<-output.BayesZExponential$CI[2,1]#
b1.upper<-output.BayesZExponential$CI[2,5]#
g0.lower<-output.BayesZExponential$CI[3,1]#
g0.upper<-output.BayesZExponential$CI[3,5]#
g1.lower<-output.BayesZExponential$CI[4,1]#
g1.upper<-output.BayesZExponential$CI[4,5]#
g2.lower<-output.BayesZExponential$CI[5,1]#
g2.upper<-output.BayesZExponential$CI[5,5]#
#b0.lower<-exp.est[i,15]-(1.959964*exp.est[i,16])#
#b0.upper<-exp.est[i,15]+(1.959964*exp.est[i,16])#
#b1.lower<-exp.est[i,17]-(1.959964*exp.est[i,18])#
#b1.upper<-exp.est[i,17]+(1.959964*exp.est[i,18])#
#g0.lower<-exp.est[i,19]-(1.959964*exp.est[i,20])#
#g0.upper<-exp.est[i,19]+(1.959964*exp.est[i,20])#
#g1.lower<-exp.est[i,21]-(1.959964*exp.est[i,22])#
#g1.upper<-exp.est[i,21]+(1.959964*exp.est[i,22])#
#g2.lower<-exp.est[i,23]-(1.959964*exp.est[i,24])#
#g2.upper<-exp.est[i,23]+(1.959964*exp.est[i,24])#
#store coverage parameters#
exp.cp[i,8]<-ifelse(tru.est[i,3]>g0.lower & tru.est[i,3]<g0.upper, 1,0)#
exp.cp[i,9]<-ifelse(tru.est[i,4]>g1.lower & tru.est[i,4]<g1.upper, 1,0)#
exp.cp[i,10]<-ifelse(tru.est[i,5]>g2.lower & tru.est[i,5]<g2.upper, 1,0)#
exp.cp[i,11]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
exp.cp[i,12]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
#
################################################################################
########################Bayesian Zombie Weibull Model###########################
################################################################################
#set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
Z<-cbind(1,z,x)#
BayesZWeibull = mcmcOF2(Y, C, X, Z, N = 400, burn = 100, thin = 1,  w = c(1, 1, 1), m = 10, form = "Weibull")#
output.BayesZWeibull = list(par = c(summary(mcmc(BayesZWeibull$beta))[[1]][,1], summary(mcmc(BayesZWeibull$gamma))[[1]][,1], #
                                    summary(mcmc(BayesZWeibull$lambda))[[1]][1]), #
                            se = c(summary(mcmc(BayesZWeibull$beta))[[1]][,4], summary(mcmc(BayesZWeibull$gamma))[[1]][,4], #
                                   summary(mcmc(BayesZWeibull$lambda))[[1]][4]),#
                            CI = rbind(summary(mcmc(BayesZWeibull$beta))[[2]], summary(mcmc(BayesZWeibull$gamma))[[2]], #
                                       summary(mcmc(BayesZWeibull$lambda))[[2]]))#
weib.est[i,19]<-output.BayesZWeibull$par[1]#
weib.est[i,20]<-output.BayesZWeibull$se[1]#
weib.est[i,21]<-output.BayesZWeibull$par[2]#
weib.est[i,22]<-output.BayesZWeibull$se[2]#
weib.est[i,23]<-output.BayesZWeibull$par[3]#
weib.est[i,24]<-output.BayesZWeibull$se[3]#
weib.est[i,25]<-output.BayesZWeibull$par[4]#
weib.est[i,26]<-output.BayesZWeibull$se[4]#
weib.est[i,27]<-output.BayesZWeibull$par[5]#
weib.est[i,28]<-output.BayesZWeibull$se[5]#
weib.est[i,29]<-output.BayesZWeibull$par[6]#
weib.est[i,30]<-output.BayesZWeibull$se[6]#
#
#store rmse#
weib.rmse[i,10]<-sqrt((tru.est[i,3]-weib.est[i,19])^2)#
weib.rmse[i,11]<-sqrt((tru.est[i,4]-weib.est[i,21])^2)#
weib.rmse[i,12]<-sqrt((tru.est[i,5]-weib.est[i,23])^2)#
weib.rmse[i,13]<-sqrt((tru.est[i,1]-weib.est[i,25])^2)#
weib.rmse[i,14]<-sqrt((tru.est[i,2]-weib.est[i,27])^2)#
weib.rmse[i,15]<-sqrt((tru.est[i,6]-weib.est[i,29])^2)#
#
#calculate upper and lower 95% CI's#
b0.lower<-output.BayesZWeibull$CI[1,1]#
b0.upper<-output.BayesZWeibull$CI[1,5]#
b1.lower<-output.BayesZWeibull$CI[2,1]#
b1.upper<-output.BayesZWeibull$CI[2,5]#
g0.lower<-output.BayesZWeibull$CI[3,1]#
g0.upper<-output.BayesZWeibull$CI[3,5]#
g1.lower<-output.BayesZWeibull$CI[4,1]#
g1.upper<-output.BayesZWeibull$CI[4,5]#
g2.lower<-output.BayesZWeibull$CI[5,1]#
g2.upper<-output.BayesZWeibull$CI[5,5]#
p.lower<-output.BayesZWeibull$CI[6,1]#
p.upper<-output.BayesZWeibull$CI[6,2]#
# g0.lower<-weib.est[i,19]-(1.959964*weib.est[i,20])#
# g0.upper<-weib.est[i,19]+(1.959964*weib.est[i,20])#
# g1.lower<-weib.est[i,21]-(1.959964*weib.est[i,22])#
# g1.upper<-weib.est[i,21]+(1.959964*weib.est[i,22])#
# g2.lower<-weib.est[i,23]-(1.959964*weib.est[i,24])#
# g2.upper<-weib.est[i,23]+(1.959964*weib.est[i,24])#
# b0.lower<-weib.est[i,25]-(1.959964*weib.est[i,26])#
# b0.upper<-weib.est[i,25]+(1.959964*weib.est[i,26])#
# b1.lower<-weib.est[i,27]-(1.959964*weib.est[i,28])#
# b1.upper<-weib.est[i,27]+(1.959964*weib.est[i,28])#
# p.lower<-weib.est[i,29]-(1.959964*weib.est[i,30])#
# p.upper<-weib.est[i,29]+(1.959964*weib.est[i,30])#
#
#store coverage parameters#
weib.cp[i,10]<-ifelse(tru.est[i,3]>g0.lower & tru.est[i,3]<g0.upper, 1,0)#
weib.cp[i,11]<-ifelse(tru.est[i,4]>g1.lower & tru.est[i,4]<g1.upper, 1,0)#
weib.cp[i,12]<-ifelse(tru.est[i,5]>g2.lower & tru.est[i,5]<g2.upper, 1,0)#
weib.cp[i,13]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
weib.cp[i,14]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
weib.cp[i,15]<-ifelse(tru.est[i,6]>p.lower & tru.est[i,6]<p.upper, 1,0)#
#
#combine matrices and label variables#
main.data[i, ]<-c(tru.est[i, ],cox.est[i, ],exp.est[i, ],weib.est[i, ],cox.rmse[i, ],exp.rmse[i, ],weib.rmse[i, ],#
                  cox.cp[i, ],exp.cp[i, ],weib.cp[i, ])#
#
}
head(main.data)
####
###1. Store True values for X0, X1, Z0, Z1, Z2, P#
###2. Store proportion censored pre & post#
###3. Simmulate n of 1000, do this 1000 times#
###4. Estimate cox, weibull, store all relevant coefficient estimates (exponentiate p's where applicable)#
###5. For each value in 4, calculate CPs and RMSEs, store.#
###6. Estimate zombie exp and zombie weibull-->store all relevant coefficient estimates (exponentiate p's where applicable).#
###7. For each value in 6, calculate CPs and RMSEs, store.#
#
###############
####Set Up#####
###############
#
#clear memory#
rm( list=ls() )#
#
#load necessary libraries 						                                 #
library(foreign)#
library(Zelig)#
library(car)#
library(MASS)#
library(VGAM)#
library(plotrix)#
library(pscl)#
library(survival)#
library(msm)#
library(verification)#
library(corpcor)#
library(Design)#
library(coda)#
library(mcmcse)#
#library(devtools)#
#install_github('bomin8319/BayesOFsurv/pkg')#
library(BayesOFsurv)#
#set working directory#
setwd("/Users/bomin8319/Desktop/BayesOFsurv/coding material/Monte Carlos/Mixture DGP/")#
#
###########################################################################
###########################################################################
############################Monte Carlo####################################
###########################################################################
#
#set seed#
set.seed(3)   #
#
#set the number of observations#
n<-1000#
#
#set the number of simulations, and create matrices to store the results#
nsims<-1000#
#history matrix for true estimates#
tru.est<-matrix(NA,nrow=nsims,ncol=8)#
#history matrix for cox estimates#
cox.est<-matrix(NA,nrow=nsims,ncol=2)#
#history matrix for exp estimates#
exp.est<-matrix(NA,nrow=nsims,ncol=24)#
#history matrix for weibull estimates#
weib.est<-matrix(NA,nrow=nsims,ncol=30)#
#history matrix for cox RMSE#
cox.rmse<-matrix(NA,nrow=nsims,ncol=1)#
#history matrix for exp RMSE#
exp.rmse<-matrix(NA,nrow=nsims,ncol=12)#
#history matrix for exp RMSE#
weib.rmse<-matrix(NA,nrow=nsims,ncol=15)#
#history matrix for cox CP#
cox.cp<-matrix(NA,nrow=nsims,ncol=1)#
#history matrix for exp CP#
exp.cp<-matrix(NA,nrow=nsims,ncol=12)#
#history matrix for exp CP#
weib.cp<-matrix(NA,nrow=nsims,ncol=15)#
main.data<-cbind(tru.est,cox.est,exp.est,weib.est,cox.rmse,exp.rmse,weib.rmse,cox.cp,exp.cp,weib.cp)#
colnames(main.data)<-c("true.x0","true.x1","true.z0","true.z1","true.z2","true.p","cen.lat","cen.obs",#
                       "cox.x1","cox.x1.se",#
                       "exp.x0","exp.x0.se","exp.x1","exp.x1.se",#
                       "zexp.z0","zexp.z0.se","zexp.z1","zexp.z1.se","zexp.z2","zexp.z2.se","zexp.x0","zexp.x0.se","zexp.x1","zexp.x1.se",#
                       "bzexp.x0","zexp.x0.se","bzexp.x1","bzexp.x1.se","bzexp.z0","bzexp.z0.se","bzexp.z1","bzexp.z1.se","bzexp.z2","bzexp.z2.se",#
                       "wei.x0","wei.x0.se","wei.x1","wei.x1.se","wei.p","wei.p.se",#
                       "zwei.z0","zwei.z0.se","zwei.z1","zwei.z1.se","zwei.z2","zwei.z2.se","zwei.x0","zwei.x0.se","zwei.x1","zwei.x1.se","zwei.p","zwei.p.se",#
                       "bzwei.x0","bzwei.x0.se","bzwei.x1","bzwei.x1.se","bzwei.z0","bzwei.z0.se","bzwei.z1","bzwei.z1.se","bzwei.z2","bzwei.z2.se","bzwei.p","bzwei.p.se",#
                       "cox.x1.rmse",#
                       "exp.x0.rmse","exp.x1.rmse","zexp.z0.rmse","zexp.z1.rmse","zexp.z2.rmse","zexp.x0.rmse","zexp.x1.rmse","bzexp.x0.rmse","bzexp.x1.rmse","bzexp.z0.rmse","bzexp.z1.rmse","bzexp.z2.rmse",#
                       "wei.x0.rmse","wei.x1.rmse","wei.p.rmse","zwei.z0.rmse","zwei.z1.rmse","zwei.z2.rmse",#
                       "zwei.x0.rmse","zwei.x1.rmse","zwei.p.rmse", "bzwei.x0.rmse","bzwei.x1.rmse","bzwei.z0.rmse","bzwei.z1.rmse","bzwei.z2.rmse","bzwei.p.rmse",#
                       "cox.x1.cp","exp.x0.cp","exp.x1.cp","zexp.z0.cp","zexp.z1.cp","zexp.z2.cp","zexp.x0.cp","zexp.x1.cp","bzexp.x0.cp","bzexp.x1.cp","bzexp.z0.cp","bzexp.z1.cp","bzexp.z2.cp",#
                       "wei.x0.cp","wei.x1.cp","wei.p.cp",#
                       "zwei.z0.cp","zwei.z1.cp","zwei.z2.cp","zwei.x0.cp","zwei.x1.cp","zwei.p.cp", "bzwei.x0.cp","bzwei.x1.cp","bzwei.z0.cp","bzwei.z1.cp","bzwei.z2.cp","bzwei.p.cp")#
#
#create covariates#
x<-runif(n, min=-2.5, max=12)#
z<-log(runif(n, min=1, max=100))#
#create a dependent variable, begin the simmulations#
for(i in 1:nsims){#
  print(i)#
#Assign parameter values#
tru.est[i,1]<-1#
tru.est[i,2]<-3.5#
tru.est[i,3]<--2#
tru.est[i,4]<-2#
tru.est[i,5]<-3#
tru.est[i,6]<-1#
#
myrates <- exp(tru.est[i,1]+(tru.est[i,2]*x)) #
y <- rexp(n, rate = myrates) # generates the r.v.#
cen <- rexp(n, rate = 1 )#
ycen <- pmin(y, cen)#
di <- as.numeric(y <= cen)#
tru.est[i,7]<-table(di)[1]#
#
#create parameters for ZG#
phi<-1/(1+exp(-(tru.est[i,3]+tru.est[i,4]*z+tru.est[i,5]*x)))#
print(mean(phi))#
yzero<-matrix(1,n,1)#
error<--1*rlogis(n)#
flag<-error<qlogis(phi)#
yzero[flag]<-error[flag]#
flag<-yzero==1#
di[flag]<-ifelse(di[flag]==0,yzero[flag],di[flag])#
tru.est[i,8]<-table(di)[1]#
#
data<-cbind(ycen,di,x,z)#
######################################################################################
###################################COX Model##########################################
######################################################################################
#
#store estimate and se#
cox.est[i,1]<-summary(coxph(Surv(ycen, di)~x,coxph.control(iter.max = 10000)))$coef[1]#
cox.est[i,2]<-summary(coxph(Surv(ycen, di)~x,coxph.control(iter.max = 10000)))$coef[3]#
#
#store rmse#
cox.rmse[i,1]<-sqrt((tru.est[i,2]-cox.est[i,1])^2)#
#
#calculate upper and lower 95% CI's#
b1.lower<-cox.est[i,1]-(1.959964*cox.est[i,2])#
b1.upper<-cox.est[i,1]+(1.959964*cox.est[i,2])#
#
#store coverage parameters#
cox.cp[i,1]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
##############################################################################
########################Simple Exponential Model##############################
##############################################################################
Exponential<- function(est,Y,C,X,data) {					      #
	n=nrow(data)							      					  #
	llik <- matrix(0, nrow=n, ncol = 1)#
	beta<-est[1:length(est)]#
	XB<-X%*%beta#
	llik<-C*(XB-exp(XB)*Y)+(1-C)*(-exp(XB)*Y)#
	llik<--1*sum(llik)#
	return(llik)#
	}#
#set starting parameters#
est<-rbind(.01,.01)#
#
#set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
#optimize#
output.Exponential<-try(optim(f=Exponential,  p=est, X=X,Y=Y,C=C, method="BFGS", control=list(maxit=10000),  data=data, hessian=TRUE), TRUE)#
#
if(class(output.Exponential)=="list"){#
	ifelse(is.positive.definite(output.Exponential$hessian)==TRUE,vcv<-solve(output.Exponential$hessian),vcv<-matrix(data=NA,nrow=2,ncol=2))#
#
#store betas and ses#
exp.est[i,1]<-output.Exponential$par[1]#
exp.est[i,2]<-sqrt(vcv[1,1])#
exp.est[i,3]<-output.Exponential$par[2]#
exp.est[i,4]<-sqrt(vcv[2,2])#
#
#store rmse#
exp.rmse[i,1]<-sqrt((tru.est[i,1]-exp.est[i,1])^2)#
exp.rmse[i,2]<-sqrt((tru.est[i,2]-exp.est[i,3])^2)#
#
#calculate upper and lower 95% CI's#
b0.lower<-exp.est[i,1]-(1.959964*exp.est[i,2])#
b0.upper<-exp.est[i,1]+(1.959964*exp.est[i,2])#
b1.lower<-exp.est[i,3]-(1.959964*exp.est[i,4])#
b1.upper<-exp.est[i,3]+(1.959964*exp.est[i,4])#
#store coverage parameters#
exp.cp[i,1]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
exp.cp[i,2]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
#
}#
#
#################################################################################
#########################Simple Weibull Model ###################################
#################################################################################
#
#Note this estiamtes the model via hazard rates, a la Stata#
#
test<-survreg(Surv(ycen, di)~x, dist="weibull")#
summary(test)#
Weibull<- function(est,Y,C,X,data) {					      #
	n=nrow(data)							      					  #
	llik <- matrix(0, nrow=n, ncol = 1)#
	beta<-est[1:length(est)-1]#
	p<-est[length(est)]#
	p<-exp(p)#
	XB<-X%*%beta#
	llik<-C*(log(exp(XB+1/p)*p*((exp(XB+1/p)*Y)^(p-1))*exp(-(exp(XB+1/p)*Y)^p)))+(1-C)*log(exp(-(exp(XB+1/p)*Y)^p))#
	llik<--1*sum(llik)#
	return(llik)#
	}#
#set starting parameters#
est<-rbind(exp.est[i,1],exp.est[i,3],.01)#
#
#set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
#optimize#
output.Weibull<-try(optim(f=Weibull,  p=est, X=X,Y=Y,C=C, method="BFGS", control=list(maxit=10000),  data=data, hessian=TRUE), TRUE)#
#
if(class(output.Weibull)=="list"){#
	ifelse(is.positive.definite(output.Weibull$hessian)==TRUE,vcv<-solve(output.Weibull$hessian),vcv<-matrix(data=NA,nrow=3,ncol=3))#
#
#store betas and ses#
weib.est[i,1]<-output.Weibull$par[1]+1/exp(output.Weibull$par[3])#
coeff<-c(weib.est[i,1],output.Weibull$par[3])#
varcov<-matrix(NA,2,2)#
varcov[1,1]<-vcv[1,1]#
varcov[1,2]<-vcv[1,3]#
varcov[2,1]<-vcv[3,1]#
varcov[2,2]<-vcv[3,3]#
weib.est[i,2]<-deltamethod(~(x1+1/exp(x2)), coeff, varcov, ses=TRUE)#
weib.est[i,3]<-output.Weibull$par[2]#
weib.est[i,4]<-sqrt(vcv[2,2])#
weib.est[i,5]<-exp(output.Weibull$par[3])#
coeff<-c(weib.est[i,5])#
varcov<-matrix(NA,1,1)#
varcov[1,1]<-vcv[3,3]#
weib.est[i,6]<-deltamethod(~(exp(x1)), coeff, varcov, ses=TRUE)#
#store rmse#
weib.rmse[i,1]<-sqrt((tru.est[i,1]-weib.est[i,1])^2)#
weib.rmse[i,2]<-sqrt((tru.est[i,2]-weib.est[i,3])^2)#
weib.rmse[i,3]<-sqrt((tru.est[i,6]-weib.est[i,5])^2)#
#
#calculate upper and lower 95% CI's#
b0.lower<-weib.est[i,1]-(1.959964*weib.est[i,2])#
b0.upper<-weib.est[i,1]+(1.959964*weib.est[i,2])#
b1.lower<-weib.est[i,3]-(1.959964*weib.est[i,4])#
b1.upper<-weib.est[i,3]+(1.959964*weib.est[i,4])#
p.lower<-weib.est[i,5]-(1.959964*weib.est[i,6])#
p.upper<-weib.est[i,5]+(1.959964*weib.est[i,6])#
#
#store coverage parameters#
weib.cp[i,1]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
weib.cp[i,2]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
weib.cp[i,3]<-ifelse(tru.est[i,6]>p.lower & tru.est[i,6]<p.upper, 1,0)#
#
}#
#
###logit estimates####
dataset<-as.data.frame(data)#
logitcoef1<-glm(di~ z+x, data = dataset, family = "binomial")$coef[1]#
logitcoef2<-glm(di~ z+x, data = dataset, family = "binomial")$coef[2]#
logitcoef3<-glm(di~ z+x, data = dataset, family = "binomial")$coef[3]#
#
################################################################################
##########################Zombie Exponential Model##############################
################################################################################
#This program estimates the Exponential loglikelihood function returning hazard rate form coefficients#
#
ZExponential<- function(est,Y,C,X,Z,data) {					      #
	n=nrow(data)							      					  #
	llik <- matrix(0, nrow=n, ncol = 1)#
	gamma<-est[1:ncol(Z)]#
	beta<-est[(ncol(Z)+1):length(est)]#
	XB<-X%*%beta#
	ZG<-Z%*%gamma#
	phi<-1/(1+exp(-ZG))#
	llik<-C*(log((1-phi)+phi*exp(XB)*exp(-exp(XB)*Y)))+(1-C)*(log(phi)+-exp(XB)*Y)#
	llik<--1*sum(llik)#
	return(llik)#
	}#
#set starting parameters#
est<-rbind(.01,.01,.01,exp.est[i,1],exp.est[i,3])#
#
#set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
Z<-cbind(1,z,x)#
#optimize#
output.ZExponential<-try(optim(f=ZExponential,  p=est, X=X,Y=Y,C=C,Z=Z, method="BFGS", control=list(maxit=10000),  data=data, hessian=TRUE), TRUE)#
#
if(class(output.ZExponential)=="list"){#
	ifelse(is.positive.definite(output.ZExponential$hessian)==TRUE,vcv<-solve(output.ZExponential$hessian),vcv<-matrix(data=NA,nrow=5,ncol=5))#
#
#store betas and ses#
exp.est[i,5]<-output.ZExponential$par[1]#
exp.est[i,6]<-sqrt(vcv[1,1])#
exp.est[i,7]<-output.ZExponential$par[2]#
exp.est[i,8]<-sqrt(vcv[2,2])#
exp.est[i,9]<-output.ZExponential$par[3]#
exp.est[i,10]<-sqrt(vcv[3,3])#
exp.est[i,11]<-output.ZExponential$par[4]#
exp.est[i,12]<-sqrt(vcv[4,4])#
exp.est[i,13]<-output.ZExponential$par[5]#
exp.est[i,14]<-sqrt(vcv[5,5])#
#
#store rmse#
exp.rmse[i,3]<-sqrt((tru.est[i,3]-exp.est[i,5])^2)#
exp.rmse[i,4]<-sqrt((tru.est[i,4]-exp.est[i,7])^2)#
exp.rmse[i,5]<-sqrt((tru.est[i,5]-exp.est[i,9])^2)#
exp.rmse[i,6]<-sqrt((tru.est[i,1]-exp.est[i,11])^2)#
exp.rmse[i,7]<-sqrt((tru.est[i,2]-exp.est[i,13])^2)#
#
#calculate upper and lower 95% CI's#
g0.lower<-exp.est[i,5]-(1.959964*exp.est[i,6])#
g0.upper<-exp.est[i,5]+(1.959964*exp.est[i,6])#
g1.lower<-exp.est[i,7]-(1.959964*exp.est[i,8])#
g1.upper<-exp.est[i,7]+(1.959964*exp.est[i,8])#
g2.lower<-exp.est[i,9]-(1.959964*exp.est[i,10])#
g2.upper<-exp.est[i,9]+(1.959964*exp.est[i,10])#
b0.lower<-exp.est[i,11]-(1.959964*exp.est[i,12])#
b0.upper<-exp.est[i,11]+(1.959964*exp.est[i,12])#
b1.lower<-exp.est[i,13]-(1.959964*exp.est[i,14])#
b1.upper<-exp.est[i,13]+(1.959964*exp.est[i,14])#
#store coverage parameters#
exp.cp[i,3]<-ifelse(tru.est[i,3]>g0.lower & tru.est[i,3]<g0.upper, 1,0)#
exp.cp[i,4]<-ifelse(tru.est[i,4]>g1.lower & tru.est[i,4]<g1.upper, 1,0)#
exp.cp[i,5]<-ifelse(tru.est[i,5]>g2.lower & tru.est[i,5]<g2.upper, 1,0)#
exp.cp[i,6]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
exp.cp[i,7]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
#
}#
#
######################################################################################
##########################Zombie Weibull Model #######################################
######################################################################################
#This program estimates the Exponential loglikelihood function returning hazard rate form coefficients#
#
ZWeibull<- function(est,Y,C,X,Z,data) {					      #
	n=nrow(data)							      					  #
	llik <- matrix(0, nrow=n, ncol = 1)#
	gamma<-est[1:ncol(Z)]#
	beta<-est[(ncol(Z)+1):(length(est)-1)]#
	p<-est[length(est)]#
	p<-exp(p)#
	XB<-X%*%beta#
	ZG<-Z%*%gamma#
	phi<-1/(1+exp(-(ZG+1/p)))#
	llik<-C*(log((1-phi)+phi*exp(XB+1/p)*p*((exp(XB+1/p)*Y)^(p-1))*exp(-(exp(XB+1/p)*Y)^p)))+(1-C)*(log(phi)+-(exp(XB+1/p)*Y)^p)#
	llik<--1*sum(llik)#
	return(llik)#
	}#
#set starting parameters#
est<-rbind(.01,.01,.01,output.Weibull$par[1],output.Weibull$par[2],output.Weibull$par[3])#
#
#set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
Z<-cbind(1,z,x)#
#optimize#
output.ZWeibull<-try(optim(f=ZWeibull,  p=est, X=X,Y=Y,C=C,Z=Z, method="BFGS", control=list(maxit=10000),  data=data, hessian=TRUE), TRUE)#
#
if(class(output.ZWeibull)=="list"){#
	ifelse(is.positive.definite(output.ZWeibull$hessian)==TRUE,vcv<-solve(output.ZWeibull$hessian),vcv<-matrix(data=NA,nrow=6,ncol=6))#
#
#store betas and ses#
weib.est[i,7]<-output.ZWeibull$par[1]+1/exp(output.ZWeibull$par[6])#
coeff<-c(weib.est[i,7],output.ZWeibull$par[6])#
varcov<-matrix(NA,2,2)#
varcov[1,1]<-vcv[1,1]#
varcov[1,2]<-vcv[1,6]#
varcov[2,1]<-vcv[6,1]#
varcov[2,2]<-vcv[6,6]#
weib.est[i,8]<-deltamethod(~(x1+1/exp(x2)), coeff, varcov, ses=TRUE)#
weib.est[i,9]<-output.ZWeibull$par[2]#
weib.est[i,10]<-sqrt(vcv[2,2])#
weib.est[i,11]<-output.ZWeibull$par[3]#
weib.est[i,12]<-sqrt(vcv[3,3])#
weib.est[i,13]<-output.ZWeibull$par[4]+1/exp(output.ZWeibull$par[6])#
coeff<-c(weib.est[i,13],output.ZWeibull$par[6])#
varcov<-matrix(NA,2,2)#
varcov[1,1]<-vcv[4,4]#
varcov[1,2]<-vcv[4,6]#
varcov[2,1]<-vcv[6,4]#
varcov[2,2]<-vcv[6,6]#
weib.est[i,14]<-deltamethod(~(x1+1/exp(x2)), coeff, varcov, ses=TRUE)#
weib.est[i,15]<-output.ZWeibull$par[5]#
weib.est[i,16]<-sqrt(vcv[5,5])#
weib.est[i,17]<-exp(output.ZWeibull$par[6])#
coeff<-c(weib.est[i,17])#
varcov<-matrix(NA,1,1)#
varcov[1,1]<-vcv[6,6]#
weib.est[i,18]<-deltamethod(~(exp(x1)), coeff, varcov, ses=TRUE)#
#store rmse#
weib.rmse[i,4]<-sqrt((tru.est[i,3]-weib.est[i,7])^2)#
weib.rmse[i,5]<-sqrt((tru.est[i,4]-weib.est[i,9])^2)#
weib.rmse[i,6]<-sqrt((tru.est[i,5]-weib.est[i,11])^2)#
weib.rmse[i,7]<-sqrt((tru.est[i,1]-weib.est[i,13])^2)#
weib.rmse[i,8]<-sqrt((tru.est[i,2]-weib.est[i,15])^2)#
weib.rmse[i,9]<-sqrt((tru.est[i,6]-weib.est[i,17])^2)#
#
#calculate upper and lower 95% CI's#
g0.lower<-weib.est[i,7]-(1.959964*weib.est[i,8])#
g0.upper<-weib.est[i,7]+(1.959964*weib.est[i,8])#
g1.lower<-weib.est[i,9]-(1.959964*weib.est[i,10])#
g1.upper<-weib.est[i,9]+(1.959964*weib.est[i,10])#
g2.lower<-weib.est[i,11]-(1.959964*weib.est[i,12])#
g2.upper<-weib.est[i,11]+(1.959964*weib.est[i,12])#
b0.lower<-weib.est[i,13]-(1.959964*weib.est[i,14])#
b0.upper<-weib.est[i,13]+(1.959964*weib.est[i,14])#
b1.lower<-weib.est[i,15]-(1.959964*weib.est[i,16])#
b1.upper<-weib.est[i,15]+(1.959964*weib.est[i,16])#
p.lower<-weib.est[i,17]-(1.959964*weib.est[i,18])#
p.upper<-weib.est[i,17]+(1.959964*weib.est[i,18])#
#
#store coverage parameters#
weib.cp[i,4]<-ifelse(tru.est[i,3]>g0.lower & tru.est[i,3]<g0.upper, 1,0)#
weib.cp[i,5]<-ifelse(tru.est[i,4]>g1.lower & tru.est[i,4]<g1.upper, 1,0)#
weib.cp[i,6]<-ifelse(tru.est[i,5]>g2.lower & tru.est[i,5]<g2.upper, 1,0)#
weib.cp[i,7]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
weib.cp[i,8]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
weib.cp[i,9]<-ifelse(tru.est[i,6]>p.lower & tru.est[i,6]<p.upper, 1,0)#
}#
################################################################################
######################Bayesian Zombie Exponential Model#########################
################################################################################
##set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
Z<-cbind(1,z,x)#
BayesZExponential = mcmcOF2(Y, C, X, Z, N = 400, burn = 100, thin = 1,  w = c(1, 1, 1), m = 10, form = "Exponential")#
output.BayesZExponential = list(par = c(summary(mcmc(BayesZExponential$beta))[[1]][,1], summary(mcmc(BayesZExponential$gamma))[[1]][,1]), #
                                se = c(summary(mcmc(BayesZExponential$beta))[[1]][,4], summary(mcmc(BayesZExponential$gamma))[[1]][,4]),#
                                CI = rbind(summary(mcmc(BayesZExponential$beta))[[2]], summary(mcmc(BayesZExponential$gamma))[[2]]))#
exp.est[i,15]<-output.BayesZExponential$par[1]#
exp.est[i,16]<-output.BayesZExponential$se[1]#
exp.est[i,17]<-output.BayesZExponential$par[2]#
exp.est[i,18]<-output.BayesZExponential$se[2]#
exp.est[i,19]<-output.BayesZExponential$par[3]#
exp.est[i,20]<-output.BayesZExponential$se[3]#
exp.est[i,21]<-output.BayesZExponential$par[4]#
exp.est[i,22]<-output.BayesZExponential$se[4]#
exp.est[i,23]<-output.BayesZExponential$par[5]#
exp.est[i,24]<-output.BayesZExponential$se[5]#
#
##store rmse#
exp.rmse[i,8]<-sqrt((tru.est[i,3]-exp.est[i,15])^2)#
exp.rmse[i,9]<-sqrt((tru.est[i,4]-exp.est[i,17])^2)#
exp.rmse[i,10]<-sqrt((tru.est[i,5]-exp.est[i,19])^2)#
exp.rmse[i,11]<-sqrt((tru.est[i,1]-exp.est[i,21])^2)#
exp.rmse[i,12]<-sqrt((tru.est[i,2]-exp.est[i,23])^2)#
##calculate upper and lower 95% CI's#
b0.lower<-output.BayesZExponential$CI[1,1]#
b0.upper<-output.BayesZExponential$CI[1,5]#
b1.lower<-output.BayesZExponential$CI[2,1]#
b1.upper<-output.BayesZExponential$CI[2,5]#
g0.lower<-output.BayesZExponential$CI[3,1]#
g0.upper<-output.BayesZExponential$CI[3,5]#
g1.lower<-output.BayesZExponential$CI[4,1]#
g1.upper<-output.BayesZExponential$CI[4,5]#
g2.lower<-output.BayesZExponential$CI[5,1]#
g2.upper<-output.BayesZExponential$CI[5,5]#
#b0.lower<-exp.est[i,15]-(1.959964*exp.est[i,16])#
#b0.upper<-exp.est[i,15]+(1.959964*exp.est[i,16])#
#b1.lower<-exp.est[i,17]-(1.959964*exp.est[i,18])#
#b1.upper<-exp.est[i,17]+(1.959964*exp.est[i,18])#
#g0.lower<-exp.est[i,19]-(1.959964*exp.est[i,20])#
#g0.upper<-exp.est[i,19]+(1.959964*exp.est[i,20])#
#g1.lower<-exp.est[i,21]-(1.959964*exp.est[i,22])#
#g1.upper<-exp.est[i,21]+(1.959964*exp.est[i,22])#
#g2.lower<-exp.est[i,23]-(1.959964*exp.est[i,24])#
#g2.upper<-exp.est[i,23]+(1.959964*exp.est[i,24])#
#store coverage parameters#
exp.cp[i,8]<-ifelse(tru.est[i,3]>g0.lower & tru.est[i,3]<g0.upper, 1,0)#
exp.cp[i,9]<-ifelse(tru.est[i,4]>g1.lower & tru.est[i,4]<g1.upper, 1,0)#
exp.cp[i,10]<-ifelse(tru.est[i,5]>g2.lower & tru.est[i,5]<g2.upper, 1,0)#
exp.cp[i,11]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
exp.cp[i,12]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
#
################################################################################
########################Bayesian Zombie Weibull Model###########################
################################################################################
#set data, Y and X#
data<-data#
Y<-ycen#
C<-di#
X<-cbind(1,x)#
Z<-cbind(1,z,x)#
BayesZWeibull = mcmcOF2(Y, C, X, Z, N = 400, burn = 100, thin = 1,  w = c(1, 1, 1), m = 10, form = "Weibull")#
output.BayesZWeibull = list(par = c(summary(mcmc(BayesZWeibull$beta))[[1]][,1], summary(mcmc(BayesZWeibull$gamma))[[1]][,1], #
                                    summary(mcmc(BayesZWeibull$lambda))[[1]][1]), #
                            se = c(summary(mcmc(BayesZWeibull$beta))[[1]][,4], summary(mcmc(BayesZWeibull$gamma))[[1]][,4], #
                                   summary(mcmc(BayesZWeibull$lambda))[[1]][4]),#
                            CI = rbind(summary(mcmc(BayesZWeibull$beta))[[2]], summary(mcmc(BayesZWeibull$gamma))[[2]], #
                                       summary(mcmc(BayesZWeibull$lambda))[[2]]))#
weib.est[i,19]<-output.BayesZWeibull$par[1]#
weib.est[i,20]<-output.BayesZWeibull$se[1]#
weib.est[i,21]<-output.BayesZWeibull$par[2]#
weib.est[i,22]<-output.BayesZWeibull$se[2]#
weib.est[i,23]<-output.BayesZWeibull$par[3]#
weib.est[i,24]<-output.BayesZWeibull$se[3]#
weib.est[i,25]<-output.BayesZWeibull$par[4]#
weib.est[i,26]<-output.BayesZWeibull$se[4]#
weib.est[i,27]<-output.BayesZWeibull$par[5]#
weib.est[i,28]<-output.BayesZWeibull$se[5]#
weib.est[i,29]<-output.BayesZWeibull$par[6]#
weib.est[i,30]<-output.BayesZWeibull$se[6]#
#
#store rmse#
weib.rmse[i,10]<-sqrt((tru.est[i,3]-weib.est[i,19])^2)#
weib.rmse[i,11]<-sqrt((tru.est[i,4]-weib.est[i,21])^2)#
weib.rmse[i,12]<-sqrt((tru.est[i,5]-weib.est[i,23])^2)#
weib.rmse[i,13]<-sqrt((tru.est[i,1]-weib.est[i,25])^2)#
weib.rmse[i,14]<-sqrt((tru.est[i,2]-weib.est[i,27])^2)#
weib.rmse[i,15]<-sqrt((tru.est[i,6]-weib.est[i,29])^2)#
#
#calculate upper and lower 95% CI's#
b0.lower<-output.BayesZWeibull$CI[1,1]#
b0.upper<-output.BayesZWeibull$CI[1,5]#
b1.lower<-output.BayesZWeibull$CI[2,1]#
b1.upper<-output.BayesZWeibull$CI[2,5]#
g0.lower<-output.BayesZWeibull$CI[3,1]#
g0.upper<-output.BayesZWeibull$CI[3,5]#
g1.lower<-output.BayesZWeibull$CI[4,1]#
g1.upper<-output.BayesZWeibull$CI[4,5]#
g2.lower<-output.BayesZWeibull$CI[5,1]#
g2.upper<-output.BayesZWeibull$CI[5,5]#
p.lower<-output.BayesZWeibull$CI[6,1]#
p.upper<-output.BayesZWeibull$CI[6,2]#
# g0.lower<-weib.est[i,19]-(1.959964*weib.est[i,20])#
# g0.upper<-weib.est[i,19]+(1.959964*weib.est[i,20])#
# g1.lower<-weib.est[i,21]-(1.959964*weib.est[i,22])#
# g1.upper<-weib.est[i,21]+(1.959964*weib.est[i,22])#
# g2.lower<-weib.est[i,23]-(1.959964*weib.est[i,24])#
# g2.upper<-weib.est[i,23]+(1.959964*weib.est[i,24])#
# b0.lower<-weib.est[i,25]-(1.959964*weib.est[i,26])#
# b0.upper<-weib.est[i,25]+(1.959964*weib.est[i,26])#
# b1.lower<-weib.est[i,27]-(1.959964*weib.est[i,28])#
# b1.upper<-weib.est[i,27]+(1.959964*weib.est[i,28])#
# p.lower<-weib.est[i,29]-(1.959964*weib.est[i,30])#
# p.upper<-weib.est[i,29]+(1.959964*weib.est[i,30])#
#
#store coverage parameters#
weib.cp[i,10]<-ifelse(tru.est[i,3]>g0.lower & tru.est[i,3]<g0.upper, 1,0)#
weib.cp[i,11]<-ifelse(tru.est[i,4]>g1.lower & tru.est[i,4]<g1.upper, 1,0)#
weib.cp[i,12]<-ifelse(tru.est[i,5]>g2.lower & tru.est[i,5]<g2.upper, 1,0)#
weib.cp[i,13]<-ifelse(tru.est[i,1]>b0.lower & tru.est[i,1]<b0.upper, 1,0)#
weib.cp[i,14]<-ifelse(tru.est[i,2]>b1.lower & tru.est[i,2]<b1.upper, 1,0)#
weib.cp[i,15]<-ifelse(tru.est[i,6]>p.lower & tru.est[i,6]<p.upper, 1,0)#
#
#combine matrices and label variables#
main.data[i, ]<-c(tru.est[i, ],cox.est[i, ],exp.est[i, ],weib.est[i, ],cox.rmse[i, ],exp.rmse[i, ],weib.rmse[i, ],#
                  cox.cp[i, ],exp.cp[i, ],weib.cp[i, ])#
#
}
#save dataset#
main.data1000<-as.data.frame(main.data)#
write.dta(main.data1000,"main.data1000.dta", )
colMeans(main.data1000)
